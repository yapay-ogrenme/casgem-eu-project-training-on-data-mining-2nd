{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RrSFTMapZqK"
      },
      "source": [
        "# Lojistik Regresyon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fAdhm74pZqa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model, metrics, model_selection\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBOM0yznpZqk"
      },
      "source": [
        "## Sınıflandırma\n",
        "\n",
        "Regresyonda sürekli bir değer tahmin etmeye çalışıyorduk - ör. sınav puanı.\n",
        "\n",
        "Sınıflandırmada, belirli bir veri noktası için bir sınıf etiketi öngörüyoruz - ör. sınavı geçmek/kalmak. Bunun gibi iki sınıflı bir problem (ikili sınıflandırma) veya çok sınıflı bir problem olabilir.\n",
        "\n",
        "Sadece bir sınıf etiketi öngörmüyoruz, aynı zamanda her sınıf için bir olasılık da tahmin ediyoruz.\n",
        "\n",
        "Ders geçme örneğimiz şu şekilde resmedilebilir\n",
        "\n",
        "![logistic_regression](https://upload.wikimedia.org/wikipedia/commons/6/6d/Exam_pass_logistic_curve.jpeg)\n",
        "\n",
        "Sınıflandırma yapmak için bir **karar** vermemiz gerekiyor. Bu, bir **karar sınırı** tanımlamayı gerektirir. \n",
        "1D'de (yani bir özellik ile) bu bir nokta olacak; \n",
        "2D'de (iki özellik ile) bu bir çizgi olacaktır; \n",
        "üç özelliği olan bir düzlem olacak; vb...\n",
        "\n",
        "Yukarıdaki durumumuzda `num_hours_studying` $\\gt 2.7$ ise, o zaman `geçti` tahmininde bulunacağımızı söyleyebiliriz.\n",
        "\n",
        "İki değişkenli bir durumda karar sınırımız şöyle görünebilir\n",
        "\n",
        "![bivariate_logistic_regression](https://i0.wp.com/ucanalytics.com/blogs/wp-content/uploads/2017/09/Scatter-Plot-with-Boundary-Logistic-Regression.jpg?resize=768%2C578 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYyRx9VMpZqv"
      },
      "source": [
        "## Hipotez\n",
        "\n",
        "Lojistik regresyon hipotezimiz doğrusal veya doğrusal olmayabilir. Doğrusal regresyondan farklı olan, sınıf olasılıklarının çıktısını almak istememizdir. Hipotezimizi bir olasılık değeri olarak ifade etmenin bir yoluna ihtiyacımız var.\n",
        "\n",
        "Bunu yapmak için sigmoid fonksiyonunu kullanıyoruz.\n",
        "\n",
        "$$\\sigma(h) = \\frac{e^h}{e^h + 1} = \\frac{1}{1 + e^{-h}}$$\n",
        "\n",
        "![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n",
        "\n",
        "Bu fonksiyonun girdisini $(0, 1)$ aralığına sıkıştıracağını ve bize geçerli bir olasılık değeri verdiğini görebiliriz.\n",
        "\n",
        "Nihai hipotezimizi elde etmek için verilerin doğrusal veya doğrusal olmayan işlevini sigmoid işlevinden geçireceğiz.\n",
        "\n",
        "$$h_\\theta(\\mathbf{x}) = P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{a}^\\top\\mathbf{x})$$\n",
        "\n",
        "$y=1$'ın ne anlama geldiğine keyfi olarak karar verebiliriz - örnekte diyelim ki sınavı geçti. Daha sonra sınavda başarısız olma olasılığı şu şekilde verilir:\n",
        "\n",
        "$$P(y=0|\\mathbf{x}) = 1 - P(y=1|\\mathbf{x}) = 1 - \\sigma(\\mathbf{a}^\\top\\mathbf{x})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9BO66SApZq4"
      },
      "source": [
        "## Kayıp Fonksiyonu\n",
        "\n",
        "Ortalama kare hatası (mean squared error (MSE)) burada uygun bir kayıp işlevi değildir. Olasılıklarla uğraşıyoruz, bu nedenle doğal seçim negatif log loss'dur.\n",
        "\n",
        "$$\\text{NLL} = -y \\log(h_\\theta) - (1 - y)\\log(1 - h_\\theta))$$\n",
        "\n",
        "$y$ ya $1$ ya da $0$ olacak, bu nedenle bu iki terimden sadece biri geçerli olacaktır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBEg8Sj3pZq8"
      },
      "source": [
        "## Optimizasyon\n",
        "\n",
        "Burada kapalı bir form kullanamayız, bu yüzden **gradyan iniş** kullanmamız gerekecek.\n",
        "\n",
        "Doğrusal regresyon hakkında bilgi edindiğimizde, kayıp yüzeyinin dışbükey olduğunu fark ettik - herhangi bir noktadan küresel minimuma nasıl ulaşacağınızı bilirsiniz.\n",
        "\n",
        "Buradaki kaybımız son derece dışbükey değildir. Bunun yerine, iyi bir yerel minimum bulmaya çalışmak için ilk başlangıç ​​noktasından akıllı adımlar atmak için yinelemeli bir algoritma kullanmamız gerekiyor. Resim şuna benziyor\n",
        "\n",
        "![gradient_descent](https://cdn-images-1.medium.com/max/1600/1*f9a162GhpMbiTVTAua_lLQ.png)\n",
        "\n",
        "Ana fikir, belirli bir noktada kayba göre parametrelerin gradyanını alırsanız, kaybı azaltmak için bu parametreleri nasıl değiştireceğinizi size söyleyecek olmasıdır.\n",
        "\n",
        "![gradient_descent_2](https://cdn-images-1.medium.com/max/800/0*rBQI7uBhBKE8KT-X.png)\n",
        "\n",
        "Açıkça bu şemada sağdan başlarsak, kaybımızı azaltmak için $w$'ı **azaltmamız** gerekir. Bu gerçekten ana fikir.\n",
        "\n",
        "Bu algoritma ile parametreleri güncellemek ve iyileştirmek için bir dizi \"adım\" atıyoruz. Güncellemenin her adımı\n",
        "\n",
        "$$\\theta' = \\theta - \\beta \\frac{\\partial \\text{NLL}(h_\\theta, y)}{\\partial \\theta}$$\n",
        "\n",
        "Burada $\\beta$ hiperparametresi çok önemlidir - adımlarımızın ne kadar büyük olduğunu kontrol eder. Yukarıdaki şemada, öğrenme oranı bizi \"U\" genişliğinin yarısı kadar bir adım atmış olsaydı, sadece bir yandan diğer yana zıplardık ve asla optimum seviyeye yerleşmezdik.\n",
        "\n",
        "Attığımız adım sayısına da dikkat etmemiz gerekiyor. Diyagramda sadece iki tane alsaydık, algoritmaya yerleşmek için zaman vermezdik. Pratikte ikiden çok daha fazla adım atacağız. Bu hiperparametreyi ayarlamanız gerekeceğini unutmayın."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vum8CU6EpZrA"
      },
      "source": [
        "## Veri Kümesi\n",
        "\n",
        "Güney Afrika'nın Western Cape bölgesinin kalp hastalığı yüksek riskli bölgesindeki erkeklerin retrospektif bir örneği. Koroner kalp hastalığı (KKH) vakası başına kabaca iki kontrol vardır. KKH pozitif erkeklerin çoğu, KKH olaylarından sonra risk faktörlerini azaltmak için kan basıncını düşürme tedavisi ve diğer programlardan geçmiştir. Bazı durumlarda ölçümler bu tedavilerden sonra yapılmıştır. Bu veriler, Rousseauw ve diğerleri, 1983, South African Medical Journal'da açıklanan daha büyük bir veri kümesinden alınmıştır. https://web.stanford.edu/~hastie/ElemStatLearn/ adresinden indirilmiştir.\n",
        "\n",
        "Özellikler:\n",
        "- sbp: sistolik kan basıncı\n",
        "- tütün (tobacco): kümülatif tütün (kg)\n",
        "- ldl: düşük yoğunluklu lipoprotein kolesterol\n",
        "- yağlanma (adiposity)\n",
        "- famhist: ailede kalp hastalığı öyküsü (Var, Yok)\n",
        "- typea: A tipi davranış\n",
        "- obezite (obesity)\n",
        "- alkol (alcohol): mevcut alkol tüketimi\n",
        "- yaş: başlangıç yaşı\n",
        "- chd: yanıt, koroner kalp hastalığı"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gyhuiogpZrC"
      },
      "source": [
        "## Veri Önişleme\n",
        "\n",
        "### Verileri Yükleme"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ROOT_DIR = \"/content/drive/MyDrive/CASGEM-Egitim/Egitim-Part2/Day11-DeepLearning/notebooks/\"\n",
        "ROOT_DIR = \"https://raw.githubusercontent.com/yapay-ogrenme/casgem-eu-project-training-on-data-mining/main/PART2/Day12-Optimization/notebooks/\"\n",
        "\n",
        "DATASET_PATH = ROOT_DIR + \"datasets/\""
      ],
      "metadata": {
        "id": "3q1pmZ6GSEVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SUGhOh_pZrF"
      },
      "outputs": [],
      "source": [
        "file_name = DATASET_PATH + 'SAheart.data'\n",
        "data = pd.read_csv(file_name, sep=',', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9i5nN_OppZrI"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kategorik Değişkenleri Düzenleme\n",
        "\n",
        "Bir kez daha, one-hot encoding olarak düzenlenmesi gereken kategorik bir değişkenimiz olan 'famhist' var. One-hot encoding, ikili değişkenler listesindeki (\"0\" veya \"1\") bir konuma bir sınıf etiketi atar ve istenen sınıfa karşılık gelen konuma bir \"1\" koyar.\n",
        "\n",
        "Böylece \"famhist\" için iki ikili değişkenin one-hot encoding olarak düzenlenecek.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\text{famhist: Present} \\rightarrow \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\\\\n",
        "\\text{famhist: Absent} \\rightarrow \\begin{bmatrix} 0 & 1 \\end{bmatrix} \\\\\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "PB12c9fDFl55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['famhist_true'] = data['famhist'] == 'Present'\n",
        "data['famhist_false'] = data['famhist'] == 'Absent'\n",
        "data = data.drop(['famhist'], axis=1)"
      ],
      "metadata": {
        "id": "mM6ko1nmFo06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUwss9tBpZrP"
      },
      "source": [
        "## Veri Görselleştirme\n",
        "\n",
        "Şimdi verilerdeki ilişkilere bir göz atalım. Sınıflandırma için yine de dağılım grafiklerini kullanabiliriz, ancak biraz farklı görünüyorlar. Yine daha sonra kullanmak için uygun bir fonksiyon yapalım. Göreceğiniz gibi, bu çizimleri biraz daha net hale getirmek için çizim boyutunu genişleteceğiz."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature(data, feature_name):\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.scatter(data[feature_name], data['chd'])\n",
        "    plt.xlabel(feature_name)\n",
        "    plt.ylabel('chd')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ey_uDhjd0M4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqfFhYoPpZrQ"
      },
      "source": [
        "Birlikte bir özelliğe göz atacağız, ardından veri kümesinin geri kalanını kendi başınıza keşfetmeli ve hangi özelliklerin yararlı olacağı konusunda kendi görüşlerinizi oluşturmalısınız."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Feature list:\n",
        "sbp tobacco ldl adiposity famhist_true famhist_false typea obesity alcohol age\n",
        "\"\"\"\n",
        "plot_feature(data, 'age')"
      ],
      "metadata": {
        "id": "4VZ1QT2H0M6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.loc[:, data.columns != 'chd']\n",
        "y = data['chd']"
      ],
      "metadata": {
        "id": "BJQbkJ8F0YkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "2pd5B2qB8M5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Build a forest and compute the feature importances\n",
        "forest = ExtraTreesClassifier(n_estimators=250,\n",
        "                              random_state=0)\n",
        "\n",
        "forest.fit(X, y)\n",
        "\n",
        "importances = forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
        "             axis=0)\n",
        "\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importances\")\n",
        "plt.barh(range(X.shape[1]), importances[indices],\n",
        "       color=\"r\", xerr=std[indices], align=\"center\")\n",
        "# If you want to define your own labels,\n",
        "# change indices to a list of labels on the following line.\n",
        "plt.yticks(range(X.shape[1]), indices)\n",
        "plt.ylim([-1, X.shape[1]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ikuVfdHj0Ymt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_index = 6\n",
        "X.columns[feature_index]"
      ],
      "metadata": {
        "id": "ytI2JREh0M9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z27BgUBpZrN"
      },
      "source": [
        "Özellik seçimini daha sonra yapacağımız için, bu sefer bir veri setini `train` ve `test` olarak ayırmak için uygun bir fonksiyon yapmak için zaman ayıralım ve ayrıca bizim için her iki sette $x$ ve $y$'ı ayıralım. Bunu sizin için yapıyoruz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm7JPtabpZrO"
      },
      "outputs": [],
      "source": [
        "def split(data):\n",
        "    # control randomization for reproducibility\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    train, test = model_selection.train_test_split(data)\n",
        "    x_train = train.loc[:, train.columns != 'chd']\n",
        "    y_train = train['chd']\n",
        "    x_test = test.loc[:, test.columns != 'chd']\n",
        "    y_test = test['chd']\n",
        "    return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GldeGsihpZrT"
      },
      "source": [
        "## Temel Model Eğitimi\n",
        "\n",
        "### Değerlendirme\n",
        "\n",
        "Sınıflarımızın etiketini tahmin ettiğimiz için (kalp hastalığı olsun veya olmasın), çok daha sezgisel bir performans ölçüsüne sahibiz: tahmin doğruluğu. Bunu hesaplamak için `sklearn.metrics.accuracy_score`u kullanacağız - buraya bakın http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb-4bkAzpZrU"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, x_train, y_train, x_test, y_test):\n",
        "    train_preds = model.predict(x_train)\n",
        "    test_preds = model.predict(x_test)\n",
        "    train_acc = metrics.accuracy_score(y_train, train_preds)\n",
        "    test_acc = metrics.accuracy_score(y_test, test_preds)\n",
        "    print('Train accuracy: %s' % train_acc)\n",
        "    print('Test accuracy: %s' % test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmUWrjmRpZrU"
      },
      "source": [
        "### Gradyan İniş Modeli\n",
        "\n",
        "Burada bir temel gradyan iniş modeli uygulanacak. Özellik seçimi veya ayar düzenlemesi yapılacak. \n",
        "\n",
        "Temel olarak, `sklearn.linear_model.SGDClassifier` sınıfını kullanır - burada http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier'a başvurun. `loss='log'` argümanını ileterek bir lojistik regresyon modeli elde ederiz.\n",
        "\n",
        "Temel (baseline) olarak işaretlemek için buradaki değişkenlere `bl` ekleniyor. Ayrıca verileri bölecek, modeli eğitecek ve eğitim ve test doğruluklarını döndürecek bir kullanışlı yardımcı işlev daha ekleyelim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Ytgx9-r7pZrV"
      },
      "outputs": [],
      "source": [
        "def split_train_evaluate(model, data):\n",
        "    x_train, y_train, x_test, y_test = split(data)\n",
        "    model.fit(x_train, y_train)\n",
        "    evaluate(model, x_train, y_train, x_test, y_test)\n",
        "\n",
        "\n",
        "model_bl = linear_model.SGDClassifier(loss='log', max_iter=10000)\n",
        "split_train_evaluate(model_bl, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COcu_kDXpZrW"
      },
      "source": [
        "### Grid Search\n",
        "\n",
        "Bu görevle ilgili zor olan şey, eğitimlerimiz sırasında ilk kez bir dizi hiper parametreye sahip olmamızdır. Bu, makine öğrenimi uygulamasındaki yaygın durumdur. Bu sorunla başa çıkmak için bir yaklaşım, her hiperparametre için makul bir değer kümesi tanımlamak ve ardından eğitim kümesinde çapraz doğrulama (cross validation) kullanarak bunların tüm kombinasyonlarını aramaktır. Bu tekniğe **grid search** denir.\n",
        "\n",
        "`sklearn.model_selection.GridSearchCV` ile Grid Search yapabiliriz, buraya bakın http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html.\n",
        "\n",
        "Nasıl çalıştığını görelim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtLhr9qXpZrW"
      },
      "outputs": [],
      "source": [
        "# this may take a few seconds to run\n",
        "x_train, y_train, x_test, y_test = split(data)\n",
        "\n",
        "grid_search = model_selection.GridSearchCV(\n",
        "    estimator=linear_model.SGDClassifier(loss='log'),\n",
        "    param_grid={'alpha': [0.01, 0.1, 1.],\n",
        "                'max_iter': [1000, 10000]},\n",
        "    cv=10,\n",
        "    return_train_score=True)\n",
        "\n",
        "\n",
        "grid_search.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-PGTGyDpZrX"
      },
      "source": [
        "Grid Search sonuçları, parametre seçeneklerinin ne yaptığını görebilmeniz için görüntüleme için doğrudan bir `DataFrame` içine yüklenebilen bir dict tipinde gelir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCt0zv06pZrY"
      },
      "outputs": [],
      "source": [
        "r = pd.DataFrame(grid_search.cv_results_)\n",
        "# we only want a subset of the columns for a precise summary\n",
        "r[['params', 'mean_train_score', 'mean_test_score']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N06xfJOApZrY"
      },
      "source": [
        "Grid Search ayrıca sizin için en iyi modeli otomatik olarak seçer ve daha sonra en uygun parametrelerle kullanabilirsiniz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlJExVA0pZrZ"
      },
      "outputs": [],
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "print(grid_search.best_params_)\n",
        "evaluate(best_model, x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcSeDsBdpZra"
      },
      "source": [
        "### Örnek - 2\n",
        "\n",
        "En iyi modelinizi bulmak ve yüzde 72,4'lük temel test doğruluğumuzu geçmek için yukarıdaki Grid Search kodunu kullanabilirsiniz.\n",
        "\n",
        "'SGDClassifier' belgelerindeki (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) parametreleri incelemek ve kendi kararlarınızı vermek isteyeceksiniz. makul bir arama alanı nasıl görünmelidir.\n",
        "\n",
        "Ayrıca, bir parametre için en iyi değeriniz arama alanınızın kenarındaysa, tırmanmaya devam edip edemeyeceğinizi görmek için alanı bu yönde daha da genişletmek istediğinizi unutmayın. Örneğin, $\\alpha$ için alanım `[0.1, 0.5, 1.]` ise ve en iyi sonuç `1.` ile geldiyse, kesinlikle `2.` ve `5.` ve benzerlerini denemeliyim. .\n",
        "\n",
        "Ayrıca özellik seçimini de unutmayın. Görselleştirdiğimiz bölüme dönün. İsterseniz, ridge regresyonu gerçekleştirmek için parametreler aracılığıyla bazı $L2$ cezalarını deneyin ve uygulayın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t77VtDWOpZrc"
      },
      "outputs": [],
      "source": [
        "def get_feature_set(data, wanted_features):\n",
        "    return data.loc[:, [col in wanted_features for col in data.columns]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYxGmaYFpZrd"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'alpha': [0.1, 0.3,  1.,  2.,  5.],\n",
        "    'max_iter': [1000, 10000, 100000]\n",
        "}\n",
        "\n",
        "#param_grid = {\n",
        "#    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
        "#    'alpha': [0.01, 0.06, 0.1, 0.3,  1.],\n",
        "#    'max_iter': [1000, 10000, 100000]\n",
        "#}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxWlvPutpZre"
      },
      "source": [
        "Özellik seçimini gerçekleştirmek için `data.columns` öğesini kendi seçtiğiniz özelliklerin bir listesiyle değiştirin - ör. `['feature1', 'feature2']`'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WlSGM9upZre"
      },
      "outputs": [],
      "source": [
        "#wanted_features = ['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist_true', 'typea',\n",
        "#                   'famhist_false', 'obesity', 'alcohol', 'age', 'chd']  # must have chd!\n",
        "\n",
        "wanted_features = ['sbp', 'tobacco', 'ldl', 'adiposity', 'typea', 'obesity', 'alcohol', 'age', 'chd']  # must have chd!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNoBPONPpZre"
      },
      "source": [
        "Özellik kümenizi ve Grid Search parametreleri tanımladıktan sonra, en iyi sonucu elde etmek için aşağıdaki hücreyi çalıştırın. \n",
        "Sabırlı olun, özellikle Grid Search büyükse, biraz zaman alabilir!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6TlAYUepZrf"
      },
      "outputs": [],
      "source": [
        "# this may take a few seconds to load\n",
        "my_data = get_feature_set(data, wanted_features)  # feature selection\n",
        "\n",
        "x_train, y_train, x_test, y_test = split(my_data) # splits\n",
        "grid_search = model_selection.GridSearchCV(       # perform grid search\n",
        "    estimator=linear_model.SGDClassifier(loss='log'),\n",
        "    param_grid=param_grid,\n",
        "    cv=10,\n",
        "    return_train_score=True)\n",
        "\n",
        "grid_search.fit(x_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "evaluate(best_model, x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiWiS34OpZrf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py37",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Day12_Optimization_Demo1_SGD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}