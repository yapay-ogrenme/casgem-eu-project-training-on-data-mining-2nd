{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day14_NLP_Preprocessing_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tfpbzdUviIoH",
        "pq5F61Nzrkyj",
        "2SevqPXLrnh1",
        "gjdD1n_Q5e6E",
        "gUjtm42DC6Q-",
        "E3IunhlqEG9O",
        "4gwvL3VjFuV_",
        "b3hOeRLB3Jy5",
        "bkdFN_ZI4k1b",
        "UFccuMVw5Ev2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "EkLlr88Jh-YD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN45bMUDrzrf"
      },
      "outputs": [],
      "source": [
        "!pip install zemberek-python\n",
        "!pip install antlr4-python3-runtime==4.9\n",
        "!pip install unicode_tr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "tfpbzdUviIoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_doc = \"\"\"\n",
        "Mustafa Kemal Atatürk[c] (1881[d] - 10 Kasım 1938), Türk asker, devlet adamı ve Türkiye Cumhuriyeti'nin kurucusudur.\n",
        "\n",
        "I. Dünya Savaşı sırasında Osmanlı ordusunda görev yapan Atatürk, Çanakkale Cephesi'nde miralaylığa, Sina ve Filistin Cephesi'nde ise Yıldırım Orduları komutanlığına atandı. Savaşın sonunda, Osmanlı İmparatorluğu'nun yenilgisini takiben \n",
        "Kurtuluş Savaşı ile simgelenen Türk Ulusal Hareketi'ne öncülük ve önderlik etti. \n",
        "Türk Kurtuluş Savaşı sürecinde Ankara Hükûmeti'ni kurdu, Türk Orduları Başkomutanı olarak Sakarya Meydan Muharebesi'ndeki başarısından dolayı 19 Eylül 1921 tarihinde \"Gazi\" unvanını aldı ve mareşallik rütbesine yükseldi. Askerî ve siyasi eylemleriyle İtilaf Devletleri ve destekçilerine karşı zafer kazandı. \n",
        "\n",
        "Savaşın ardından Cumhuriyet Halk Partisi'ni Halk Fırkası adıyla kurdu ve ilk genel başkanı oldu. \n",
        "29 Ekim 1923'te Cumhuriyetin İlanı akabinde Cumhurbaşkanı seçildi. \n",
        "1938'deki ölümüne dek dört dönem bu görevi yürüterek Türkiye'de en uzun süre cumhurbaşkanlığı yapmış kişi oldu. \n",
        "\n",
        "\n",
        "İstanbul, Isparta,\n",
        "http://yavuzkomecoglu.com/, www.basakbuluz.com, #yâpayzeka, yaklaşık ~12',\n",
        "e-posta adresim: komecoglu.yavuz@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "sample_doc"
      ],
      "metadata": {
        "id": "IZd5-v2ZoFzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK"
      ],
      "metadata": {
        "id": "F748aPkMoIeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "HjoDGhFviCWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sent_tokenize"
      ],
      "metadata": {
        "id": "uKceY016raYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(sample_doc, language='turkish')\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "Mw7_kQmNiCY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### word_tokenize"
      ],
      "metadata": {
        "id": "VkzhPQdDrXy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  words = word_tokenize(sentence)\n",
        "  print(words)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "xPOZHMkYrE90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zemberek"
      ],
      "metadata": {
        "id": "YOTwYJXsqDzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zemberek import TurkishSentenceExtractor"
      ],
      "metadata": {
        "id": "WwC6wfL8qMaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### zemberek - sentence tokenizer"
      ],
      "metadata": {
        "id": "pq5F61Nzrkyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = TurkishSentenceExtractor()\n",
        "sentences = extractor.from_paragraph(sample_doc)\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "q-qKOhXEqMgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### zemberek - word tokenizer"
      ],
      "metadata": {
        "id": "2SevqPXLrnh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zemberek import TurkishTokenizer\n",
        "from zemberek.tokenization.token import Token\n",
        "\n",
        "tokenizer = TurkishTokenizer.builder().accept_all().ignore_types(\n",
        "    [Token.Type.NewLine, \n",
        "     Token.Type.SpaceTab,\n",
        "     Token.Type.Punctuation]).build()\n",
        "\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  words = [token.content for token in tokens]\n",
        "  print(words)\n",
        "  print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "xjAVH0KeqMnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CpLURHkf6pOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZATION\n",
        "tokenizer = TurkishTokenizer.DEFAULT\n",
        "\n",
        "tokens = tokenizer.tokenize(\"Saat 12:00.\")\n",
        "for token in tokens:\n",
        "    print('Content = ', token.content)\n",
        "    print('Type = ', token.type_.name)\n",
        "    print('Start = ', token.start)\n",
        "    print('Stop = ', token.end, '\\n')"
      ],
      "metadata": {
        "id": "Aume7reB6pRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lower(): Dökümanı küçük harfe çevirir"
      ],
      "metadata": {
        "id": "gjdD1n_Q5e6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from unicode_tr import unicode_tr"
      ],
      "metadata": {
        "id": "MV-DnwEHtBlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_lower = [sentence.lower() for sentence in sentences]\n",
        "for sentence_lower in sentences_lower:\n",
        "  print(sentence_lower)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "R_AMnpt_lDP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_lower = [unicode_tr(sentence).lower() for sentence in sentences]\n",
        "for sentence_lower in sentences_lower:\n",
        "  print(sentence_lower)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "86MWdp8_lwnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clean_word(): "
      ],
      "metadata": {
        "id": "gUjtm42DC6Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_word(word):\n",
        "  unwanted_list = [u\"&bull;\", u\"&lsquo;\",u\",\",u\"?\",u\"!\",u'\"',u\"'\",u\"‘\",u\"’\",u\"/\",u\"<\",u\">\",u\"|\",u\"“\",\";\",\"&\",\"(\",\")\",\"=\",\"+\",\"-\",\"\\\\\",\"*\",\":\",\"~\",\"@\",\".\"]\n",
        "  alpha_list = [u\"a\",u\"b\",u\"c\",u\"ç\",u\"d\",u\"e\",u\"f\",u\"g\",u\"ğ\",u\"h\",u\"ı\",u\"i\",u\"j\",u\"k\",u\"l\",u\"m\",u\"n\",u\"o\",u\"ö\",u\"p\",u\"q\",u\"r\",u\"s\",u\"ş\",u\"t\",u\"u\",u\"ü\",u\"v\",u\"w\",u\"x\",u\"y\",u\"z\",\" \",]\n",
        "\n",
        "  # küçük harfe çeviriliyor\n",
        "  word = unicode_tr(word).lower()\n",
        "  txt = word.replace(u\"â\", u\"a\").lower()  # word.lower()\n",
        "\n",
        "  # farklı karakterler siliniyor\n",
        "  for unwanted_char in unwanted_list:\n",
        "    # txt = txt.replace(uw,' ')\n",
        "    # \"Ankara'da\" gibi tırnak kaldırıldığında \"Ankara da\" şeklinde oluşan aradaki boşluk birleştirildi.\n",
        "    txt = txt.replace(unwanted_char, \"\")\n",
        "\n",
        "  # geçerli harf listesi dışındaki harfler siliniyor\n",
        "  chars = list(set(txt))\n",
        "  for char in chars:\n",
        "    if not char in alpha_list:\n",
        "      txt = txt.replace(char, \"\")\n",
        "\n",
        "  return txt"
      ],
      "metadata": {
        "id": "Niu25FtI5ZW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_word(\"#yâpayzeka\")"
      ],
      "metadata": {
        "id": "7evwg_CQzHzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  for token in tokens:\n",
        "    print(clean_word(token.content))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "qPnjRwscm23t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### remove_URL: Dökümandan URL'leri kaldırın"
      ],
      "metadata": {
        "id": "E3IunhlqEG9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_URL(text):\n",
        "  # Remove URLs from a sample string\n",
        "  pattern_url = r\"\"\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))\"\"\"\n",
        "  text_clean = re.sub(pattern_url, \"\", text)\n",
        "  \n",
        "  return text_clean"
      ],
      "metadata": {
        "id": "AMfLywfVDoK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_lower = [remove_URL(sentence) for sentence in sentences]\n",
        "for sentence_lower in sentences_lower:\n",
        "  print(sentence_lower)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "5IpRdQkPnmnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### remove_email: Dökümanlardan e-posta adreslerini "
      ],
      "metadata": {
        "id": "4gwvL3VjFuV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_email(text):\n",
        "  pattern_email = \"\\S*@\\S*\\s?\"\n",
        "  text_clean = re.sub(pattern_email, \"\", text)\n",
        "  return text_clean\n"
      ],
      "metadata": {
        "id": "ksPgl7_v5ZZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_lower = [remove_email(sentence) for sentence in sentences]\n",
        "for sentence_lower in sentences_lower:\n",
        "  print(sentence_lower)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "a7bFfeJRnvgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5_kBJNu-5Zb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK - STOP WORDS"
      ],
      "metadata": {
        "id": "b3hOeRLB3Jy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_list = set(stopwords.words('turkish'))"
      ],
      "metadata": {
        "id": "qnxJFoEW5ZeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  words = [token.content for token in tokens]\n",
        "  print(words)\n",
        "\n",
        "  print(\"words_filtered\")\n",
        "  words_filtered = []\n",
        "  for w in words:\n",
        "    if w not in stopwords_list:\n",
        "      words_filtered.append(w)\n",
        "\n",
        "  print(words_filtered)\n",
        "  print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "kEh7Qtot3QjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J9SqYg6yw24_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zemberek - Normalizer"
      ],
      "metadata": {
        "id": "bkdFN_ZI4k1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zemberek import TurkishMorphology, TurkishSentenceNormalizer\n",
        "\n",
        "examples = [\"Yrn okua gidicem\",\n",
        "            \"Tmm, yarin havuza giricem ve aksama kadar yaticam :)\",\n",
        "            \"ah aynen ya annemde fark ettı siz evinizden cıkmayın diyo\",\n",
        "            \"gercek mı bu? Yuh! Artık unutulması bile beklenmiyo\",\n",
        "            \"Hayır hayat telaşm olmasa alacam buraları gökdelen dikicem.\",\n",
        "            \"yok hocam kesınlıkle oyle birşey yok\",\n",
        "            \"herseyi soyle hayatında olmaması gerek bence boyle ınsanların falan baskı yapıyosa\",\n",
        "            \"email adresim komecoglu.yavuz@gmail.com\",\n",
        "            \"Kredi başvrusu yapmk istiyrum.\",\n",
        "            \"Bankanizin hesp blgilerini ogrenmek istyorum.\"]\n",
        "\n",
        "morphology = TurkishMorphology.create_with_defaults()\n",
        "\n",
        "normalizer = TurkishSentenceNormalizer(morphology)"
      ],
      "metadata": {
        "id": "RsfZ5BQu4o7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example in examples:\n",
        "    print(example)\n",
        "    print(normalizer.normalize(example), \"\\n\")"
      ],
      "metadata": {
        "id": "0oVW13Fx5Brv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zemberek - SpellChecker"
      ],
      "metadata": {
        "id": "UFccuMVw5Ev2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zemberek import TurkishSpellChecker\n",
        "\n",
        "sc = TurkishSpellChecker(morphology)"
      ],
      "metadata": {
        "id": "H0cPFhXg4-8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "li = [\"okuyablirim\", \"tartısıyor\", \"Ankar'ada\", \"knlıca\", \"yapablrim\", \"kıredi\", \"geldm\", \"geliyom\", \"aldm\", \"asln\"]\n",
        "start = time.time()\n",
        "for word in li:\n",
        "  print(word + \" = \" + ' '.join(sc.suggest_for_word(word)))"
      ],
      "metadata": {
        "id": "oWYYmbuz5hr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zemberek - MORPHOLOGICAL ANALYSIS"
      ],
      "metadata": {
        "id": "RhJPf4lb6NEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SINGLE WORD MORPHOLOGICAL ANALYSIS\n",
        "results = morphology.analyze(\"kalemin\")\n",
        "for result in results:\n",
        "    print(result)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "xqkskK8R6NWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = morphology.analyze(\"yazlık\")\n",
        "for result in results:\n",
        "    print(result)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "oDSPXGY48Jtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zemberek - Stemming"
      ],
      "metadata": {
        "id": "xAu9UXbN7LlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_analyze = morphology.analyze(\"görgüsüzlük\")\n",
        "if len(word_analyze.analysis_results) > 0:\n",
        "  word_stem = word_analyze.analysis_results[0]\n",
        "  print(word_stem.get_stem())\n",
        "else:\n",
        "  print(word)"
      ],
      "metadata": {
        "id": "Zkxm-B9K6YyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_analyze = morphology.analyze(\"yazlık\")\n",
        "if len(word_analyze.analysis_results) > 0:\n",
        "  word_stem = word_analyze.analysis_results[0]\n",
        "  print(word_stem.get_stem())\n",
        "else:\n",
        "  print(word)"
      ],
      "metadata": {
        "id": "Im76rdIk8uGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k5m7JoO-5YOp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}