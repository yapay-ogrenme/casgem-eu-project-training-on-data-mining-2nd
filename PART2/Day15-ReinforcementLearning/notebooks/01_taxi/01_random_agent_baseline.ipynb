{"cells":[{"cell_type":"markdown","id":"4c2ff31f","metadata":{"id":"4c2ff31f"},"source":["# 01 Random agent baseline"]},{"cell_type":"markdown","id":"ba09a498","metadata":{"id":"ba09a498"},"source":["#### üëâBir Peki≈ütirmeli √ñƒürenme problemini √ß√∂zmeye √ßalƒ±≈ümadan √∂nce, onun zorluƒüunu anlamalƒ±yƒ±z.\n","\n","#### üëâ Bunu yapmak i√ßin, g√∂revi √ßok fazla kafa yormadan ger√ßekle≈ütirebilecek ve performansƒ±nƒ± deƒüerlendirebileceƒüimiz kukla bir ajan tasarlamamƒ±z gerekir.\n","\n","#### üëâBunu yapmanƒ±n basit bir yolu, ortamƒ±n mevcut durumuna dikkat etmeden bir sonraki eylemini rastgele se√ßen bir Rastgele Ajan (Random Agent) kullanmaktƒ±r.\n","\n","**Temel model olarak rastgele bir ajan kullanalƒ±m.**"]},{"cell_type":"code","execution_count":null,"id":"e3629346","metadata":{"id":"e3629346"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","%pylab inline\n","%config InlineBackend.figure_format = 'svg'"]},{"cell_type":"markdown","id":"76e9a06d","metadata":{"id":"76e9a06d"},"source":["## √áevre (Environment) üåé"]},{"cell_type":"code","execution_count":null,"id":"ebfba291","metadata":{"id":"ebfba291"},"outputs":[],"source":["import gym\n","env = gym.make(\"Taxi-v3\").env"]},{"cell_type":"markdown","id":"52a806a2","metadata":{"id":"52a806a2"},"source":["## Rastgele Ajan (Random agent) ü§ñüç∑"]},{"cell_type":"code","execution_count":null,"id":"a44f5647","metadata":{"id":"a44f5647"},"outputs":[],"source":["class RandomAgent:\n","    \"\"\"\n","    This taxi driver selects actions randomly.\n","    You better not get into this taxi!\n","    \"\"\"\n","    def __init__(self, env):\n","        self.env = env\n","\n","    def get_action(self, state) -> int:\n","        \"\"\"\n","        We have `state` as an input to keep\n","        a consistent API for all our agents, but it\n","        is not used.\n","        \n","        i.e. The agent does not consider the state of\n","        the environment when deciding what to do next.\n","        This is why we call it \"random\".\n","        \"\"\"\n","        return self.env.action_space.sample()\n","\n","agent = RandomAgent(env)"]},{"cell_type":"markdown","id":"be5af56b","metadata":{"id":"be5af56b"},"source":["## Bu s√ºr√ºc√ºy√º sabit bir durumdan (`state = 123`) ba≈ülayarak deƒüerlendirelim"]},{"cell_type":"code","execution_count":null,"id":"52c11f92","metadata":{"id":"52c11f92"},"outputs":[],"source":["# set initial state of the environment\n","env.reset()\n","state = 123\n","env.s = state\n","\n","epochs = 0\n","penalties = 0  # wrong pick up or dropp off\n","reward = 0\n","\n","# store frames to latter plot them\n","frames = []\n","\n","done = False\n","\n","while not done:\n","    \n","    action = agent.get_action(state)\n","    \n","    state, reward, done, info = env.step(action)\n","\n","    if reward == -10:\n","        penalties += 1\n","    \n","    frames.append({\n","        'frame': env.render(mode='ansi'),\n","        'state': state,\n","        'action': action,\n","        'reward': reward\n","        }\n","    )\n","\n","    epochs += 1\n","    \n","    \n","print(\"Timesteps taken: {}\".format(epochs))\n","print(\"Penalties incurred: {}\".format(penalties))"]},{"cell_type":"code","execution_count":null,"id":"dbeebb90","metadata":{"id":"dbeebb90"},"outputs":[],"source":["from IPython.display import clear_output\n","from time import sleep\n","\n","def print_frames(frames):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'])\n","        print(f\"Timestep: {i + 1} of {len(frames)}\")\n","        print(f\"State: {frame['state']}\")\n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        sleep(.01)\n","        \n","print_frames(frames)"]},{"cell_type":"markdown","id":"724205e2","metadata":{"id":"724205e2"},"source":["#### Olduk√ßa k√∂t√º bir s√ºr√º≈ü, deƒüil mi?"]},{"cell_type":"markdown","id":"7de99f7e","metadata":{"id":"7de99f7e"},"source":["## Performansƒ± √∂l√ßmek i√ßin histogramlar olu≈üturalƒ±m\n","\n","Daha temsili bir performans √∂l√ß√ºs√º elde etmek i√ßin, her seferinde rastgele bir durumda ba≈ülayarak aynƒ± deƒüerlendirme d√∂ng√ºs√ºn√º n=100 kez tekrarlayabiliriz."]},{"cell_type":"code","execution_count":null,"id":"26de2a7d","metadata":{"id":"26de2a7d"},"outputs":[],"source":["from tqdm import tqdm\n","\n","n_episodes = 100\n","\n","# For plotting metrics\n","timesteps_per_episode = []\n","penalties_per_episode = []\n","\n","for i in tqdm(range(0, n_episodes)):\n","    \n","    # reset environment to a random state\n","    state = env.reset()\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","    \n","    while not done:\n","        \n","        action = agent.get_action(state)       \n","        next_state, reward, done, info = env.step(action) \n","               \n","        if reward == -10:\n","            penalties += 1\n","\n","        state = next_state\n","        epochs += 1\n","    \n","    timesteps_per_episode.append(epochs)\n","    penalties_per_episode.append(penalties)"]},{"cell_type":"markdown","source":["### **timesteps_per_episode ve notice_per_episode'u √ßizerseniz**, aracƒ± daha fazla b√∂l√ºm tamamladƒ±k√ßa bunlarƒ±n hi√ßbirinin azalmadƒ±ƒüƒ±nƒ± g√∂zlemleyebilirsiniz. Ba≈üka bir deyi≈üle, **temsilci hi√ßbir ≈üey √ñƒûRENMƒ∞YOR.**"],"metadata":{"id":"fMfSuYJ2hnTz"},"id":"fMfSuYJ2hnTz"},{"cell_type":"code","execution_count":null,"id":"20149669","metadata":{"id":"20149669"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots(figsize = (12, 4))\n","ax.set_title(\"Timesteps to complete ride\")    \n","pd.Series(timesteps_per_episode).plot(kind=\"line\")\n","plt.show()\n","\n","fig, ax = plt.subplots(figsize = (12, 4))\n","ax.set_title(\"Penalties per ride\")    \n","pd.Series(penalties_per_episode).plot(kind=\"line\")\n","plt.show()"]},{"cell_type":"markdown","source":["### Performansƒ±n √∂zet istatistiklerini istiyorsanƒ±z, ortalamalarƒ± alabilirsiniz:"],"metadata":{"id":"RrS79TYth5AN"},"id":"RrS79TYth5AN"},{"cell_type":"code","execution_count":null,"id":"ee7baa61","metadata":{"id":"ee7baa61"},"outputs":[],"source":["print(f'Avg steps to complete ride: {np.array(timesteps_per_episode).mean()}')\n","print(f'Avg penalties to complete ride: {np.array(penalties_per_episode).mean()}')"]},{"cell_type":"code","execution_count":null,"id":"28c67b86","metadata":{"id":"28c67b86"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"01_random_agent_baseline.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}