{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f0fd6807",
      "metadata": {
        "id": "f0fd6807"
      },
      "source": [
        "# 02 Q-agent\n",
        "\n",
        "[Q-öğrenme (Q-learning)](https://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf)  ([Chris Walkins](www.cs.rhul.ac.uk/~chrisw/) ve  [Peter Dayan](https://en.wikipedia.org/wiki/Peter_Dayan) tarafından) optimal q-değeri fonksiyonunu bulan bir algoritmadır.\n",
        "\n",
        "Kısım 1'de söylediğimiz gibi, bir π politikasıyla ilişkili **q-değer fonksiyonu Q(s, a), ajanın s durumunda a eylemini yaptığı ve ardından π politikasını izlediği zaman ajanın almayı beklediği toplam ödüldür.**\n",
        "\n",
        "Optimum q-değer fonksiyonu **Q*(s, a)**, optimal politika π* ile ilişkili q-değer fonksiyonudur.\n",
        "\n",
        "**Q*(s, a)**'yı biliyorsanız, π*: sonucunu çıkarabilirsiniz, yani bir sonraki eylem olarak mevcut s durumu için Q*(s, a)'yı maksimize eden eylemi seçersiniz.\n",
        "\n",
        "Q-öğrenme, keyfi bir başlangıç **Q⁰(s, a)** tahmininden başlayarak, optimal q-değer fonksiyonu **Q*(s, a)** için daha iyi ve daha iyi yaklaşımları hesaplamak için yinelemeli bir algoritmadır.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1260/0*323DNjYPk23v3Mpq.png\" width=500px/>\n",
        "\n",
        "Sonlu sayıda durum ve eylem içeren `Taxi-v3` gibi bir tablo ortamında, bir **q işlevi esasen bir matristir**. **Durumlar kadar satır ve eylemler kadar sütun içerir, yani 500 x 6**.\n",
        "\n",
        "Tamam, ama Q⁰(s, a)'dan sonraki Q¹(s, a) yaklaşımını tam olarak nasıl hesaplarsınız?\n",
        "\n",
        "Bu, Q-öğrenmedeki anahtar formüldür:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1260/1*ODDuOVNWybX6g0CI4i2Uiw.png\" width=500px/>\n",
        "\n",
        "\n",
        "q-agent çevrede gezinirken ve bir sonraki s' durumunu ve r ödülünü gözlemledikçe, q-değer matrisinizi bu formülle güncellersiniz.\n",
        "\n",
        "## *Bu formüldeki öğrenme oranı 𝛼 nedir?*\n",
        "\n",
        "**Öğrenme oranı (learning rate)** (makine öğreniminde her zamanki gibi), q-fonksiyonu güncellemelerin ne kadar büyük olduğunu kontrol eden küçük bir sayıdır. Ayarlamanız gerekir, çünkü çok büyük bir değer dengesiz eğitime neden olur ve çok küçük yerel minimumlardan kaçmak için yeterli olmayabilir.\n",
        "\n",
        "## *Ve bu indirim faktörü 𝛾?*\n",
        "\n",
        "**İndirim faktörü (discount factor)**, ajanımızın yakın gelecekteki ödüllere göre uzak gelecekteki ödülleri ne kadar önemsediğini belirleyen 0 ile 1 arasında bir (hiper) parametredir.\n",
        "\n",
        "- 𝛾=0 olduğunda, temsilci yalnızca anında ödülü maksimize etmekle ilgilenir. Hayatta olduğu gibi, anında ödülü en üst düzeye çıkarmak, optimal uzun vadeli sonuçlar için en iyi reçete değildir. Bu, RL aracılarında da olur.\n",
        "\n",
        "- 𝛾=1 olduğunda, temsilci her eylemini gelecekteki tüm ödüllerinin toplamına göre değerlendirir. Bu durumda temsilci, anlık ödülleri ve gelecekteki ödülleri eşit olarak değerlendirir.\n",
        "\n",
        "İndirim faktörü tipik olarak bir ara değerdir, örn. 0.6.\n",
        "\n",
        "## Özetle, eğer\n",
        "\n",
        "- yeterince uzun eğitim\n",
        "- iyi bir öğrenme oranı ve indirim faktörü ile\n",
        "- ve ajan durum uzayını yeterince araştırıyor\n",
        "- ve q-değer matrisini Q-öğrenme formülüyle güncellersiniz\n",
        "\n",
        "ilk yaklaşımınız sonunda optimal q-matrise yakınsar. işte!\n",
        "\n",
        "O zaman bir Q-agent için bir Python sınıfı uygulayalım.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abcf6613",
      "metadata": {
        "id": "abcf6613"
      },
      "source": [
        "👉Akıllı bir taksi şoförü yetiştirmek için Q-learning'i kullanalım.\n",
        "\n",
        "👉Akıllı bir taksi şoförü ceza almamalı (yani kaza yapmamalı) ve sürüşü tamamlamak için gereken zaman adımlarını en aza indirmelidir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3629346",
      "metadata": {
        "id": "e3629346"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%pylab inline\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e9a06d",
      "metadata": {
        "id": "76e9a06d"
      },
      "source": [
        "## Environment 🌎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebfba291",
      "metadata": {
        "id": "ebfba291"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "env = gym.make(\"Taxi-v3\").env"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9e7c87",
      "metadata": {
        "id": "8e9e7c87"
      },
      "source": [
        "## Q-agent 🤖🧠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ee8884",
      "metadata": {
        "id": "a5ee8884"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class QAgent:\n",
        "\n",
        "    def __init__(self, env, alpha, gamma):\n",
        "        self.env = env\n",
        "\n",
        "        # table with q-values: n_states * n_actions\n",
        "        self.q_table = np.zeros([env.observation_space.n,\n",
        "                                 env.action_space.n])\n",
        "\n",
        "        # hyper-parameters\n",
        "        self.alpha = alpha  # learning rate\n",
        "        self.gamma = gamma  # discount factor\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\"\"\"\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update_parameters(self, state, action, reward, next_state):\n",
        "        \"\"\"\"\"\"\n",
        "        # Q-learning formula\n",
        "        old_value = self.q_table[state, action]\n",
        "        next_max = np.max(self.q_table[next_state])\n",
        "        new_value = \\\n",
        "            old_value + \\\n",
        "            self.alpha * (reward + self.gamma * next_max - old_value)\n",
        "\n",
        "        # update the q_table\n",
        "        self.q_table[state, action] = new_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13436308",
      "metadata": {
        "id": "13436308"
      },
      "outputs": [],
      "source": [
        "# hyper-parameters (hiper parametreler)\n",
        "\n",
        "# RL problems are full of these hyper-parameters. \n",
        "# (RL problemleri bu hiper parametrelerle doludur.)\n",
        "\n",
        "# For the moment, trust me when I set these values.\n",
        "# We will later play with these and see how they impact learning.\n",
        "\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "\n",
        "agent = QAgent(env, alpha, gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "851dfc0a",
      "metadata": {
        "id": "851dfc0a"
      },
      "source": [
        "## Training loop 🎡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e08672d",
      "metadata": {
        "id": "5e08672d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# exploration vs exploitation prob\n",
        "epsilon = 0.1\n",
        "\n",
        "n_episodes = 10000\n",
        "\n",
        "# For plotting metrics\n",
        "timesteps_per_episode = []\n",
        "penalties_per_episode = []\n",
        "\n",
        "\n",
        "for i in tqdm(range(0, n_episodes)):\n",
        "    \n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        \n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Explore action space\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Exploit learned values\n",
        "            action = agent.get_action(state)\n",
        "        \n",
        "        next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "        agent.update_parameters(state, action, reward, next_state)\n",
        "        \n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "    timesteps_per_episode.append(epochs)\n",
        "    penalties_per_episode.append(penalties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218461f8",
      "metadata": {
        "id": "218461f8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12, 4))\n",
        "ax.set_title(\"Timesteps to complete ride\")    \n",
        "pd.Series(timesteps_per_episode).plot(kind='line')\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12, 4))\n",
        "ax.set_title(\"Penalties per ride\")    \n",
        "pd.Series(penalties_per_episode).plot(kind='line')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1bfec95",
      "metadata": {
        "id": "f1bfec95"
      },
      "source": [
        "## Sabit bir `state = 123`'ten başlayarak bu sürücüyü değerlendirelim (evaluate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b1ee76",
      "metadata": {
        "id": "f1b1ee76"
      },
      "outputs": [],
      "source": [
        "# set initial state of the environment\n",
        "state = 123\n",
        "env.s = state\n",
        "\n",
        "epochs = 0\n",
        "penalties = 0\n",
        "reward = 0\n",
        "\n",
        "# store frames to latter plot them\n",
        "frames = []\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    action = agent.get_action(state)\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    agent.update_parameters(state, action, reward, next_state)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    \n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85d12b7",
      "metadata": {
        "id": "e85d12b7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1} of {len(frames)}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b78f92",
      "metadata": {
        "id": "c9b78f92"
      },
      "source": [
        "## Bu eğitimli ajanı 100 bölüm üzerinden değerlendirelim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dfb254",
      "metadata": {
        "id": "b8dfb254"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# exploration vs exploitation prob\n",
        "epsilon = 0.05\n",
        "\n",
        "n_episodes = 100\n",
        "\n",
        "# For plotting metrics\n",
        "timesteps_per_episode = []\n",
        "penalties_per_episode = []\n",
        "\n",
        "\n",
        "for i in tqdm(range(0, n_episodes)):\n",
        "    \n",
        "    state = env.reset()      \n",
        "    \n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        \n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Explore action space\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Exploit learned values\n",
        "            action = agent.get_action(state)\n",
        "        \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        agent.update_parameters(state, action, reward, next_state)\n",
        "                      \n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "            \n",
        "    timesteps_per_episode.append(epochs)\n",
        "    penalties_per_episode.append(penalties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe0b7e7",
      "metadata": {
        "id": "bbe0b7e7"
      },
      "outputs": [],
      "source": [
        "print(f'Avg steps to complete ride: {np.array(timesteps_per_episode).mean()}')\n",
        "print(f'Avg penalties to complete ride: {np.array(penalties_per_episode).mean()}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1-b6Db0Gn0rH"
      },
      "id": "1-b6Db0Gn0rH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "02_q_agent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}