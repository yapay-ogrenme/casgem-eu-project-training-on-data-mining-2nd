{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f0fd6807",
      "metadata": {
        "id": "f0fd6807"
      },
      "source": [
        "# 02 Q-agent\n",
        "\n",
        "[Q-Ã¶ÄŸrenme (Q-learning)](https://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf)  ([Chris Walkins](www.cs.rhul.ac.uk/~chrisw/) ve  [Peter Dayan](https://en.wikipedia.org/wiki/Peter_Dayan) tarafÄ±ndan) optimal q-deÄŸeri fonksiyonunu bulan bir algoritmadÄ±r.\n",
        "\n",
        "KÄ±sÄ±m 1'de sÃ¶ylediÄŸimiz gibi, bir Ï€ politikasÄ±yla iliÅŸkili **q-deÄŸer fonksiyonu Q(s, a), ajanÄ±n s durumunda a eylemini yaptÄ±ÄŸÄ± ve ardÄ±ndan Ï€ politikasÄ±nÄ± izlediÄŸi zaman ajanÄ±n almayÄ± beklediÄŸi toplam Ã¶dÃ¼ldÃ¼r.**\n",
        "\n",
        "Optimum q-deÄŸer fonksiyonu **Q*(s, a)**, optimal politika Ï€* ile iliÅŸkili q-deÄŸer fonksiyonudur.\n",
        "\n",
        "**Q*(s, a)**'yÄ± biliyorsanÄ±z, Ï€*: sonucunu Ã§Ä±karabilirsiniz, yani bir sonraki eylem olarak mevcut s durumu iÃ§in Q*(s, a)'yÄ± maksimize eden eylemi seÃ§ersiniz.\n",
        "\n",
        "Q-Ã¶ÄŸrenme, keyfi bir baÅŸlangÄ±Ã§ **Qâ°(s, a)** tahmininden baÅŸlayarak, optimal q-deÄŸer fonksiyonu **Q*(s, a)** iÃ§in daha iyi ve daha iyi yaklaÅŸÄ±mlarÄ± hesaplamak iÃ§in yinelemeli bir algoritmadÄ±r.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1260/0*323DNjYPk23v3Mpq.png\" width=500px/>\n",
        "\n",
        "Sonlu sayÄ±da durum ve eylem iÃ§eren `Taxi-v3` gibi bir tablo ortamÄ±nda, bir **q iÅŸlevi esasen bir matristir**. **Durumlar kadar satÄ±r ve eylemler kadar sÃ¼tun iÃ§erir, yani 500 x 6**.\n",
        "\n",
        "Tamam, ama Qâ°(s, a)'dan sonraki QÂ¹(s, a) yaklaÅŸÄ±mÄ±nÄ± tam olarak nasÄ±l hesaplarsÄ±nÄ±z?\n",
        "\n",
        "Bu, Q-Ã¶ÄŸrenmedeki anahtar formÃ¼ldÃ¼r:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1260/1*ODDuOVNWybX6g0CI4i2Uiw.png\" width=500px/>\n",
        "\n",
        "\n",
        "q-agent Ã§evrede gezinirken ve bir sonraki s' durumunu ve r Ã¶dÃ¼lÃ¼nÃ¼ gÃ¶zlemledikÃ§e, q-deÄŸer matrisinizi bu formÃ¼lle gÃ¼ncellersiniz.\n",
        "\n",
        "## *Bu formÃ¼ldeki Ã¶ÄŸrenme oranÄ± ğ›¼ nedir?*\n",
        "\n",
        "**Ã–ÄŸrenme oranÄ± (learning rate)** (makine Ã¶ÄŸreniminde her zamanki gibi), q-fonksiyonu gÃ¼ncellemelerin ne kadar bÃ¼yÃ¼k olduÄŸunu kontrol eden kÃ¼Ã§Ã¼k bir sayÄ±dÄ±r. AyarlamanÄ±z gerekir, Ã§Ã¼nkÃ¼ Ã§ok bÃ¼yÃ¼k bir deÄŸer dengesiz eÄŸitime neden olur ve Ã§ok kÃ¼Ã§Ã¼k yerel minimumlardan kaÃ§mak iÃ§in yeterli olmayabilir.\n",
        "\n",
        "## *Ve bu indirim faktÃ¶rÃ¼ ğ›¾?*\n",
        "\n",
        "**Ä°ndirim faktÃ¶rÃ¼ (discount factor)**, ajanÄ±mÄ±zÄ±n yakÄ±n gelecekteki Ã¶dÃ¼llere gÃ¶re uzak gelecekteki Ã¶dÃ¼lleri ne kadar Ã¶nemsediÄŸini belirleyen 0 ile 1 arasÄ±nda bir (hiper) parametredir.\n",
        "\n",
        "- ğ›¾=0 olduÄŸunda, temsilci yalnÄ±zca anÄ±nda Ã¶dÃ¼lÃ¼ maksimize etmekle ilgilenir. Hayatta olduÄŸu gibi, anÄ±nda Ã¶dÃ¼lÃ¼ en Ã¼st dÃ¼zeye Ã§Ä±karmak, optimal uzun vadeli sonuÃ§lar iÃ§in en iyi reÃ§ete deÄŸildir. Bu, RL aracÄ±larÄ±nda da olur.\n",
        "\n",
        "- ğ›¾=1 olduÄŸunda, temsilci her eylemini gelecekteki tÃ¼m Ã¶dÃ¼llerinin toplamÄ±na gÃ¶re deÄŸerlendirir. Bu durumda temsilci, anlÄ±k Ã¶dÃ¼lleri ve gelecekteki Ã¶dÃ¼lleri eÅŸit olarak deÄŸerlendirir.\n",
        "\n",
        "Ä°ndirim faktÃ¶rÃ¼ tipik olarak bir ara deÄŸerdir, Ã¶rn. 0.6.\n",
        "\n",
        "## Ã–zetle, eÄŸer\n",
        "\n",
        "- yeterince uzun eÄŸitim\n",
        "- iyi bir Ã¶ÄŸrenme oranÄ± ve indirim faktÃ¶rÃ¼ ile\n",
        "- ve ajan durum uzayÄ±nÄ± yeterince araÅŸtÄ±rÄ±yor\n",
        "- ve q-deÄŸer matrisini Q-Ã¶ÄŸrenme formÃ¼lÃ¼yle gÃ¼ncellersiniz\n",
        "\n",
        "ilk yaklaÅŸÄ±mÄ±nÄ±z sonunda optimal q-matrise yakÄ±nsar. iÅŸte!\n",
        "\n",
        "O zaman bir Q-agent iÃ§in bir Python sÄ±nÄ±fÄ± uygulayalÄ±m.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abcf6613",
      "metadata": {
        "id": "abcf6613"
      },
      "source": [
        "ğŸ‘‰AkÄ±llÄ± bir taksi ÅŸofÃ¶rÃ¼ yetiÅŸtirmek iÃ§in Q-learning'i kullanalÄ±m.\n",
        "\n",
        "ğŸ‘‰AkÄ±llÄ± bir taksi ÅŸofÃ¶rÃ¼ ceza almamalÄ± (yani kaza yapmamalÄ±) ve sÃ¼rÃ¼ÅŸÃ¼ tamamlamak iÃ§in gereken zaman adÄ±mlarÄ±nÄ± en aza indirmelidir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3629346",
      "metadata": {
        "id": "e3629346"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%pylab inline\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e9a06d",
      "metadata": {
        "id": "76e9a06d"
      },
      "source": [
        "## Environment ğŸŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebfba291",
      "metadata": {
        "id": "ebfba291"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "env = gym.make(\"Taxi-v3\").env"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9e7c87",
      "metadata": {
        "id": "8e9e7c87"
      },
      "source": [
        "## Q-agent ğŸ¤–ğŸ§ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ee8884",
      "metadata": {
        "id": "a5ee8884"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class QAgent:\n",
        "\n",
        "    def __init__(self, env, alpha, gamma):\n",
        "        self.env = env\n",
        "\n",
        "        # table with q-values: n_states * n_actions\n",
        "        self.q_table = np.zeros([env.observation_space.n,\n",
        "                                 env.action_space.n])\n",
        "\n",
        "        # hyper-parameters\n",
        "        self.alpha = alpha  # learning rate\n",
        "        self.gamma = gamma  # discount factor\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\"\"\"\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update_parameters(self, state, action, reward, next_state):\n",
        "        \"\"\"\"\"\"\n",
        "        # Q-learning formula\n",
        "        old_value = self.q_table[state, action]\n",
        "        next_max = np.max(self.q_table[next_state])\n",
        "        new_value = \\\n",
        "            old_value + \\\n",
        "            self.alpha * (reward + self.gamma * next_max - old_value)\n",
        "\n",
        "        # update the q_table\n",
        "        self.q_table[state, action] = new_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13436308",
      "metadata": {
        "id": "13436308"
      },
      "outputs": [],
      "source": [
        "# hyper-parameters (hiper parametreler)\n",
        "\n",
        "# RL problems are full of these hyper-parameters. \n",
        "# (RL problemleri bu hiper parametrelerle doludur.)\n",
        "\n",
        "# For the moment, trust me when I set these values.\n",
        "# We will later play with these and see how they impact learning.\n",
        "\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "\n",
        "agent = QAgent(env, alpha, gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "851dfc0a",
      "metadata": {
        "id": "851dfc0a"
      },
      "source": [
        "## Training loop ğŸ¡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e08672d",
      "metadata": {
        "id": "5e08672d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# exploration vs exploitation prob\n",
        "epsilon = 0.1\n",
        "\n",
        "n_episodes = 10000\n",
        "\n",
        "# For plotting metrics\n",
        "timesteps_per_episode = []\n",
        "penalties_per_episode = []\n",
        "\n",
        "\n",
        "for i in tqdm(range(0, n_episodes)):\n",
        "    \n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        \n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Explore action space\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Exploit learned values\n",
        "            action = agent.get_action(state)\n",
        "        \n",
        "        next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "        agent.update_parameters(state, action, reward, next_state)\n",
        "        \n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "    timesteps_per_episode.append(epochs)\n",
        "    penalties_per_episode.append(penalties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218461f8",
      "metadata": {
        "id": "218461f8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12, 4))\n",
        "ax.set_title(\"Timesteps to complete ride\")    \n",
        "pd.Series(timesteps_per_episode).plot(kind='line')\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12, 4))\n",
        "ax.set_title(\"Penalties per ride\")    \n",
        "pd.Series(penalties_per_episode).plot(kind='line')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1bfec95",
      "metadata": {
        "id": "f1bfec95"
      },
      "source": [
        "## Sabit bir `state = 123`'ten baÅŸlayarak bu sÃ¼rÃ¼cÃ¼yÃ¼ deÄŸerlendirelim (evaluate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b1ee76",
      "metadata": {
        "id": "f1b1ee76"
      },
      "outputs": [],
      "source": [
        "# set initial state of the environment\n",
        "state = 123\n",
        "env.s = state\n",
        "\n",
        "epochs = 0\n",
        "penalties = 0\n",
        "reward = 0\n",
        "\n",
        "# store frames to latter plot them\n",
        "frames = []\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    action = agent.get_action(state)\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    agent.update_parameters(state, action, reward, next_state)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    \n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85d12b7",
      "metadata": {
        "id": "e85d12b7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1} of {len(frames)}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b78f92",
      "metadata": {
        "id": "c9b78f92"
      },
      "source": [
        "## Bu eÄŸitimli ajanÄ± 100 bÃ¶lÃ¼m Ã¼zerinden deÄŸerlendirelim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dfb254",
      "metadata": {
        "id": "b8dfb254"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# exploration vs exploitation prob\n",
        "epsilon = 0.05\n",
        "\n",
        "n_episodes = 100\n",
        "\n",
        "# For plotting metrics\n",
        "timesteps_per_episode = []\n",
        "penalties_per_episode = []\n",
        "\n",
        "\n",
        "for i in tqdm(range(0, n_episodes)):\n",
        "    \n",
        "    state = env.reset()      \n",
        "    \n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        \n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Explore action space\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Exploit learned values\n",
        "            action = agent.get_action(state)\n",
        "        \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        agent.update_parameters(state, action, reward, next_state)\n",
        "                      \n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "            \n",
        "    timesteps_per_episode.append(epochs)\n",
        "    penalties_per_episode.append(penalties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe0b7e7",
      "metadata": {
        "id": "bbe0b7e7"
      },
      "outputs": [],
      "source": [
        "print(f'Avg steps to complete ride: {np.array(timesteps_per_episode).mean()}')\n",
        "print(f'Avg penalties to complete ride: {np.array(penalties_per_episode).mean()}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1-b6Db0Gn0rH"
      },
      "id": "1-b6Db0Gn0rH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "02_q_agent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}