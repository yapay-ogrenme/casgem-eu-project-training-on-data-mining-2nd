{"cells":[{"cell_type":"markdown","id":"f0fd6807","metadata":{"id":"f0fd6807"},"source":["# 03 Q-agent hiperparametre analizi"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"id":"Hy27IAC8qcyU"},"id":"Hy27IAC8qcyU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls '/content/drive/MyDrive/CASGEM-Egitim/Egitim-Part2/Day15-ReinforcementLearning/notebooks/01_taxi'"],"metadata":{"id":"LSCTqkF5qdtu"},"id":"LSCTqkF5qdtu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/CASGEM-Egitim/Egitim-Part2/Day15-ReinforcementLearning/notebooks/01_taxi')"],"metadata":{"id":"pCfejzEuq5oT"},"id":"pCfejzEuq5oT","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"abcf6613","metadata":{"id":"abcf6613"},"source":["#### 👉RL ajanları hiper parametrelere son derece duyarlıdır.\n","\n","#### 👉Hiper parametrelerin öğrenmeyi nasıl etkilediklerine dair daha iyi bir sezgi kazanmak için onlarla sistematik bir şekilde oynayalım.\n","\n","**Alfa (learning rate) ve gama (discount factor) için farklı değerler kullanarak q-agent'ı eğitelim. Epsilon'a gelince, %10'da tutuyoruz.**\n","\n","**Kodu temiz tutmak için q-agent tanımını src/q_agent.py içine ve eğitim döngüsünü\n","src/loops.py içindeki train() işlevinin içine yerleştirebiliriz.**\n"]},{"cell_type":"code","execution_count":null,"id":"e3629346","metadata":{"id":"e3629346"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","%pylab inline\n","%config InlineBackend.figure_format = 'svg'"]},{"cell_type":"markdown","id":"76e9a06d","metadata":{"id":"76e9a06d"},"source":["## Çevre (Environment) 🌎"]},{"cell_type":"code","execution_count":null,"id":"ebfba291","metadata":{"id":"ebfba291"},"outputs":[],"source":["import gym\n","env = gym.make(\"Taxi-v3\").env"]},{"cell_type":"markdown","id":"8e9e7c87","metadata":{"id":"8e9e7c87"},"source":["## Q-agent 🤖🧠"]},{"cell_type":"code","execution_count":null,"id":"a5ee8884","metadata":{"id":"a5ee8884"},"outputs":[],"source":["# No need to copy paste the same QAgent\n","# definition in every notebook, don't you think?\n","from src.q_agent import QAgent\n","\n","# hyper-parameters\n","# RL problems are full of these hyper-parameters.\n","# For the moment, trust me when I set these values.\n","# We will later play with these and see how they impact learning.\n","alphas = [0.01, 0.1, 1]\n","gammas = [0.1, 0.6, 0.9]"]},{"cell_type":"markdown","id":"0209ba20","metadata":{"id":"0209ba20"},"source":["## Training loop 🎡"]},{"cell_type":"code","execution_count":null,"id":"5e08672d","metadata":{"id":"5e08672d"},"outputs":[],"source":["import pandas as pd\n","\n","from src.loops import train\n","\n","# exploration vs exploitation prob\n","# let's start with a constant probability of 10%.\n","epsilon = 0.1\n","n_episodes = 1000\n","\n","results = pd.DataFrame()\n","for alpha in alphas:\n","    for gamma in gammas:\n","        \n","        print(f'alpha: {alpha}, gamma: {gamma}')\n","        agent = QAgent(env, alpha, gamma)\n","        \n","        _, timesteps, penalties = train(agent,\n","                                        env,\n","                                        n_episodes,\n","                                        epsilon)\n","        \n","        # collect timesteps and penalties for this pair\n","        # of hyper-parameters (alpha, gamma)\n","        results_ = pd.DataFrame()\n","        results_['timesteps'] = timesteps\n","        results_['penalties'] = penalties\n","        results_['alpha'] = alpha\n","        results_['gamma'] = gamma\n","        results = pd.concat([results, results_])\n","\n","# index -> episode\n","results = results.reset_index().rename(\n","    columns={'index': 'episode'})\n","\n","# add column with the 2 hyper-parameters\n","results['hyperparameters'] = [\n","    f'alpha={a}, gamma={g}'\n","    for (a, g) in zip(results['alpha'], results['gamma'])\n","]"]},{"cell_type":"markdown","source":["**Her bir hiper parametre kombinasyonu için episode başına zaman adımlarını çizelim.**"],"metadata":{"id":"DB9fd2RymDs0"},"id":"DB9fd2RymDs0"},{"cell_type":"code","execution_count":null,"id":"e975b843","metadata":{"id":"e975b843"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","fig = plt.gcf()\n","fig.set_size_inches(12, 8)\n","sns.lineplot('episode', 'timesteps',\n","             hue='hyperparameters', data=results)"]},{"cell_type":"markdown","id":"4769fa4e","metadata":{"id":"4769fa4e"},"source":["Grafik iddialı görünüyor ama biraz fazla gürültülü.\n","\n","**Yine de gözlemleyebileceğimiz şey, alfa = 0.01 olduğunda öğrenmenin daha yavaş olduğudur. alfa (öğrenme hızı), her yinelemede q değerlerini ne kadar güncellediğimizi kontrol eder. Değerin çok küçük olması daha yavaş öğrenme anlamına gelir.**\n","\n","Alfa = 0.01'i atayalım ve her hiper parametre kombinasyonu için 10 antrenman yapalım. Bu 10 çalıştırmayı kullanarak, 1'den 1000'e kadar her bölüm numarası (episode number) için zaman adımlarının ortalamasını alıyoruz.\n","\n","Kodu daha temiz tutmak için src/loops.py dosyasında train_many_runs() işlevini oluşturalım:\n"]},{"cell_type":"code","execution_count":null,"id":"d6188e2a","metadata":{"scrolled":true,"id":"d6188e2a"},"outputs":[],"source":["from src.loops import train_many_runs\n","\n","alphas = [0.1, 1]\n","gammas = [0.1, 0.6, 0.9]\n","\n","epsilon = 0.1\n","n_episodes = 1000\n","n_runs = 10\n","\n","results = pd.DataFrame()\n","for alpha in alphas:\n","    for gamma in gammas:\n","        \n","        print(f'alpha: {alpha}, gamma: {gamma}')\n","        agent = QAgent(env, alpha, gamma)\n","        \n","        timesteps, penalties = train_many_runs(agent,\n","                                               env,\n","                                               n_episodes,\n","                                               epsilon,\n","                                               n_runs)\n","        \n","        # collect timesteps and penalties for this pair of\n","        # hyper-parameters (alpha, gamma)\n","        results_ = pd.DataFrame()\n","        results_['timesteps'] = timesteps\n","        results_['penalties'] = penalties\n","        results_['alpha'] = alpha\n","        results_['gamma'] = gamma\n","        results = pd.concat([results, results_])\n","\n","# index -> episode\n","results = results.reset_index().rename(\n","    columns={'index': 'episode'})\n","\n","results['hyperparameters'] = [\n","    f'alpha={a}, gamma={g}'\n","    for (a, g) in zip(results['alpha'], results['gamma'])]"]},{"cell_type":"code","execution_count":null,"id":"2bca4000","metadata":{"id":"2bca4000"},"outputs":[],"source":["fig = plt.gcf()\n","fig.set_size_inches(12, 8)\n","sns.lineplot('episode', 'timesteps', hue='hyperparameters', data=results)"]},{"cell_type":"markdown","id":"100f5206","metadata":{"id":"100f5206"},"source":["**Görünüşe göre alfa = 1.0 en iyi çalışan değer, gama ise daha az etkiye sahip gibi görünüyor.**\n","\n","Hiper parametreleri ayarlamak zaman alıcı ve sıkıcı olabilir. Optuna gibi, az önceki manuel süreci otomatikleştirmek için mükemmel kütüphaneler de var. Şimdilik, yeni bulduğumuz değerlerle çalışalım.\n","\n","epsilon = %10'da ne oluyor?\n","Mevcut %10'luk değer en iyisi mi?\n","\n","Bunu kontrol edelim.\n","\n","Bulunan en iyi alfa ve gama değerlerini alıyoruz.\n","\n","**alfa = 1.0**\n","**gama = 0.9 (0,1 veya 0,6 da alabilirdik)**\n","\n","**Ve farklı epsilonlarla antrenman yapalım = [0.01, 0.1, 0.9]**"]},{"cell_type":"code","execution_count":null,"id":"7fb9250a","metadata":{"scrolled":true,"id":"7fb9250a"},"outputs":[],"source":["# best hyper-parameters so far\n","alpha = 1.0\n","gamma = 0.9\n","\n","epsilons = [0.01, 0.10, 0.9]\n","n_runs = 10\n","n_episodes = 200\n","\n","results = pd.DataFrame()\n","for epsilon in epsilons:\n","        \n","    print(f'epsilon: {epsilon}')\n","    agent = QAgent(env, alpha, gamma)\n","\n","    timesteps, penalties = train_many_runs(agent,\n","                                           env,\n","                                           n_episodes,\n","                                           epsilon,\n","                                           n_runs)\n","\n","    # collect timesteps and penalties for this pair of\n","    # hyper-parameters (alpha, gamma)\n","    results_ = pd.DataFrame()\n","    results_['timesteps'] = timesteps\n","    results_['penalties'] = penalties\n","    results_['epsilon'] = epsilon\n","    results = pd.concat([results, results_])\n","\n","# index -> episode\n","results = results.reset_index().rename(columns={'index': 'episode'})"]},{"cell_type":"markdown","source":["## Ve ortaya çıkan zaman adımlarını ve ceza eğrilerini çizelim:"],"metadata":{"id":"YYyi7N8ZpWLa"},"id":"YYyi7N8ZpWLa"},{"cell_type":"code","execution_count":null,"id":"3f352227","metadata":{"id":"3f352227"},"outputs":[],"source":["fig = plt.gcf()\n","fig.set_size_inches(12, 8)\n","sns.lineplot('episode', 'timesteps', hue='epsilon', data=results)\n","plt.show()\n","\n","fig = plt.gcf()\n","fig.set_size_inches(12, 8)\n","sns.lineplot('episode', 'penalties', hue='epsilon', data=results)"]},{"cell_type":"markdown","source":["Gördüğünüz gibi, hem epsilon = 0.01 hem de epsilon = 0.1, keşif ve sömürü arasında doğru dengeyi tuttukları için eşit derecede iyi çalışıyor gibi görünüyor.\n","\n","Öte yandan, epsilon = 0.9 çok büyük bir değerdir, eğitim sırasında \"çok fazla\" rastgeleliğe neden olur ve q-matriksimizin optimal olana yakınsamasını engeller. \n","\n","Performansın bölüm başına yaklaşık 250 zaman adımında nasıl plato yaptığını gözlemleyin.\n","\n","Genel olarak, epsilon hiper parametresini seçmek için en iyi strateji ilerleyici epsilon bozunumudur (epsilon-decay). Yani, eğitimin başlangıcında, ajan q-değeri tahmininden çok emin olmadığında, mümkün olduğu kadar çok durumun ziyaret edilmesi en iyisidir ve bunun için büyük bir epsilon harika bir seçimdir (örneğin %50).\n","\n","Eğitim ilerledikçe ve aracı q-değeri tahminini iyileştirdikçe, o kadar fazla araştırma yapmak artık optimal değildir. Bunun yerine, epsilon'u azaltarak, ajan q-değerlerini mükemmelleştirmeyi ve ince ayar yapmayı öğrenebilir, böylece onları optimal değerlere daha hızlı yakınsayabilir. \n","\n","Epsilon'un çok büyük olması, epsilon = 0.9 için gördüğümüz gibi yakınsama sorunlarına neden olabilir.\n"],"metadata":{"id":"lluvRRGIpi2_"},"id":"lluvRRGIpi2_"},{"cell_type":"code","execution_count":null,"id":"5e87921c","metadata":{"id":"5e87921c"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"03_q_agent_hyperparameters_analysis.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}