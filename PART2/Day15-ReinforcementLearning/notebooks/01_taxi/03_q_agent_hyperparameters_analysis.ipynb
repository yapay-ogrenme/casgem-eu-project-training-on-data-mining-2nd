{"cells":[{"cell_type":"markdown","id":"f0fd6807","metadata":{"id":"f0fd6807"},"source":["# 03 Q-agent hiperparametre analizi"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"id":"Hy27IAC8qcyU"},"id":"Hy27IAC8qcyU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls '/content/drive/MyDrive/CASGEM-Egitim/Egitim-Part2/Day15-ReinforcementLearning/notebooks/01_taxi'"],"metadata":{"id":"LSCTqkF5qdtu"},"id":"LSCTqkF5qdtu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/CASGEM-Egitim/Egitim-Part2/Day15-ReinforcementLearning/notebooks/01_taxi')"],"metadata":{"id":"pCfejzEuq5oT"},"id":"pCfejzEuq5oT","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"abcf6613","metadata":{"id":"abcf6613"},"source":["#### ğŸ‘‰RL ajanlarÄ± hiper parametrelere son derece duyarlÄ±dÄ±r.\n","\n","#### ğŸ‘‰Hiper parametrelerin Ã¶ÄŸrenmeyi nasÄ±l etkilediklerine dair daha iyi bir sezgi kazanmak iÃ§in onlarla sistematik bir ÅŸekilde oynayalÄ±m.\n","\n","**Alfa (learning rate) ve gama (discount factor) iÃ§in farklÄ± deÄŸerler kullanarak q-agent'Ä± eÄŸitelim. Epsilon'a gelince, %10'da tutuyoruz.**\n","\n","**Kodu temiz tutmak iÃ§in q-agent tanÄ±mÄ±nÄ± src/q_agent.py iÃ§ine ve eÄŸitim dÃ¶ngÃ¼sÃ¼nÃ¼\n","src/loops.py iÃ§indeki train() iÅŸlevinin iÃ§ine yerleÅŸtirebiliriz.**\n"]},{"cell_type":"code","execution_count":null,"id":"e3629346","metadata":{"id":"e3629346"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","%pylab inline\n","%config InlineBackend.figure_format = 'svg'"]},{"cell_type":"markdown","id":"76e9a06d","metadata":{"id":"76e9a06d"},"source":["## Ã‡evre (Environment) ğŸŒ"]},{"cell_type":"code","execution_count":null,"id":"ebfba291","metadata":{"id":"ebfba291"},"outputs":[],"source":["import gym\n","env = gym.make(\"Taxi-v3\").env"]},{"cell_type":"markdown","id":"8e9e7c87","metadata":{"id":"8e9e7c87"},"source":["## Q-agent ğŸ¤–ğŸ§ "]},{"cell_type":"code","execution_count":null,"id":"a5ee8884","metadata":{"id":"a5ee8884"},"outputs":[],"source":["# No need to copy paste the same QAgent\n","# definition in every notebook, don't you think?\n","from src.q_agent import QAgent\n","\n","# hyper-parameters\n","# RL problems are full of these hyper-parameters.\n","# For the moment, trust me when I set these values.\n","# We will later play with these and see how they impact learning.\n","alphas = [0.01, 0.1, 1]\n","gammas = [0.1, 0.6, 0.9]"]},{"cell_type":"markdown","id":"0209ba20","metadata":{"id":"0209ba20"},"source":["## Training loop ğŸ¡"]},{"cell_type":"code","execution_count":null,"id":"5e08672d","metadata":{"id":"5e08672d"},"outputs":[],"source":["import pandas as pd\n","\n","from src.loops import train\n","\n","# exploration vs exploitation prob\n","# let's start with a constant probability of 10%.\n","epsilon = 0.1\n","n_episodes = 1000\n","\n","results = pd.DataFrame()\n","for alpha in alphas:\n","    for gamma in gammas:\n","        \n","        print(f'alpha: {alpha}, gamma: {gamma}')\n","        agent = QAgent(env, alpha, gamma)\n","        \n","        _, timesteps, penalties = train(agent,\n","                                        env,\n","                                        n_episodes,\n","                                        epsilon)\n","        \n","        # collect timesteps and penalties for this pair\n","        # of hyper-parameters (alpha, gamma)\n","        results_ = pd.DataFrame()\n","        results_['timesteps'] = timesteps\n","        results_['penalties'] = penalties\n","        results_['alpha'] = alpha\n","        results_['gamma'] = gamma\n","        results = pd.concat([results, results_])\n","\n","# index -> episode\n","results = results.reset_index().rename(\n","    columns={'index': 'episode'})\n","\n","# add column with the 2 hyper-parameters\n","results['hyperparameters'] = [\n","    f'alpha={a}, gamma={g}'\n","    for (a, g) in zip(results['alpha'], results['gamma'])\n","]"]},{"cell_type":"markdown","source":["**Her bir hiper parametre kombinasyonu iÃ§in episode baÅŸÄ±na zaman adÄ±mlarÄ±nÄ± Ã§izelim.**"],"metadata":{"id":"DB9fd2RymDs0"},"id":"DB9fd2RymDs0"},{"cell_type":"code","execution_count":null,"id":"e975b843","metadata":{"id":"e975b843"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","fig = plt.gcf()\n","fig.set_size_inches(12, 8)\n","sns.lineplot('episode', 'timesteps',\n","             hue='hyperparameters', data=results)"]},{"cell_type":"markdown","id":"4769fa4e","metadata":{"id":"4769fa4e"},"source":["Grafik iddialÄ± gÃ¶rÃ¼nÃ¼yor ama biraz fazla gÃ¼rÃ¼ltÃ¼lÃ¼.\n","\n","**Yine de gÃ¶zlemleyebileceÄŸimiz ÅŸey, alfa = 0.01 olduÄŸunda Ã¶ÄŸrenmenin daha yavaÅŸ olduÄŸudur. alfa (Ã¶ÄŸrenme hÄ±zÄ±), her yinelemede q deÄŸerlerini ne kadar gÃ¼ncellediÄŸimizi kontrol eder. DeÄŸerin Ã§ok kÃ¼Ã§Ã¼k olmasÄ± daha yavaÅŸ Ã¶ÄŸrenme anlamÄ±na gelir.**\n","\n","Alfa = 0.01'i atayalÄ±m ve her hiper parametre kombinasyonu iÃ§in 10 antrenman yapalÄ±m. Bu 10 Ã§alÄ±ÅŸtÄ±rmayÄ± kullanarak, 1'den 1000'e kadar her bÃ¶lÃ¼m numarasÄ± (episode number) iÃ§in zaman adÄ±mlarÄ±nÄ±n ortalamasÄ±nÄ± alÄ±yoruz.\n","\n","Kodu daha temiz tutmak iÃ§in src/loops.py dosyasÄ±nda train_many_runs() iÅŸlevini oluÅŸturalÄ±m:\n"]},{"cell_type":"code","execution_count":null,"id":"d6188e2a","metadata":{"scrolled":true,"id":"d6188e2a"},"outputs":[],"source":["from src.loops import train_many_runs\n","\n","alphas = [0.1, 1]\n","gammas = [0.1, 0.6, 0.9]\n","\n","epsilon = 0.1\n","n_episodes = 1000\n","n_runs = 10\n","\n","results = pd.DataFrame()\n","for alpha in alphas:\n","    for gamma in gammas:\n","        \n","        print(f'alpha: {alpha}, gamma: {gamma}')\n","        agent = QAgent(env, alpha, gamma)\n","        \n","        timesteps, penalties = train_many_runs(agent,\n","                                               env,\n","                                               n_episodes,\n","                                               epsilon,\n","                                               n_runs)\n","        \n","        # collect timesteps and penalties for this pair of\n","        # hyper-parameters (alpha, gamma)\n","        results_ = pd.DataFrame()\n","        results_['timesteps'] = timesteps\n","        results_['penalties'] = penalties\n","        results_['alpha'] = alpha\n","        results_['gamma'] = gamma\n","        results = pd.concat([results, results_])\n","\n","# index -> episode\n","results = results.reset_index().rename(\n","    columns={'index': 'episode'})\n","\n","results['hyperparameters'] = [\n","    f'alpha={a}, gamma={g}'\n","    for (a, g) in zip(results['alpha'], results['gamma'])]"]},{"cell_type":"code","execution_count":null,"id":"2bca4000","metadata":{"id":"2bca4000"},"outputs":[],"source":["fig = plt.gcf()\n","fig.set_size_inches(12, 8)\n","sns.lineplot('episode', 'timesteps', hue='hyperparameters', data=results)"]},{"cell_type":"markdown","id":"100f5206","metadata":{"id":"100f5206"},"source":["**GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re alfa = 1.0 en iyi Ã§alÄ±ÅŸan deÄŸer, gama ise daha az etkiye sahip gibi gÃ¶rÃ¼nÃ¼yor.**\n","\n","Hiper parametreleri ayarlamak zaman alÄ±cÄ± ve sÄ±kÄ±cÄ± olabilir. Optuna gibi, az Ã¶nceki manuel sÃ¼reci otomatikleÅŸtirmek iÃ§in mÃ¼kemmel kÃ¼tÃ¼phaneler de var. Åimdilik, yeni bulduÄŸumuz deÄŸerlerle Ã§alÄ±ÅŸalÄ±m.\n","\n","epsilon = %10'da ne oluyor?\n","Mevcut %10'luk deÄŸer en iyisi mi?\n","\n","Bunu kontrol edelim.\n","\n","Bulunan en iyi alfa ve gama deÄŸerlerini alÄ±yoruz.\n","\n","**alfa = 1.0**\n","**gama = 0.9 (0,1 veya 0,6 da alabilirdik)**\n","\n","**Ve farklÄ± epsilonlarla antrenman yapalÄ±m = [0.01, 0.1, 0.9]**"]},{"cell_type":"code","execution_count":null,"id":"7fb9250a","metadata":{"scrolled":true,"id":"7fb9250a"},"outputs":[],"source":["# best hyper-parameters so far\n","alpha = 1.0\n","gamma = 0.9\n","\n","epsilons = [0.01, 0.10, 0.9]\n","n_runs = 10\n","n_episodes = 200\n","\n","results = pd.DataFrame()\n","for epsilon in epsilons:\n","        \n","    print(f'epsilon: {epsilon}')\n","    agent = QAgent(env, alpha, gamma)\n","\n","    timesteps, penalties = train_many_runs(agent,\n","                                           env,\n","                                           n_episodes,\n","                                           epsilon,\n","                                           n_runs)\n","\n","    # collect timesteps and penalties for this pair of\n","    # hyper-parameters (alpha, gamma)\n","    results_ = pd.DataFrame()\n","    results_['timesteps'] = timesteps\n","    results_['penalties'] = penalties\n","    results_['epsilon'] = epsilon\n","    results = pd.concat([results, results_])\n","\n","# index -> episode\n","results = results.reset_index().rename(columns={'index': 'episode'})"]},{"cell_type":"markdown","source":["## Ve ortaya Ã§Ä±kan zaman adÄ±mlarÄ±nÄ± ve ceza eÄŸrilerini Ã§izelim:"],"metadata":{"id":"YYyi7N8ZpWLa"},"id":"YYyi7N8ZpWLa"},{"cell_type":"code","execution_count":null,"id":"3f352227","metadata":{"id":"3f352227"},"outputs":[],"source":["fig = plt.gcf()\n","fig.set_size_inches(12, 8)\n","sns.lineplot('episode', 'timesteps', hue='epsilon', data=results)\n","plt.show()\n","\n","fig = plt.gcf()\n","fig.set_size_inches(12, 8)\n","sns.lineplot('episode', 'penalties', hue='epsilon', data=results)"]},{"cell_type":"markdown","source":["GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi, hem epsilon = 0.01 hem de epsilon = 0.1, keÅŸif ve sÃ¶mÃ¼rÃ¼ arasÄ±nda doÄŸru dengeyi tuttuklarÄ± iÃ§in eÅŸit derecede iyi Ã§alÄ±ÅŸÄ±yor gibi gÃ¶rÃ¼nÃ¼yor.\n","\n","Ã–te yandan, epsilon = 0.9 Ã§ok bÃ¼yÃ¼k bir deÄŸerdir, eÄŸitim sÄ±rasÄ±nda \"Ã§ok fazla\" rastgeleliÄŸe neden olur ve q-matriksimizin optimal olana yakÄ±nsamasÄ±nÄ± engeller. \n","\n","PerformansÄ±n bÃ¶lÃ¼m baÅŸÄ±na yaklaÅŸÄ±k 250 zaman adÄ±mÄ±nda nasÄ±l plato yaptÄ±ÄŸÄ±nÄ± gÃ¶zlemleyin.\n","\n","Genel olarak, epsilon hiper parametresini seÃ§mek iÃ§in en iyi strateji ilerleyici epsilon bozunumudur (epsilon-decay). Yani, eÄŸitimin baÅŸlangÄ±cÄ±nda, ajan q-deÄŸeri tahmininden Ã§ok emin olmadÄ±ÄŸÄ±nda, mÃ¼mkÃ¼n olduÄŸu kadar Ã§ok durumun ziyaret edilmesi en iyisidir ve bunun iÃ§in bÃ¼yÃ¼k bir epsilon harika bir seÃ§imdir (Ã¶rneÄŸin %50).\n","\n","EÄŸitim ilerledikÃ§e ve aracÄ± q-deÄŸeri tahminini iyileÅŸtirdikÃ§e, o kadar fazla araÅŸtÄ±rma yapmak artÄ±k optimal deÄŸildir. Bunun yerine, epsilon'u azaltarak, ajan q-deÄŸerlerini mÃ¼kemmelleÅŸtirmeyi ve ince ayar yapmayÄ± Ã¶ÄŸrenebilir, bÃ¶ylece onlarÄ± optimal deÄŸerlere daha hÄ±zlÄ± yakÄ±nsayabilir. \n","\n","Epsilon'un Ã§ok bÃ¼yÃ¼k olmasÄ±, epsilon = 0.9 iÃ§in gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi yakÄ±nsama sorunlarÄ±na neden olabilir.\n"],"metadata":{"id":"lluvRRGIpi2_"},"id":"lluvRRGIpi2_"},{"cell_type":"code","execution_count":null,"id":"5e87921c","metadata":{"id":"5e87921c"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"03_q_agent_hyperparameters_analysis.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}