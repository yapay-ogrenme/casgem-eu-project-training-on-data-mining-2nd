{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT01XvVDb34I"
      },
      "source": [
        "# Sinir Ağı zihniyetiyle Lojistik Regresyon\n",
        "\n",
        "Bu çalışma dosyasında, kedileri tanımak için bir lojistik regresyon sınıflandırıcısı oluşturacaksınız. Bir Sinir Ağı zihniyetiyle bunu nasıl yapacağınız konusunda size adım atacak ve ayrıca derin öğrenme hakkındaki sezgilerinizi geliştirecek.\n",
        "\n",
        "**Bu çalışma dosyasında neler öğrenilecek:**\n",
        "- Aşağıdakiler dahil bir öğrenme algoritmasının genel mimarisini oluşturma:\n",
        "     - Başlatma parametreleri\n",
        "     - Maliyet fonksiyonunun ve gradyanının hesaplanması\n",
        "     - Bir optimizasyon algoritması kullanma (gradyan iniş)\n",
        "- Yukarıdaki üç işlevi de doğru sırayla bir ana model fonksiyonu altında toplama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKHjIUp8b34N"
      },
      "source": [
        "## 1 - Paketler ##\n",
        "\n",
        "Öncelikle, bu atama sırasında ihtiyaç duyacağınız tüm paketleri içe aktarmak için aşağıdaki hücreyi çalıştıralım.\n",
        "- [numpy](www.numpy.org), Python ile bilimsel hesaplama için temel pakettir.\n",
        "- [h5py](http://www.h5py.org), bir H5 dosyasında depolanan bir veri kümesiyle etkileşim kurmak için yaygın bir pakettir.\n",
        "- [matplotlib](http://matplotlib.org), Python'da grafik çizmek için ünlü bir kütüphanedir.\n",
        "- [PIL](http://www.pythonware.com/products/pil/) ve [scipy](https://www.scipy.org/) burada sonunda kendi resminizle modelinizi test etmek için kullanılır ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vGQLMNWb34P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "import scipy.misc\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "#from lr_utils import load_dataset\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"https://media.githubusercontent.com/media/yapay-ogrenme/casgem-eu-project-training-on-data-mining-2nd/main/PART2/Day9-NeuralNetworks/notebooks/\"\n",
        "\n",
        "DATASET_PATH = ROOT_DIR + \"datasets/\""
      ],
      "metadata": {
        "id": "Z0tEp3UohJka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH_TRAIN = DATASET_PATH + 'train_catvnoncat_2.h5'\n",
        "DATASET_PATH_TEST = DATASET_PATH + 'test_catvnoncat_2.h5'\n",
        "\n",
        "!wget $DATASET_PATH_TRAIN\n",
        "!wget $DATASET_PATH_TEST"
      ],
      "metadata": {
        "id": "NLr4x7HMjTh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Dataset\n",
        "def load_dataset():\n",
        "    train_dataset = h5py.File('train_catvnoncat_2.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('test_catvnoncat_2.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "    \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "    \n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
      ],
      "metadata": {
        "id": "TmjjhjvJfp5j",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "HEMLqw_Jb34R"
      },
      "source": [
        "## 2 - Probleme Genel Bakış ##\n",
        "\n",
        "**Problem Açıklaması**: Size aşağıdakileri içeren bir veri kümesi (\"data.h5\") verilir:\n",
        "- kedi (y=1) veya kedi olmayan (y=0) olarak etiketlenen m_train görüntülerinden oluşan bir eğitim seti\n",
        "- kedi veya kedi olmayan olarak etiketlenmiş bir m_test görüntüleri test seti\n",
        "- her görüntü şekle (num_px, num_px, 3) sahiptir, burada 3, 3 kanal (RGB) içindir. Böylece, her görüntü karedir (yükseklik = num_px) ve (genişlik = num_px).\n",
        "\n",
        "Resimleri kedi veya kedi olmayan olarak doğru bir şekilde sınıflandırabilen basit bir görüntü tanıma algoritması oluşturacaksınız.\n",
        "\n",
        "Veri setini daha yakından tanıyalım. Aşağıdaki kodu çalıştırarak verileri yükleyin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5v3GSDib34R"
      },
      "outputs": [],
      "source": [
        "# Loading the data (cat/non-cat)\n",
        "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87IXddjcb34T"
      },
      "source": [
        "Görüntü veri kümelerinin (eğitim ve test) sonuna \"_orig\" ekledik çünkü onları ön işlemlere tabi tutacağız. Ön işlemlerden sonra, train_set_x ve test_set_x ile değişkenlerine atayacağız. \n",
        "(train_set_y ve test_set_y etiketleri herhangi bir ön işlemeye ihtiyaç duymaz).\n",
        "\n",
        "train_set_x_orig ve test_set_x_orig öğelerinizin her satırı, bir görüntüyü temsil eden bir dizidir. Aşağıdaki kodu çalıştırarak bir örneği görselleştirebilirsiniz. Ayrıca `index` değerini değiştirmekten çekinmeyin ve diğer resimleri görmek için yeniden çalıştırın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCL26aW1b34T"
      },
      "outputs": [],
      "source": [
        "# Example of a picture\n",
        "index = 34\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[:,index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:,index])].decode(\"utf-8\") +  \"' picture.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1SPjeQ-b34U"
      },
      "source": [
        "**Alıştırma:** Şunun değerlerini bulun:\n",
        "- m_train (eğitim örneklerinin sayısı)\n",
        "- m_test (test örneği sayısı)\n",
        "- num_px (= bir eğitim görüntüsünün yüksekliği = genişliği)\n",
        "\n",
        "`train_set_x_orig` öğesinin (m_train, num_px, num_px, 3) boyutlarında bir numpy-array olduğunu unutmayın.\n",
        "\n",
        "Örneğin, `train_set_x_orig.shape[0]` yazarak `m_train`'e erişebilirsiniz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9_kpPM84b34W"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ### (≈ 3 lines of code)\n",
        "m_train = train_set_y.shape[1]\n",
        "m_test = test_set_y.shape[1]\n",
        "num_px = train_set_x_orig.shape[1]\n",
        "### END CODE HERE ###\n",
        "\n",
        "print (\"Number of training examples: m_train = \" + str(m_train))\n",
        "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
        "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWlUeLEIb34X"
      },
      "source": [
        "**m_train, m_test ve num_px için Beklenen Çıktı**:\n",
        "<table style=\"width:15%\">\n",
        "  <tr>\n",
        "    <td>m_train</td>\n",
        "    <td> 209 </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>m_test</td>\n",
        "    <td> 50 </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>num_px</td>\n",
        "    <td> 64 </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z7emcT_b34Y"
      },
      "source": [
        "Kolaylık sağlamak için, şimdi (num_px, num_px, 3) boyutlarındaki bir resmi, (num_px $*$ num_px $*$ 3, 1) boyutlarında numpy-array olarak yeniden şekillendirmelisiniz. \n",
        "\n",
        "Bundan sonra, eğitim (ve test) veri kümemiz, her sütunun düzleştirilmiş (flattened) bir görüntüyü temsil ettiği bir sayısal dizidir. m_train (sırasıyla m_test) sütunları olmalıdır.\n",
        "\n",
        "**Alıştırma:** Eğitim ve test veri kümelerini, (num_px, num_px, 3) boyutundaki görüntülerin tek boyutlu vektörlerine (num\\_px $*$ num\\_px $*$ 3, 1) düzleştirileceği şekilde yeniden şekillendirin.\n",
        "\n",
        "(a,b,c,d) şeklindeki bir X matrisini X_flatten şekilli bir matrise (b$*$c$*$d, a) düzleştirmek istediğinizde kullanabileceğiniz bir hile kullanmaktır:\n",
        "```python\n",
        "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86U0BVJdb34Z"
      },
      "outputs": [],
      "source": [
        "# Reshape the training and test examples\n",
        "\n",
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
        "### END CODE HERE ###\n",
        "\n",
        "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
        "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kru1ifaNb34a"
      },
      "source": [
        "**Beklenen çıktı**:\n",
        "\n",
        "<table style=\"width:35%\">\n",
        "  <tr>\n",
        "    <td>train_set_x_flatten shape</td>\n",
        "    <td> (12288, 209)</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>train_set_y shape</td>\n",
        "    <td>(1, 209)</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>test_set_x_flatten shape</td>\n",
        "    <td>(12288, 50)</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>test_set_y shape</td>\n",
        "    <td>(1, 50)</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "  <td>sanity check after reshaping</td>\n",
        "  <td>[17 31 56 22 33]</td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzclaXiNb34b"
      },
      "source": [
        "Renkli görüntüleri temsil etmek için, her piksel için kırmızı(red), yeşil(green) ve mavi(blue) kanallar (RGB) belirtilmelidir ve bu nedenle piksel değeri aslında 0 ile 255 arasında değişen üç sayıdan oluşan bir vektördür.\n",
        "\n",
        "Makine öğrenimindeki yaygın bir ön işleme adımı, veri kümenizi ortalamak ve standart hale getirmektir; bu, tüm numpy dizisinin ortalamasını her örnekten çıkarmanız ve ardından her örneği tüm numpy dizisinin standart sapmasına bölmeniz anlamına gelir. Ancak resim veri kümeleri için, daha basit ve daha kullanışlıdır ve veri kümesinin her satırını 255'e (bir piksel kanalının maksimum değeri) bölmek neredeyse işe yarar.\n",
        "\n",
        "> NOT: Modelinizin eğitimi sırasında, nöron aktivasyonlarını gözlemlemek için ağırlıkları çarpacak ve bazı ilk girdilere önyargılar ekleyeceksiniz. Ardından, modeli eğitmek için gradyanlarla geri yayılırsınız. Ancak, gradyanlarımızın patlamaması için her özelliğin benzer bir aralığa sahip olması son derece önemlidir. Bunu daha sonra derslerde daha ayrıntılı olarak göreceksiniz.\n",
        "\n",
        "Veri kümemizi standartlaştıralım."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amj4WPcBb34b"
      },
      "outputs": [],
      "source": [
        "train_set_x = train_set_x_flatten / 255.\n",
        "test_set_x = test_set_x_flatten / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtFcuFOTb34c"
      },
      "source": [
        "<font color='blue'>\n",
        "**Hatırlamanız gerekenler:**\n",
        "\n",
        "\n",
        "Yeni bir veri kümesini önceden işlemek için yaygın olarak kullanılan adımlar şunlardır:\n",
        "- Problemin boyutlarını belirleyin (m_train, m_test, num_px, ...)\n",
        "- Veri kümelerini, her örnek bir boyutlu vektörü olacak şekilde yeniden boyutlandırma (num_px \\* num_px \\* 3, 1)\n",
        "- Verileri \"Standardize\" etme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj1aFpTob34c"
      },
      "source": [
        "## 3 - Öğrenme Algoritmasının Genel Mimarisi ##\n",
        "\n",
        "Kedi görüntülerini kedi olmayan görüntülerden ayırt etmek için basit bir algoritma tasarlamanın zamanı geldi.\n",
        "\n",
        "Bir Sinir Ağı zihniyetini kullanarak bir Lojistik Regresyon oluşturacaksınız. Aşağıdaki Şekil, **Lojistik Regresyonun aslında neden çok basit bir Sinir Ağı olduğunu açıklıyor!**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Pm8ZbTzQzTpHlkTpek8u1KTUn1X0bJZK\" style=\"width:650px;height:400px;\" alt=\"LogReg_kiank\" title=\"LogReg_kiank\">\n",
        "\n",
        "**Algoritmanın matematiksel ifadesi**:\n",
        "\n",
        "For one example $x^{(i)}$:\n",
        "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
        "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
        "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
        "\n",
        "The cost is then computed by summing over all training examples:\n",
        "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
        "\n",
        "**Temel adımlar**:\n",
        "Bu alıştırmada aşağıdaki adımları gerçekleştireceksiniz:\n",
        "- Modelin parametrelerini başlatma\n",
        "- Maliyeti en aza indirerek modelin parametrelerini öğrenme\n",
        "- Tahmin yapmak için öğrenilen parametreleri kullanma (test setinde)\n",
        "- Sonuçları analiz etme ve sonuca varma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dummqP7b34d"
      },
      "source": [
        "## 4 - Algoritmamızın parçalarını oluşturmak ##\n",
        "\n",
        "Bir Sinir Ağı oluşturmanın ana adımları şunlardır:\n",
        "1. Model yapısını tanımlayın (giriş özelliklerinin sayısı gibi)\n",
        "2. Modelin parametrelerini başlatın\n",
        "3. Döngü:\n",
        "- Yitim (kayıp) (loss) değerini hesaplayın (ileri yayılım) (forward propagation)\n",
        "- Mevcut gradyanı hesaplayın (geriye doğru yayılma) (backward propagation)\n",
        "- Parametreleri güncelle (gradyan iniş) (gradient descent)\n",
        "\n",
        "Genellikle 1-3'ü ayrı ayrı oluşturur ve bunları `model()` dediğimiz tek bir fonksiyına entegre edersiniz.\n",
        "\n",
        "### 4.1 - Yardımcı işlevler\n",
        "\n",
        "**Alıştırma**: `sigmoid()` fonksiyonunu tanımlayın ve uygulayın. \n",
        "Yukarıdaki şekilde gördüğünüz gibi, tahmin yapabilmek için $sigmoid( w^T x + b)$ hesaplamanız gerekiyor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G12NAGSsb34d"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: sigmoid\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    s = 1 / (1 + np.exp(-z))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "8ytzZueKb34d"
      },
      "outputs": [],
      "source": [
        "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
        "print (\"sigmoid(9.2) = \" + str(sigmoid(9.2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkD64DYWb34e"
      },
      "source": [
        "**Beklenen Çıktı**: \n",
        "\n",
        "<table style=\"width:20%\">\n",
        "  <tr>\n",
        "    <td>sigmoid(0)</td>\n",
        "    <td> 0.5</td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>sigmoid(9.2)</td>\n",
        "    <td> 0.999898970806 </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntPPsuTZb34e"
      },
      "source": [
        "### 4.2 - Parametreleri başlatma\n",
        "\n",
        "**Alıştırma:** Aşağıdaki hücrede parametre başlatmayı uygulayın. w'yi sıfır vektörü olarak başlatmanız gerekir. Hangi numpy işlevini kullanacağınızı bilmiyorsanız, Numpy kitaplığının belgelerinde np.zeros() öğesine bakın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF78wRQXb34e"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: initialize_with_zeros\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    w = np.zeros(shape=(dim, 1))\n",
        "    b = 0\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DciEjjPHb34f"
      },
      "outputs": [],
      "source": [
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPF03-f_b34g"
      },
      "source": [
        "**Beklenen Çıktı**: \n",
        "<table style=\"width:15%\">\n",
        "    <tr>\n",
        "        <td>w</td>\n",
        "        <td> [[ 0.]\n",
        " [ 0.]] </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>b</td>\n",
        "        <td> 0 </td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "Görüntü girişleri için w (num_px $\\times$ num_px $\\times$ 3, 1) boyutlarında olacaktır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htGCnrZLb34g"
      },
      "source": [
        "### 4.3 - İleri ve Geri Yayılım\n",
        "\n",
        "Artık parametreleriniz başlatıldığına göre, parametreleri öğrenmek için \"ileri\" ve \"geri\" yayılma adımlarını yapabilirsiniz.\n",
        "\n",
        "**Alıştırma:** Maliyet fonksiyonunu (cost function) ve gradyanını hesaplayan bir `propagate()` fonksiyonunu uygulayın.\n",
        "\n",
        "**İpuçları**:\n",
        "\n",
        "İleri Yayılım:\n",
        "- X alınız\n",
        "- $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$ değerini hesaplayınız\n",
        "- Maliyet fonksiyonunu hesaplayınız: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
        "\n",
        "İşte kullanacağınız iki formül:\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU74E3BQb34g"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: propagate\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    Tips:\n",
        "    - Write your code step by step for the propagation\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    A = sigmoid(np.dot(w.T, X) + b)  # compute activation\n",
        "    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  # compute cost\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
        "    db = (1 / m) * np.sum(A - Y)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivwpb69Lb34i"
      },
      "outputs": [],
      "source": [
        "w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
        "\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LOo17Jib34i"
      },
      "source": [
        "**Beklenen Çıktı**:\n",
        "\n",
        "<table style=\"width:50%\">\n",
        "    <tr>\n",
        "        <td>dw</td>\n",
        "        <td> [[ 0.99993216]\n",
        " [ 1.99980262]]</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>db</td>\n",
        "        <td> 0.499935230625 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>cost</td>\n",
        "        <td> 6.000064773192205</td>\n",
        "    </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GbBmFs1b34i"
      },
      "source": [
        "### d) Optimizasyon\n",
        "- Parametrelerinizi başlattınız.\n",
        "- Ayrıca bir maliyet fonksiyonunu ve onun gradyanını da hesaplayabilirsiniz.\n",
        "- Şimdi, gradyan inişini kullanarak parametreleri güncellemek istiyorsunuz.\n",
        "\n",
        "**Alıştırma:** Optimizasyon fonksiyonunu yazın. Amaç, maliyet fonksiyonunu $J$'ı minimize ederek $w$ ve $b$'ı öğrenmektir. $\\theta$ parametresi için güncelleme kuralı $ \\theta = \\theta - \\alpha \\text{ } d\\theta$'dır, burada $\\alpha$ öğrenme oranıdır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO6NuERwb34i"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: optimize\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        \n",
        "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
        "        ### START CODE HERE ### \n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update rule (≈ 2 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        w = w - learning_rate * dw  # need to broadcast\n",
        "        b = b - learning_rate * db\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training examples\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10HrJUxAb34k"
      },
      "outputs": [],
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPXR9cfJb34l"
      },
      "source": [
        "**Alıştırma:** Önceki fonksiyon, öğrenilen w ve b'yi verir. Bir X veri kümesinin etiketlerini tahmin etmek için w ve b'yi kullanabiliriz. \n",
        "\n",
        "`predict()` fonksiyonunu uygulayın. Tahminleri hesaplamanın iki adımı vardır:\n",
        "\n",
        "1. $\\hat{Y} = A = \\sigma(w^T X + b)$ hesaplayın\n",
        "\n",
        "2. a'nın girişlerini 0'a (aktivasyon <= 0,5 ise) veya 1'e (aktivasyon > 0,5 ise) dönüştürün, tahminleri bir `Y_prediction` vektöründe depolar. İsterseniz, bir `for` döngüsünde `if`/`else` ifadesini kullanabilirsiniz (ancak bunu vektörleştirmenin bir yolu da vardır)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls-05Diub34l"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        # Convert probabilities a[0,i] to actual predictions p[0,i]\n",
        "        ### START CODE HERE ### (≈ 4 lines of code)\n",
        "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "    \n",
        "    return Y_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxsCWnjLb34m"
      },
      "outputs": [],
      "source": [
        "print(\"predictions = \" + str(predict(w, b, X)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwgoYET-b34n"
      },
      "source": [
        "**Beklenen Çıktı**: \n",
        "\n",
        "<table style=\"width:30%\">\n",
        "    <tr>\n",
        "         <td>\n",
        "            predictions\n",
        "         </td>\n",
        "          <td>\n",
        "            [[ 1.  1.]]\n",
        "         </td>  \n",
        "   </tr>\n",
        "\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VcYzqg0ob34n"
      },
      "source": [
        "<font color='blue'>\n",
        "**Hatırlanması gerekenler:**\n",
        "\n",
        "Şunları yapan birkaç fonksiyonu uyguladınız:\n",
        "\n",
        "- Initialize (w,b)\n",
        "- Parametreleri (w,b) öğrenmek için kaybı yinelemeli olarak optimize edin:\n",
        "  - maliyetin ve gradyanının hesaplanması\n",
        "  - gradyan iniş kullanarak parametreleri güncelleme\n",
        "- Verilen bir dizi örnek için etiketleri tahmin etmek için öğrenilenleri (w,b) kullanın"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZhHQqGmb34r"
      },
      "source": [
        "## 5 - Tüm fonksiyonları bir modelde birleştirin ##\n",
        "\n",
        "Şimdi tüm yapı taşlarını (önceki bölümlerde uygulanan fonksiyonlar) doğru sırayla bir araya getirerek genel modelin nasıl yapılandırıldığını göreceksiniz.\n",
        "\n",
        "**Alıştırma:** \n",
        "Model fonksiyonunu uygulayın. Aşağıdaki gösterimi kullanın:\n",
        "- Test kümesindeki tahminleriniz için Y_prediction\n",
        "- Tren kümesindeki tahminleriniz için Y_prediction_train\n",
        "- optimize() fonksiyonun çıktıları: w, costs, grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpwbAQVQb34s"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # initialize parameters with zeros (≈ 1 line of code)\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent (≈ 1 line of code)\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwBK--Iqb34t"
      },
      "source": [
        "Modelinizi eğitmek için aşağıdaki kod bloğunu çalıştırın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPNjAOgtb34t"
      },
      "outputs": [],
      "source": [
        "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caeVHKMUb34u"
      },
      "source": [
        "**Beklenen Çıktı**: \n",
        "\n",
        "<table style=\"width:40%\"> \n",
        "    <tr>\n",
        "        <td> Train Accuracy </td> \n",
        "        <td> 99.04306220095694 % </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Test Accuracy </td> \n",
        "        <td> 70.0 % </td>\n",
        "    </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "57pJPre-b34u"
      },
      "source": [
        "**Yorum**: Eğitim doğruluğu %100'e yakındır. Modeliniz çalışıyor ve eğitim verilerine uyacak kadar yüksek kapasiteye sahip. Test hatası %68'dir. Kullandığımız küçük veri kümesi ve lojistik regresyonun doğrusal bir sınıflandırıcı olduğu düşünüldüğünde, aslında bu basit model için fena değil. \n",
        "\n",
        "Ayrıca, modelin eğitim verilerine aşırı uyduğunu görüyorsunuz. Bu uzmanlıkta daha sonra, örneğin düzenlileştirmeyi kullanarak aşırı uymayı nasıl azaltacağınızı öğreneceksiniz. Aşağıdaki kodu kullanarak (ve `index` değişkenini değiştirerek) test kümesindeki resimlerindeki tahminlere bakabilirsiniz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLwv6z08b34u"
      },
      "outputs": [],
      "source": [
        "# Example of a picture that was wrongly classified.\n",
        "index = 1\n",
        "plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n",
        "print (\"y = \" + str(test_set_y[0, index]) + \", you predicted that it is a \\\"\" + classes[int(d[\"Y_prediction_test\"][0, index])].decode(\"utf-8\") +  \"\\\" picture.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmwGzUY5b34v"
      },
      "source": [
        "Ayrıca maliyet fonksiyonunu ve gradyanları da çizelim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjCQRVIGb34v"
      },
      "outputs": [],
      "source": [
        "# Plot learning curve (with costs)\n",
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "9hGi2KLxb34w"
      },
      "source": [
        "Maliyetin düştüğünü görebilirsiniz. Parametrelerin öğrenildiğini gösterir. Ancak eğitim kümesinde modeli daha da fazla eğitebileceğinizi görüyorsunuz. Yukarıdaki hücredeki yineleme sayısını artırmaya çalışın ve hücreleri yeniden çalıştırın. Eğitim kümesinde doğruluğunun arttığını ancak test seti doğruluğunun düştüğünü görebilirsiniz. Buna aşırı uyum (overfitting) denir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEtv0Y-tb34w"
      },
      "source": [
        "## 6 - İleri analiz ##\n",
        "\n",
        "İlk görüntü sınıflandırma modelinizi oluşturduğunuz için tebrikler. Bunu biraz daha analiz edelim ve $\\alpha$ öğrenme oranı için olası seçenekleri inceleyelim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpmNuytAb34w"
      },
      "source": [
        "#### Öğrenme oranı seçimi ####\n",
        "\n",
        "**Hatırlatma**:\n",
        "Gradient Descent'in çalışması için öğrenme oranını akıllıca seçmelisiniz. $\\alpha$ öğrenme oranı, parametreleri ne kadar hızlı güncellediğimizi belirler. Öğrenme oranı çok büyükse, optimal değeri \"aşabiliriz (overshoot)\". Benzer şekilde, eğer çok küçükse, en iyi değerlere yakınsamak için çok fazla iterasyona ihtiyacımız olacaktır. Bu yüzden iyi ayarlanmış bir öğrenme oranı kullanmak çok önemlidir.\n",
        "\n",
        "Modelimizin öğrenme eğrisini çeşitli öğrenme oranları seçenekleriyle karşılaştıralım. Aşağıdaki hücreyi çalıştırın. Bu yaklaşık 1 dakika sürmelidir. Ayrıca, içermesi için `learning_rates` değişkenini başlattığımız üç değerden farklı değerler denemekten çekinmeyin ve ne olduğunu görün."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08PztPCUb34x"
      },
      "outputs": [],
      "source": [
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "models = {}\n",
        "for i in learning_rates:\n",
        "    print (\"learning rate is: \" + str(i))\n",
        "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
        "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
        "\n",
        "for i in learning_rates:\n",
        "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
        "\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations')\n",
        "\n",
        "legend = plt.legend(loc='upper center', shadow=True)\n",
        "frame = legend.get_frame()\n",
        "frame.set_facecolor('0.90')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYRb_N1ib34x"
      },
      "source": [
        "- Farklı öğrenme oranları, farklı maliyetler ve dolayısıyla farklı tahmin sonuçları verir.\n",
        "- Öğrenme oranı çok büyükse (0.01), maliyet yukarı ve aşağı salınım yapabilir. Hatta farklılaşabilir (bu örnekte, 0.01'in kullanılması yine de sonunda maliyet için iyi bir değerde sonuçlanır).\n",
        "- Daha düşük maliyet, daha iyi bir model anlamına gelmez. Aşırı uyum olup olmadığını kontrol etmelisiniz. Eğitim doğruluğu, test doğruluğundan çok daha yüksek olduğunda olur.\n",
        "\n",
        "- Derin öğrenmede genellikle şunları yapmanızı öneririz:\n",
        "  - Maliyet fonksiyonunu daha iyi en aza indiren öğrenme oranını seçin.\n",
        "  - Modeliniz aşırı uyum gösteriyorsa, aşırı uyumu azaltma tekniklerini kullanın."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5SyxZZ6AmNaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Day9_ANN_1_Logistic_Regression_with_a_Neural_Network_mindset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}