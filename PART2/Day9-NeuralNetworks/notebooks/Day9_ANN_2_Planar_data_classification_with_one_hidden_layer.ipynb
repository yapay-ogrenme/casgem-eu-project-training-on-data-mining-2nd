{"cells":[{"cell_type":"markdown","metadata":{"id":"f9H5rlXjb78J"},"source":["# Bir gizli katmanla düzlemsel veri sınıflandırması\n","\n","Gizli bir katmana sahip olacak ilk sinir ağınızı oluşturmanın zamanı geldi. Bu model ile lojistik regresyon kullanarak uyguladığınız model arasında büyük bir fark göreceksiniz.\n","\n","**Nasıl yapacağınızı öğreneceksiniz:**\n","- Tek bir gizli katmana sahip 2 sınıflı bir sınıflandırma sinir ağı uygulayın\n","- Tanh gibi doğrusal olmayan aktivasyon fonksiyonu sahip birimleri kullanın\n","- Çapraz entropi kaybını hesaplayın\n","- İleri ve geri yayılımı uygulayın"]},{"cell_type":"markdown","metadata":{"id":"Mzf9l-Oab78P"},"source":["## 1 - Paketler ##\n","\n","Öncelikle bu çalışma dosyası sırasında ihtiyaç duyacağınız tüm paketleri import edelim.\n","- [numpy](www.numpy.org), Python ile bilimsel hesaplama için temel pakettir.\n","- [sklearn](http://scikit-learn.org/stable/) veri madenciliği ve veri analizi için basit ve verimli araçlar sağlar.\n","- [matplotlib](http://matplotlib.org), Python'da grafik çizmek için bir kütüphanedir."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4wZZhUqb78R"},"outputs":[],"source":["# Package imports\n","import numpy as np\n","import matplotlib.pyplot as plt\n","#from testCases import *\n","import sklearn\n","import sklearn.datasets\n","import sklearn.linear_model\n","#from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n","\n","%matplotlib inline\n","\n","np.random.seed(1) # set a seed so that the results are consistent"]},{"cell_type":"code","source":["#@title yardımcı fonksiyonlar\n","def plot_decision_boundary(model, X, y):\n","    # Set min and max values and give it some padding\n","    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n","    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n","    h = 0.01\n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    # Predict the function value for the whole grid\n","    Z = model(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","    plt.ylabel('x2')\n","    plt.xlabel('x1')\n","    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n","    \n","\n","def sigmoid(x):\n","    \"\"\"\n","    Compute the sigmoid of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(x)\n","    \"\"\"\n","    s = 1/(1+np.exp(-x))\n","    return s\n","\n","def load_planar_dataset():\n","    np.random.seed(1)\n","    m = 400 # number of examples\n","    N = int(m/2) # number of points per class\n","    D = 2 # dimensionality\n","    X = np.zeros((m,D)) # data matrix where each row is a single example\n","    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n","    a = 4 # maximum ray of the flower\n","\n","    for j in range(2):\n","        ix = range(N*j,N*(j+1))\n","        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n","        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n","        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n","        Y[ix] = j\n","        \n","    X = X.T\n","    Y = Y.T\n","\n","    return X, Y\n","\n","def load_extra_datasets():  \n","    N = 200\n","    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)\n","    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n","    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)\n","    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n","    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n","    \n","    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure"],"metadata":{"id":"lXHJZlwBnOtN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E2VHPGedb78U"},"source":["## 2 - Veri Kümesi ##\n","\n","Öncelikle üzerinde çalışacağınız veri kümesini alalım. \n","Aşağıdaki kod, `X` ve `Y` değişkenlerine \"çiçek(flower)\" 2 sınıfı bir veri kümesi yükleyecektir."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xdltgrsb78V"},"outputs":[],"source":["X, Y = load_planar_dataset()"]},{"cell_type":"markdown","metadata":{"id":"7jAUJIipb78W"},"source":["Matplotlib kullanarak veri kümesini görselleştirin. Veriler, biraz kırmızı (etiket y=0) ve biraz mavi (y=1) noktaları olan bir \"çiçek(flower)\" gibi görünüyor. Amacınız bu verilere uyacak bir model oluşturmaktır."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrFGOSczb78X"},"outputs":[],"source":["# Visualize the data:\n","plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"]},{"cell_type":"markdown","metadata":{"id":"9igX9nOyb78Z"},"source":["- özniteliklerini içeren bir sayısal dizi (matris) X (x1, x2)\n","- etiketlerinizi içeren bir sayısal dizi (vektör) Y (kırmızı:0, mavi:1).\n","\n","Önce verilerimizin neye benzediğini daha iyi anlayalım.\n","\n","**Alıştırma**: Kaç örneğiniz var? Ayrıca, \"X\" ve \"Y\" değişkenlerinin \"boyutları\" nedir?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnCglvcbb78b"},"outputs":[],"source":["### START CODE HERE ### (≈ 3 lines of code)\n","shape_X = X.shape\n","shape_Y = Y.shape\n","m = Y.shape[1]  # training set size\n","### END CODE HERE ###\n","\n","print ('The shape of X is: ' + str(shape_X))\n","print ('The shape of Y is: ' + str(shape_Y))\n","print ('I have m = %d training examples!' % (m))"]},{"cell_type":"markdown","metadata":{"id":"f9JUT_TUb78e"},"source":["**Beklenen Çıktı**:\n","       \n","<table style=\"width:20%\">\n","  <tr>\n","    <td>shape of X</td>\n","    <td> (2, 400) </td> \n","  </tr>\n","  <tr>\n","    <td>shape of Y</td>\n","    <td>(1, 400) </td> \n","  </tr>\n","  <tr>\n","    <td>m</td>\n","    <td> 400 </td> \n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"ZI_AFQPub78f"},"source":["## 3 - Basit Lojistik Regresyon\n","\n","Tam bir sinir ağı kurmadan önce, lojistik regresyonun bu problem üzerinde nasıl performans gösterdiğini görelim. Bunu yapmak için sklearn'in yerleşik fonksiyonlarını kullanabilirsiniz. Veri kümesinde bir lojistik regresyon sınıflandırıcısı eğitmek için aşağıdaki kodu çalıştırın."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uch2T59Tb78g"},"outputs":[],"source":["# Train the logistic regression classifier\n","clf = sklearn.linear_model.LogisticRegressionCV();\n","clf.fit(X.T, Y.T);"]},{"cell_type":"markdown","metadata":{"id":"1GT-0bL_b78h"},"source":["Artık bu modellerin karar sınırlarını çizebilirsiniz. Aşağıdaki kodu çalıştırın."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"u3LsXteRb78h"},"outputs":[],"source":["# Plot the decision boundary for logistic regression\n","plot_decision_boundary(lambda x: clf.predict(x), X, Y)\n","plt.title(\"Logistic Regression\")\n","\n","# Print accuracy\n","LR_predictions = clf.predict(X.T)\n","print ('Accuracy of logistic regression: %d ' % float((np.dot(Y, LR_predictions) + np.dot(1 - Y,1 - LR_predictions)) / float(Y.size) * 100) +\n","       '% ' + \"(percentage of correctly labelled datapoints)\")"]},{"cell_type":"markdown","metadata":{"id":"g7bqxD-Ib78i"},"source":["**Beklenen Çıktı**:\n","\n","<table style=\"width:20%\">\n","  <tr>\n","    <td>Accuracy</td>\n","    <td> 47% </td> \n","  </tr>\n","  \n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"IhN_jgEab78j"},"source":["**Yorum**: Veri kümesi doğrusal olarak ayrılamaz, bu nedenle lojistik regresyon iyi performans göstermez. Umarım bir sinir ağı daha iyisini yapar. Şimdi bunu deneyelim!"]},{"cell_type":"markdown","metadata":{"id":"ijX1zCMwb78j"},"source":["## 4 - Sinir Ağı modeli\n","\n","Lojistik regresyon, \"çiçek veri kümesi (flower dataset)\" üzerinde iyi çalışmadı. Şimdi tek bir gizli katmana sahip bir Sinir Ağı eğiteceksiniz.\n","\n","**İşte modelimiz**:\n","\n","<img src=\"https://drive.google.com/uc?id=1GG7lJxSkfx5cgS5sAX9BHV0Z_W_xcHpP\" style=\"width:600px;height:300px;\" alt=\"classification_kiank\" title=\"classification_kiank\">\n","\n","**Matematiksel olarak**:\n","\n","bir örnek için $x^{(i)}$:\n","$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n","$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n","$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n","$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n","$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n","\n","Tüm örneklerdeki tahminler göz önüne alındığında, $J$ maliyetini aşağıdaki gibi de hesaplayabilirsiniz:\n","$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n","\n","**Hatırlatma**: Bir Sinir Ağı oluşturmak için genel metodoloji şudur:\n","  1. Sinir ağı yapısını tanımlayın (giriş birimlerinin sayısı, gizli birimlerin sayısı, vb.).\n","  2. Modelin parametrelerini başlatın\n","  3. Döngü:\n","    - İleri yayılımı uygula\n","    - Hesaplama kaybı\n","    - Gradyanları elde etmek için geriye doğru yayılım uygulayın\n","    - Parametreleri güncelle (gradyan iniş)\n","\n","Genellikle 1-3 arasındaki adımları hesaplamak için yardımcı fonksiyonlar oluşturursunuz ve ardından bunları `nn_model()` dediğimiz tek bir fonksiyonda birleştirirsiniz. `nn_model()`i oluşturup doğru parametreleri öğrendikten sonra, yeni veriler üzerinde tahminler yapabilirsiniz."]},{"cell_type":"markdown","metadata":{"id":"_llh8d2Ub78k"},"source":["### 4.1 - Sinir ağı yapısını tanımlama ####\n","\n","**Alıştırma**: Üç değişken tanımlayın:\n","  - n_x: giriş katmanının boyutu\n","  - n_h: gizli katmanın boyutu (bunu 4 olarak ayarlayın)\n","  - n_y: çıktı katmanının boyutu\n","\n","**İpucu**: n_x ve n_y'yi bulmak için X ve Y şekillerini kullanın. Ayrıca, gizli katman boyutunu 4 olarak sabit kodlayın."]},{"cell_type":"code","source":["#@title test_cases\n","#test_case\n","def layer_sizes_test_case():\n","    np.random.seed(1)\n","    X_assess = np.random.randn(5, 3)\n","    Y_assess = np.random.randn(2, 3)\n","    return X_assess, Y_assess\n","\n","def initialize_parameters_test_case():\n","    n_x, n_h, n_y = 2, 4, 1\n","    return n_x, n_h, n_y\n","\n","\n","def forward_propagation_test_case():\n","    np.random.seed(1)\n","    X_assess = np.random.randn(2, 3)\n","    b1 = np.random.randn(4,1)\n","    b2 = np.array([[ -1.3]])\n","\n","    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n","        [-0.02136196,  0.01640271],\n","        [-0.01793436, -0.00841747],\n","        [ 0.00502881, -0.01245288]]),\n","     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n","     'b1': b1,\n","     'b2': b2}\n","\n","    return X_assess, parameters\n","\n","def compute_cost_test_case():\n","    np.random.seed(1)\n","    Y_assess = (np.random.randn(1, 3) > 0)\n","    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n","        [-0.02136196,  0.01640271],\n","        [-0.01793436, -0.00841747],\n","        [ 0.00502881, -0.01245288]]),\n","     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n","     'b1': np.array([[ 0.],\n","        [ 0.],\n","        [ 0.],\n","        [ 0.]]),\n","     'b2': np.array([[ 0.]])}\n","\n","    a2 = (np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]))\n","    \n","    return a2, Y_assess, parameters\n","\n","def backward_propagation_test_case():\n","    np.random.seed(1)\n","    X_assess = np.random.randn(2, 3)\n","    Y_assess = (np.random.randn(1, 3) > 0)\n","    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n","        [-0.02136196,  0.01640271],\n","        [-0.01793436, -0.00841747],\n","        [ 0.00502881, -0.01245288]]),\n","     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n","     'b1': np.array([[ 0.],\n","        [ 0.],\n","        [ 0.],\n","        [ 0.]]),\n","     'b2': np.array([[ 0.]])}\n","\n","    cache = {'A1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],\n","         [-0.05225116,  0.02725659, -0.02646251],\n","         [-0.02009721,  0.0036869 ,  0.02883756],\n","         [ 0.02152675, -0.01385234,  0.02599885]]),\n","  'A2': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]),\n","  'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],\n","         [-0.05229879,  0.02726335, -0.02646869],\n","         [-0.02009991,  0.00368692,  0.02884556],\n","         [ 0.02153007, -0.01385322,  0.02600471]]),\n","  'Z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]])}\n","    return parameters, cache, X_assess, Y_assess\n","\n","def update_parameters_test_case():\n","    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n","        [-0.02311792,  0.03137121],\n","        [-0.0169217 , -0.01752545],\n","        [ 0.00935436, -0.05018221]]),\n"," 'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n"," 'b1': np.array([[ -8.97523455e-07],\n","        [  8.15562092e-06],\n","        [  6.04810633e-07],\n","        [ -2.54560700e-06]]),\n"," 'b2': np.array([[  9.14954378e-05]])}\n","\n","    grads = {'dW1': np.array([[ 0.00023322, -0.00205423],\n","        [ 0.00082222, -0.00700776],\n","        [-0.00031831,  0.0028636 ],\n","        [-0.00092857,  0.00809933]]),\n"," 'dW2': np.array([[ -1.75740039e-05,   3.70231337e-03,  -1.25683095e-03,\n","          -2.55715317e-03]]),\n"," 'db1': np.array([[  1.05570087e-07],\n","        [ -3.81814487e-06],\n","        [ -1.90155145e-07],\n","        [  5.46467802e-07]]),\n"," 'db2': np.array([[ -1.08923140e-05]])}\n","    return parameters, grads\n","\n","def nn_model_test_case():\n","    np.random.seed(1)\n","    X_assess = np.random.randn(2, 3)\n","    Y_assess = (np.random.randn(1, 3) > 0)\n","    return X_assess, Y_assess\n","\n","def predict_test_case():\n","    np.random.seed(1)\n","    X_assess = np.random.randn(2, 3)\n","    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n","        [-0.02311792,  0.03137121],\n","        [-0.0169217 , -0.01752545],\n","        [ 0.00935436, -0.05018221]]),\n","     'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n","     'b1': np.array([[ -8.97523455e-07],\n","        [  8.15562092e-06],\n","        [  6.04810633e-07],\n","        [ -2.54560700e-06]]),\n","     'b2': np.array([[  9.14954378e-05]])}\n","    return parameters, X_assess\n"],"metadata":{"id":"smfCtMmmnyMO","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hBUEPQcFb78k"},"outputs":[],"source":["# GRADED FUNCTION: layer_sizes\n","\n","def layer_sizes(X, Y):\n","    \"\"\"\n","    Arguments:\n","    X -- input dataset of shape (input size, number of examples)\n","    Y -- labels of shape (output size, number of examples)\n","    \n","    Returns:\n","    n_x -- the size of the input layer\n","    n_h -- the size of the hidden layer\n","    n_y -- the size of the output layer\n","    \"\"\"\n","    ### START CODE HERE ### (≈ 3 lines of code)\n","    n_x = X.shape[0] # size of input layer\n","    n_h = 4\n","    n_y = Y.shape[0] # size of output layer\n","    ### END CODE HERE ###\n","    return (n_x, n_h, n_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5KuOP2Ib78l"},"outputs":[],"source":["X_assess, Y_assess = layer_sizes_test_case()\n","(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\n","print(\"The size of the input layer is: n_x = \" + str(n_x))\n","print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n","print(\"The size of the output layer is: n_y = \" + str(n_y))"]},{"cell_type":"markdown","metadata":{"id":"3ANKfo-Hb78l"},"source":["**Beklenen Çıktı** (bunlar ağınız için kullanacağınız boyutlar değildir, sadece az önce kodladığınız işlevi değerlendirmek için kullanılırlar).\n","\n","<table style=\"width:20%\">\n","  <tr>\n","    <td>n_x</td>\n","    <td> 5 </td> \n","  </tr>\n","  <tr>\n","    <td>n_h</td>\n","    <td> 4 </td> \n","  </tr>\n","  <tr>\n","    <td>n_y</td>\n","    <td> 2 </td> \n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"DA7NeymGb78m"},"source":["### 4.2 - Modelin parametrelerini sıfırlayın ####\n","\n","**Alıştırma**: `initialize_parameters()` fonksiyonunu uygulayın.\n","\n","**Talimatlar**:\n","- Parametrelerinizin boyutlarının doğru olduğundan emin olun. Gerekirse yukarıdaki sinir ağı şekline bakın.\n","- Ağırlık matrislerini rastgele değerlerle başlatacaksınız.\n","  - Bir şekil matrisini (a,b) rastgele başlatmak için `np.random.randn(a,b) * 0.01` kullanın.\n","\n","- Önyargı vektörlerini (bias vectors) sıfır olarak başlatacaksınız.\n","  - (a,b) şeklindeki bir matrisi sıfırlarla başlatmak için `np.zeros((a,b))` kullanın."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVXj87wBb78m"},"outputs":[],"source":["# GRADED FUNCTION: initialize_parameters\n","\n","def initialize_parameters(n_x, n_h, n_y):\n","    \"\"\"\n","    Argument:\n","    n_x -- size of the input layer\n","    n_h -- size of the hidden layer\n","    n_y -- size of the output layer\n","    \n","    Returns:\n","    params -- python dictionary containing your parameters:\n","                    W1 -- weight matrix of shape (n_h, n_x)\n","                    b1 -- bias vector of shape (n_h, 1)\n","                    W2 -- weight matrix of shape (n_y, n_h)\n","                    b2 -- bias vector of shape (n_y, 1)\n","    \"\"\"\n","    \n","    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n","    \n","    ### START CODE HERE ### (≈ 4 lines of code)\n","    W1 = np.random.randn(n_h, n_x) * 0.01\n","    b1 = np.zeros(shape=(n_h, 1))\n","    W2 = np.random.randn(n_y, n_h) * 0.01\n","    b2 = np.zeros(shape=(n_y, 1))\n","    ### END CODE HERE ###\n","    \n","    assert (W1.shape == (n_h, n_x))\n","    assert (b1.shape == (n_h, 1))\n","    assert (W2.shape == (n_y, n_h))\n","    assert (b2.shape == (n_y, 1))\n","    \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    \n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ue07qhfb78n"},"outputs":[],"source":["n_x, n_h, n_y = initialize_parameters_test_case()\n","\n","parameters = initialize_parameters(n_x, n_h, n_y)\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"qYvN31QHb78o"},"source":["**Beklenen Çıktı**:\n","\n","<table style=\"width:90%\">\n","  <tr>\n","    <td>W1</td>\n","    <td> [[-0.00416758 -0.00056267]\n"," [-0.02136196  0.01640271]\n"," [-0.01793436 -0.00841747]\n"," [ 0.00502881 -0.01245288]] </td> \n","  </tr>\n","  <tr>\n","    <td>b1</td>\n","    <td> [[ 0.]\n"," [ 0.]\n"," [ 0.]\n"," [ 0.]] </td> \n","  </tr>\n","  <tr>\n","    <td>W2</td>\n","    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> \n","  </tr>\n","  <tr>\n","    <td>b2</td>\n","    <td> [[ 0.]] </td> \n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"2K_9Ablgb78o"},"source":["### 4.3 - Döngü ####\n","\n","**Soru**: `forward_propagation()` öğesini uygulayın.\n","\n","**Talimatlar**:\n","- Sınıflandırıcınızın matematiksel temsiline yukarıda bakın.\n","- `sigmoid()` fonksiyonunu kullanabilirsiniz. Yukarıda yardımcı fonksiyonlar içerisinde tanımlanmıştı.\n","- `np.tanh()` fonksiyonunu kullanabilirsiniz. Numpy kütüphanesinin bir parçasıdır.\n","- Uygulamanız gereken adımlar şunlardır:\n","     1. Her parametreyi \"parametreler\" sözlüğünden (`initialize_parameters()` çıktısıdır) `parameters[\"..\"]` kullanarak alın.\n","     2. İleri Yayılımı uygulayın. \n","$Z^{[1]}, A^{[1]}, Z^{[2]}$ ve $A^{[2]}$ \n","\n","(eğitimdeki tüm örneklerle ilgili tüm tahminlerinizin vektörü) hesaplayın Ayarlamak).\n","- Geri yayılımda ihtiyaç duyulan değerler \"`cache`\"de saklanır. \"`cache`\", geri yayılım işlevine bir girdi olarak verilecektir.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K26pvCOrb78q"},"outputs":[],"source":["# GRADED FUNCTION: forward_propagation\n","\n","def forward_propagation(X, parameters):\n","    \"\"\"\n","    Argument:\n","    X -- input data of size (n_x, m)\n","    parameters -- python dictionary containing your parameters (output of initialization function)\n","    \n","    Returns:\n","    A2 -- The sigmoid output of the second activation\n","    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n","    \"\"\"\n","    # Retrieve each parameter from the dictionary \"parameters\"\n","    ### START CODE HERE ### (≈ 4 lines of code)\n","    W1 = parameters['W1']\n","    b1 = parameters['b1']\n","    W2 = parameters['W2']\n","    b2 = parameters['b2']\n","    ### END CODE HERE ###\n","    \n","    # Implement Forward Propagation to calculate A2 (probabilities)\n","    ### START CODE HERE ### (≈ 4 lines of code)\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = np.tanh(Z1)\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = sigmoid(Z2)\n","    ### END CODE HERE ###\n","    \n","    assert(A2.shape == (1, X.shape[1]))\n","    \n","    cache = {\"Z1\": Z1,\n","             \"A1\": A1,\n","             \"Z2\": Z2,\n","             \"A2\": A2}\n","    \n","    return A2, cache"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43EFIhbAb78q"},"outputs":[],"source":["X_assess, parameters = forward_propagation_test_case()\n","\n","A2, cache = forward_propagation(X_assess, parameters)\n","\n","# Note: we use the mean here just to make sure that your output matches ours. \n","print(np.mean(cache['Z1']), np.mean(cache['A1']), np.mean(cache['Z2']), np.mean(cache['A2']))"]},{"cell_type":"markdown","metadata":{"id":"Ef8ezp6bb78q"},"source":["**Beklenen Çıktı**:\n","<table style=\"width:55%\">\n","  <tr>\n","    <td> -0.000499755777742 -0.000496963353232 0.000438187450959 0.500109546852 </td> \n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"_l5RjAWHb78s"},"source":["Artık her örnek için $a^{[2](i)}$ içeren $A^{[2]}$ hesapladığınıza göre, aşağıdaki gibi maliyet fonksiyonunu hesaplayabilirsiniz. :\n","\n","$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n","\n","**Alıştırma**: $J$ maliyetinin değerini hesaplamak için `compute_cost()` öğesini uygulayın.\n","\n","**Talimatlar**:\n","- Çapraz entropi kaybını uygulamanın birçok yolu vardır. Size yardımcı olmak için, nasıl uygulayacağımızı size veriyoruz\n","$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n","```python\n","logprobs = np.multiply(np.log(A2),Y)\n","cost = - np.sum(logprobs)                # no need to use a for loop!\n","```\n","\n","(`np.multiply()` ve ardından `np.sum()` veya doğrudan `np.dot()` kullanabilirsiniz).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMO2piMYb78t"},"outputs":[],"source":["# GRADED FUNCTION: compute_cost\n","\n","def compute_cost(A2, Y, parameters):\n","    \"\"\"\n","    Computes the cross-entropy cost given in equation (13)\n","    \n","    Arguments:\n","    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n","    Y -- \"true\" labels vector of shape (1, number of examples)\n","    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n","    \n","    Returns:\n","    cost -- cross-entropy cost given equation (13)\n","    \"\"\"\n","    \n","    m = Y.shape[1] # number of example\n","    \n","    # Retrieve W1 and W2 from parameters\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    W1 = parameters['W1']\n","    W2 = parameters['W2']\n","    ### END CODE HERE ###\n","    \n","    # Compute the cross-entropy cost\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n","    cost = - np.sum(logprobs) / m\n","    ### END CODE HERE ###\n","    \n","    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n","                                # E.g., turns [[17]] into 17 \n","    assert(isinstance(cost, float))\n","    \n","    return cost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddX6GKFFb78u"},"outputs":[],"source":["A2, Y_assess, parameters = compute_cost_test_case()\n","\n","print(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"]},{"cell_type":"markdown","metadata":{"id":"nv6Jb64kb78v"},"source":["**Beklenen Çıktı**:\n","<table style=\"width:20%\">\n","  <tr>\n","    <td>cost</td>\n","    <td> 0.692919893776 </td> \n","  </tr>\n","  \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"KYQVljV7b78w"},"source":["**Soru**: `backward_propagation()` fonksiyonunu uygulayın.\n","\n","**Talimatlar**:\n","Geri yayılım, derin öğrenmede genellikle en zor (en matematiksel) kısımdır. Size yardımcı olmak için, geri yayılımla ilgili dersten bir slayt yine burada. Vektörleştirilmiş bir uygulama oluşturduğunuz için bu slaydın sağındaki altı denklemi kullanmak isteyeceksiniz.\n","\n","<img src=\"https://drive.google.com/uc?id=1nbhCtgM93yCN5QYaCXe3osezFCVgeZIl\" style=\"width:600px;height:300px;\" alt=\"grad_summary\" title=\"grad_summary\">\n","\n","<!--\n","$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n","\n","$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n","\n","$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n","\n","$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $\n","\n","$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n","\n","$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n","\n","- Note that $*$ denotes elementwise multiplication.\n","- The notation you will use is common in deep learning coding:\n","    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n","    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n","    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n","    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n","    \n","!-->\n","\n","**İpuçları:**\n","dZ1'i hesaplamak için $g^{[1]'}(Z^{[1]})$ hesaplamanız gerekir. \n","  $g^{[1]}(.)$ tanh aktivasyon işlevi olduğundan, $a = g^{[1]}(z)$ ise $g^{[1]'}(z) = 1- a^2$. \n","`(1 - np.power(A1, 2))` kullanarak $g^{[1]'}(Z^{[1]})$'i hesaplayabilirsin."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7QSN0jPb78x"},"outputs":[],"source":["# GRADED FUNCTION: backward_propagation\n","\n","def backward_propagation(parameters, cache, X, Y):\n","    \"\"\"\n","    Implement the backward propagation using the instructions above.\n","    \n","    Arguments:\n","    parameters -- python dictionary containing our parameters \n","    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n","    X -- input data of shape (2, number of examples)\n","    Y -- \"true\" labels vector of shape (1, number of examples)\n","    \n","    Returns:\n","    grads -- python dictionary containing your gradients with respect to different parameters\n","    \"\"\"\n","    m = X.shape[1]\n","    \n","    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    W1 = parameters['W1']\n","    W2 = parameters['W2']\n","    ### END CODE HERE ###\n","        \n","    # Retrieve also A1 and A2 from dictionary \"cache\".\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    A1 = cache['A1']\n","    A2 = cache['A2']\n","    ### END CODE HERE ###\n","    \n","    # Backward propagation: calculate dW1, db1, dW2, db2. \n","    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)\n","    dZ2= A2 - Y\n","    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n","    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n","    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n","    dW1 = (1 / m) * np.dot(dZ1, X.T)\n","    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n","    ### END CODE HERE ###\n","    \n","    grads = {\"dW1\": dW1,\n","             \"db1\": db1,\n","             \"dW2\": dW2,\n","             \"db2\": db2}\n","    \n","    return grads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzPQytURb78z"},"outputs":[],"source":["parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n","\n","grads = backward_propagation(parameters, cache, X_assess, Y_assess)\n","print (\"dW1 = \"+ str(grads[\"dW1\"]))\n","print (\"db1 = \"+ str(grads[\"db1\"]))\n","print (\"dW2 = \"+ str(grads[\"dW2\"]))\n","print (\"db2 = \"+ str(grads[\"db2\"]))"]},{"cell_type":"markdown","metadata":{"id":"834mYAiKb780"},"source":["**Beklenen Çıktı**:\n","\n","<table style=\"width:80%\">\n","  <tr>\n","    <td>dW1</td>\n","    <td> [[ 0.01018708 -0.00708701]\n"," [ 0.00873447 -0.0060768 ]\n"," [-0.00530847  0.00369379]\n"," [-0.02206365  0.01535126]] </td> \n","  </tr>\n","  <tr>\n","    <td>db1</td>\n","    <td>  [[-0.00069728]\n"," [-0.00060606]\n"," [ 0.000364  ]\n"," [ 0.00151207]] </td> \n","  </tr>\n","  <tr>\n","    <td>dW2</td>\n","    <td> [[ 0.00363613  0.03153604  0.01162914 -0.01318316]] </td> \n","  </tr>\n","  <tr>\n","    <td>db2</td>\n","    <td> [[ 0.06589489]] </td> \n","  </tr>\n","</table>  "]},{"cell_type":"markdown","metadata":{"id":"Hhsw6wnmb780"},"source":["**Soru**: Güncelleme kuralını uygulayın. Gradyan iniş kullanın. (W1, b1, W2, b2)'yi güncellemek için (dW1, db1, dW2, db2) kullanmanız gerekir.\n","\n","**Genel gradyan iniş kuralı**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ burada $\\alpha$ öğrenme oranıdır ve $\\theta$ bir parametreyi temsil eder.\n","\n","**Resim**: İyi bir öğrenme oranına (yakınsayan) ve kötü bir öğrenme oranına (uzaklaşan) sahip gradyan iniş algoritması.\n","\n","<img src=\"https://drive.google.com/uc?id=1zEB5wdgCLh7KTKTEypMKr0czFIKoX5F_\" style=\"width:650px;height:400px;\" alt=\"sgd\" title=\"sgd\">\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcj02CsUb781"},"outputs":[],"source":["# GRADED FUNCTION: update_parameters\n","\n","def update_parameters(parameters, grads, learning_rate=1.2):\n","    \"\"\"\n","    Updates parameters using the gradient descent update rule given above\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    grads -- python dictionary containing your gradients \n","    \n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","    \"\"\"\n","    # Retrieve each parameter from the dictionary \"parameters\"\n","    ### START CODE HERE ### (≈ 4 lines of code)\n","    W1 = parameters['W1']\n","    b1 = parameters['b1']\n","    W2 = parameters['W2']\n","    b2 = parameters['b2']\n","    ### END CODE HERE ###\n","    \n","    # Retrieve each gradient from the dictionary \"grads\"\n","    ### START CODE HERE ### (≈ 4 lines of code)\n","    dW1 = grads['dW1']\n","    db1 = grads['db1']\n","    dW2 = grads['dW2']\n","    db2 = grads['db2']\n","    ## END CODE HERE ###\n","    \n","    # Update rule for each parameter\n","    ### START CODE HERE ### (≈ 4 lines of code)\n","    W1 = W1 - learning_rate * dW1\n","    b1 = b1 - learning_rate * db1\n","    W2 = W2 - learning_rate * dW2\n","    b2 = b2 - learning_rate * db2\n","    ### END CODE HERE ###\n","    \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    \n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"x8oDaRBNb782"},"outputs":[],"source":["parameters, grads = update_parameters_test_case()\n","parameters = update_parameters(parameters, grads)\n","\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"ejG_Jjk0b783"},"source":["**Beklenen Çıktı**:\n","\n","\n","<table style=\"width:80%\">\n","  <tr>\n","    <td>W1</td>\n","    <td> [[-0.00643025  0.01936718]\n"," [-0.02410458  0.03978052]\n"," [-0.01653973 -0.02096177]\n"," [ 0.01046864 -0.05990141]]</td> \n","  </tr>\n","  <tr>\n","    <td>b1</td>\n","    <td> [[ -1.02420756e-06]\n"," [  1.27373948e-05]\n"," [  8.32996807e-07]\n"," [ -3.20136836e-06]]</td> \n","  </tr>\n","  <tr>\n","    <td>W2</td>\n","    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td> \n","  </tr>\n","  <tr>\n","    <td>b2</td>\n","    <td> [[ 0.00010457]] </td> \n","  </tr>\n","</table>  "]},{"cell_type":"markdown","metadata":{"id":"BN8g-58vb784"},"source":["### 4.4 - 4.1, 4.2 ve 4.3 parçalarını nn_model() içinde entegre edin ####\n","\n","**Soru**: Sinir ağı modelinizi `nn_model()` içinde oluşturun.\n","\n","**Talimatlar**: Sinir ağı modeli, önceki fonksiyonları doğru sırada kullanmalıdır."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhqWV8GXb784"},"outputs":[],"source":["# GRADED FUNCTION: nn_model\n","\n","def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False):\n","    \"\"\"\n","    Arguments:\n","    X -- dataset of shape (2, number of examples)\n","    Y -- labels of shape (1, number of examples)\n","    n_h -- size of the hidden layer\n","    num_iterations -- Number of iterations in gradient descent loop\n","    print_cost -- if True, print the cost every 1000 iterations\n","    \n","    Returns:\n","    parameters -- parameters learnt by the model. They can then be used to predict.\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    n_x = layer_sizes(X, Y)[0]\n","    n_y = layer_sizes(X, Y)[2]\n","    \n","    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n","    ### START CODE HERE ### (≈ 5 lines of code)\n","    parameters = initialize_parameters(n_x, n_h, n_y)\n","    W1 = parameters['W1']\n","    b1 = parameters['b1']\n","    W2 = parameters['W2']\n","    b2 = parameters['b2']\n","    ### END CODE HERE ###\n","    \n","    # Loop (gradient descent)\n","\n","    for i in range(0, num_iterations):\n","         \n","        ### START CODE HERE ### (≈ 4 lines of code)\n","        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n","        A2, cache = forward_propagation(X, parameters)\n","        \n","        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n","        cost = compute_cost(A2, Y, parameters)\n"," \n","        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n","        grads = backward_propagation(parameters, cache, X, Y)\n"," \n","        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n","        parameters = update_parameters(parameters, grads)\n","        \n","        ### END CODE HERE ###\n","        \n","        # Print the cost every 1000 iterations\n","        if print_cost and i % 1000 == 0:\n","            print (\"Cost after iteration %i: %f\" % (i, cost))\n","\n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8YPGrinb786"},"outputs":[],"source":["X_assess, Y_assess = nn_model_test_case()\n","\n","parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=False)\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"atL3-z_Rb786"},"source":["**Beklenen Çıktı**:\n","\n","<table style=\"width:90%\">\n","  <tr>\n","    <td>W1</td>\n","    <td> [[-4.18494056  5.33220609]\n"," [-7.52989382  1.24306181]\n"," [-4.1929459   5.32632331]\n"," [ 7.52983719 -1.24309422]]</td> \n","  </tr>\n","  <tr>\n","    <td>b1</td>\n","    <td> [[ 2.32926819]\n"," [ 3.79458998]\n"," [ 2.33002577]\n"," [-3.79468846]]</td> \n","  </tr>\n","  <tr>\n","    <td>W2</td>\n","    <td> [[-6033.83672146 -6008.12980822 -6033.10095287  6008.06637269]] </td> \n","  </tr>\n","  <tr>\n","    <td>b2</td>\n","    <td> [[-52.66607724]] </td> \n","  </tr>\n","</table>  "]},{"cell_type":"markdown","metadata":{"id":"1tMV2RpIb788"},"source":["### 4.5 Tahminler\n","\n","**Soru**: Tahmin() oluşturarak tahmin yapmak için modelinizi kullanın.\n","Sonuçları tahmin etmek için ileri yayılımı kullanın.\n","\n","**Hatırlatma**\n","\n","predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n","1 & \\text{if}\\ activation > 0.5 \\\\\n","0 & \\text{otherwise}\n","\\end{cases}$  \n","    \n","Örnek olarak, bir eşiğe dayalı olarak X matrisinin girişlerini 0 ve 1 olarak ayarlamak isterseniz şunları yaparsınız: ```X_new = (X > threshold)```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Qor7r3yb789"},"outputs":[],"source":["# GRADED FUNCTION: predict\n","\n","def predict(parameters, X):\n","    \"\"\"\n","    Using the learned parameters, predicts a class for each example in X\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    X -- input data of size (n_x, m)\n","    \n","    Returns\n","    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n","    \"\"\"\n","    \n","    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    A2, cache = forward_propagation(X, parameters)\n","    predictions = np.round(A2)\n","    ### END CODE HERE ###\n","    \n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nd1NZ6Nlb78-"},"outputs":[],"source":["parameters, X_assess = predict_test_case()\n","\n","predictions = predict(parameters, X_assess)\n","print(\"predictions mean = \" + str(np.mean(predictions)))"]},{"cell_type":"markdown","metadata":{"id":"CEVsnd0Mb78_"},"source":["**Beklenen Çıktı**: \n","\n","\n","<table style=\"width:40%\">\n","  <tr>\n","    <td>predictions mean</td>\n","    <td> 0.666666666667 </td> \n","  </tr>\n","  \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"WBNeVLeGb79A"},"source":["Modeli çalıştırmanın ve düzlemsel bir veri kümesinde nasıl performans gösterdiğini görmenin zamanı geldi. Modelinizi $n_h$ gizli birimlerinden oluşan tek bir gizli katmanla test etmek için aşağıdaki kodu çalıştırın."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"26nAdWgjb79D"},"outputs":[],"source":["# Build a model with a n_h-dimensional hidden layer\n","parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)\n","\n","# Plot the decision boundary\n","plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n","plt.title(\"Decision Boundary for hidden layer size \" + str(4))"]},{"cell_type":"markdown","metadata":{"id":"inBbyREqb79E"},"source":["**Beklenen Çıktı**:\n","\n","<table style=\"width:40%\">\n","  <tr>\n","    <td>Cost after iteration 9000</td>\n","    <td> 0.218607 </td> \n","  </tr>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t84mKXyfb79F"},"outputs":[],"source":["# Print accuracy\n","predictions = predict(parameters, X)\n","print ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')"]},{"cell_type":"markdown","metadata":{"id":"9uu3SEqCb79K"},"source":["**Beklenen Çıktı**: \n","\n","<table style=\"width:15%\">\n","  <tr>\n","    <td>Accuracy</td>\n","    <td> 90% </td> \n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Rd-IycQ6b79L"},"source":["Doğruluk, Lojistik Regresyona kıyasla gerçekten yüksektir. Model, çiçeğin yaprak desenlerini öğrendi! Sinir ağları, lojistik regresyondan farklı olarak, oldukça doğrusal olmayan karar sınırlarını bile öğrenebilir.\n","\n","Şimdi birkaç gizli katman boyutunu deneyelim."]},{"cell_type":"markdown","metadata":{"id":"7EGf6M5Fb79M"},"source":["### 4.6 - Gizli katman boyutunu ayarlama  ###\n","\n","Aşağıdaki kodu çalıştırın. 1-2 dakika sürebilir. Çeşitli gizli katman boyutları için modelin farklı davranışlarını gözlemleyeceksiniz."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"osVzCvKZb79M"},"outputs":[],"source":["# This may take about 2 minutes to run\n","plt.figure(figsize=(16, 32))\n","hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n","\n","for i, n_h in enumerate(hidden_layer_sizes):\n","    plt.subplot(5, 2, i + 1)\n","    plt.title('Hidden Layer of size %d' % n_h)\n","\n","    parameters = nn_model(X, Y, n_h, num_iterations=5000)\n","\n","    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n","\n","    predictions = predict(parameters, X)\n","    \n","    accuracy = float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100)\n","    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"]},{"cell_type":"markdown","metadata":{"id":"fSTxtWHCb79O"},"source":["**Tercüme**:\n","- Daha büyük modeller (daha fazla gizli birim içeren), eğitim kümesinde aşırı uyuma sebep olabilir.\n","- En iyi gizli katman boyutu n_h = 5 civarında görünüyor. Gerçekten de, buradaki bir değer, gözle görülür bir aşırı uyum sağlamadan verilere iyi uyuyor gibi görünüyor.\n","- Daha sonra, çok büyük modelleri (n_h = 50 gibi) aşıyı uyuma sebebiyet vermeden düzenlileştirme (regularization) hakkında da bilgi edineceksiniz."]},{"cell_type":"markdown","metadata":{"id":"mmWCs5mnb79O"},"source":["**İsteğe bağlı sorular**:\n","\n","Dilerseniz keşfedebileceğiniz bazı isteğe bağlı/not verilmeyen sorular:\n","- Bir sigmoid aktivasyonu veya bir ReLU aktivasyonu için tanh aktivasyonunu değiştirdiğinizde ne olur?\n","- Learning_rate ile oynayın. Ne oluyor?\n","- Ya veri setini değiştirirsek? (Aşağıdaki 5. bölüme bakın!)"]},{"cell_type":"markdown","metadata":{"id":"4zL7uZheb79P"},"source":["<font color='blue'>\n","**Şunu yapmayı öğrendiniz:**\n","\n","- Gizli bir katmanla eksiksiz bir sinir ağı oluşturma.\n","- Doğrusal olmayan bir birimi iyi kullanıldı.\n","- İleri yayılım ve geri yayılım uygulandı ve bir sinir ağı eğitildi.\n","- Fazla takma da dahil olmak üzere gizli katman boyutunu değiştirmenin etkisi görüldü."]},{"cell_type":"markdown","metadata":{"id":"nmDZGXm5b79Q"},"source":["## 5) Diğer veri kümelerindeki performans"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"CiLZzQx9b79Q"},"source":["İsterseniz, aşağıdaki veri kümelerinin her biri için tüm çalışma dosyasını yeniden çalıştırabilirsiniz."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"TLNdYfXbb79R"},"outputs":[],"source":["# Datasets\n","noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n","\n","datasets = {\"noisy_circles\": noisy_circles,\n","            \"noisy_moons\": noisy_moons,\n","            \"blobs\": blobs,\n","            \"gaussian_quantiles\": gaussian_quantiles}\n","\n","### START CODE HERE ### (choose your dataset)\n","dataset = \"noisy_moons\"\n","### END CODE HERE ###\n","\n","X, Y = datasets[dataset]\n","X, Y = X.T, Y.reshape(1, Y.shape[0])\n","\n","# make blobs binary\n","if dataset == \"blobs\":\n","    Y = Y % 2\n","\n","# Visualize the data\n","plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"]},{"cell_type":"markdown","metadata":{"id":"OTlgT5jHb79S"},"source":["Referanslar:\n","- http://scs.ryerson.ca/~aharley/neural-networks/\n","- http://cs231n.github.io/neural-networks-case-study/"]},{"cell_type":"code","source":[""],"metadata":{"id":"XFyOBKybn_Ns"},"execution_count":null,"outputs":[]}],"metadata":{"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"wRuwL","launcher_item_id":"NI888"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"Day9_ANN_2_Planar data classification with one hidden layer.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}