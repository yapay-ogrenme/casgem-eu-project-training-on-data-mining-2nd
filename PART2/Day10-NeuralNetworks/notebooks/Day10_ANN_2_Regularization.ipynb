{"cells":[{"cell_type":"markdown","metadata":{"id":"70kywzINW4wC"},"source":["# Düzenlileştirme (Regularization)\n","\n","Derin Öğrenme modelleri o kadar fazla esnekliğe ve kapasiteye sahiptir ki, eğitim veri kümesi yeterince büyük değilse **aşırı uyum (overfitting) ciddi bir sorun olabilir**. Elbette eğitim kümesinde iyi sonuç veriyor, ancak öğrenilen ağ daha önce hiç görmediği **yeni örneklere genelleme yapmıyor**!\n","\n","Öncelikle kullanacağımız paketleri import edelim."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GS4pAxfyW4wE"},"outputs":[],"source":["# import packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","#from reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\n","#from reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\n","import sklearn\n","import sklearn.datasets\n","import sklearn.linear_model\n","import scipy.io\n","#from testCases import *\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["ROOT_DIR = \"https://raw.githubusercontent.com/yapay-ogrenme/casgem-eu-project-training-on-data-mining/main/PART2/Day9-NeuralNetworks/notebooks/\"\n","\n","DATASET_PATH = ROOT_DIR + \"datasets/\""],"metadata":{"id":"F1N6grDhZ6lg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATASET_2D = DATASET_PATH + 'data.mat'\n","\n","!wget $DATASET_2D"],"metadata":{"id":"ELYVfGCsZ7Ep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Yardımcı Fonksiyonlar\n","def sigmoid(x):\n","    \"\"\"\n","    Compute the sigmoid of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(x)\n","    \"\"\"\n","    s = 1/(1+np.exp(-x))\n","    return s\n","\n","def relu(x):\n","    \"\"\"\n","    Compute the relu of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- relu(x)\n","    \"\"\"\n","    s = np.maximum(0,x)\n","    \n","    return s\n","\n","def load_planar_dataset(seed):\n","    \n","    np.random.seed(seed)\n","    \n","    m = 400 # number of examples\n","    N = int(m/2) # number of points per class\n","    D = 2 # dimensionality\n","    X = np.zeros((m,D)) # data matrix where each row is a single example\n","    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n","    a = 4 # maximum ray of the flower\n","\n","    for j in range(2):\n","        ix = range(N*j,N*(j+1))\n","        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n","        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n","        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n","        Y[ix] = j\n","        \n","    X = X.T\n","    Y = Y.T\n","\n","    return X, Y\n","\n","def initialize_parameters(layer_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    b1 -- bias vector of shape (layer_dims[l], 1)\n","                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n","                    bl -- bias vector of shape (1, layer_dims[l])\n","                    \n","    Tips:\n","    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n","    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n","    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims) # number of layers in the network\n","\n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        \n","        assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n","        assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n","\n","        \n","    return parameters\n","\n","def forward_propagation(X, parameters):\n","    \"\"\"\n","    Implements the forward propagation (and computes the loss) presented in Figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","                    W1 -- weight matrix of shape ()\n","                    b1 -- bias vector of shape ()\n","                    W2 -- weight matrix of shape ()\n","                    b2 -- bias vector of shape ()\n","                    W3 -- weight matrix of shape ()\n","                    b3 -- bias vector of shape ()\n","    \n","    Returns:\n","    loss -- the loss function (vanilla logistic loss)\n","    \"\"\"\n","        \n","    # retrieve parameters\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","    \n","    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = relu(Z1)\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = relu(Z2)\n","    Z3 = np.dot(W3, A2) + b3\n","    A3 = sigmoid(Z3)\n","    \n","    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n","    \n","    return A3, cache\n","\n","def backward_propagation(X, Y, cache):\n","    \"\"\"\n","    Implement the backward propagation presented in figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n","    cache -- cache output from forward_propagation()\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n","    \"\"\"\n","    m = X.shape[1]\n","    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","    \n","    dZ3 = A3 - Y\n","    dW3 = 1./m * np.dot(dZ3, A2.T)\n","    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n","    \n","    dA2 = np.dot(W3.T, dZ3)\n","    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n","    dW2 = 1./m * np.dot(dZ2, A1.T)\n","    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n","    \n","    dA1 = np.dot(W2.T, dZ2)\n","    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","    dW1 = 1./m * np.dot(dZ1, X.T)\n","    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n","    \n","    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n","                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n","                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients\n","\n","def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Update parameters using gradient descent\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters:\n","                    parameters['W' + str(i)] = Wi\n","                    parameters['b' + str(i)] = bi\n","    grads -- python dictionary containing your gradients for each parameters:\n","                    grads['dW' + str(i)] = dWi\n","                    grads['db' + str(i)] = dbi\n","    learning_rate -- the learning rate, scalar.\n","    \n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","    \"\"\"\n","    \n","    n = len(parameters) // 2 # number of layers in the neural networks\n","\n","    # Update rule for each parameter\n","    for k in range(n):\n","        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n","        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n","        \n","    return parameters\n","\n","def predict(X, y, parameters):\n","    \"\"\"\n","    This function is used to predict the results of a  n-layer neural network.\n","    \n","    Arguments:\n","    X -- data set of examples you would like to label\n","    parameters -- parameters of the trained model\n","    \n","    Returns:\n","    p -- predictions for the given dataset X\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    p = np.zeros((1,m), dtype = np.int)\n","    \n","    # Forward propagation\n","    a3, caches = forward_propagation(X, parameters)\n","    \n","    # convert probas to 0/1 predictions\n","    for i in range(0, a3.shape[1]):\n","        if a3[0,i] > 0.5:\n","            p[0,i] = 1\n","        else:\n","            p[0,i] = 0\n","\n","    # print results\n","\n","    #print (\"predictions: \" + str(p[0,:]))\n","    #print (\"true labels: \" + str(y[0,:]))\n","    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n","    \n","    return p\n","\n","def compute_cost(a3, Y):\n","    \"\"\"\n","    Implement the cost function\n","    \n","    Arguments:\n","    a3 -- post-activation, output of forward propagation\n","    Y -- \"true\" labels vector, same shape as a3\n","    \n","    Returns:\n","    cost - value of the cost function\n","    \"\"\"\n","    m = Y.shape[1]\n","    \n","    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n","    cost = 1./m * np.nansum(logprobs)\n","    \n","    return cost\n","\n","def load_dataset():\n","    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    \n","    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    \n","    train_set_x_orig = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n","    test_set_x_orig = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n","    \n","    train_set_x = train_set_x_orig/255\n","    test_set_x = test_set_x_orig/255\n","\n","    return train_set_x, train_set_y, test_set_x, test_set_y, classes\n","\n","\n","def predict_dec(parameters, X):\n","    \"\"\"\n","    Used for plotting decision boundary.\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    X -- input data of size (m, K)\n","    \n","    Returns\n","    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n","    \"\"\"\n","    \n","    # Predict using forward propagation and a classification threshold of 0.5\n","    a3, cache = forward_propagation(X, parameters)\n","    predictions = (a3>0.5)\n","    return predictions\n","\n","def load_planar_dataset(randomness, seed):\n","    \n","    np.random.seed(seed)\n","    \n","    m = 50\n","    N = int(m/2) # number of points per class\n","    D = 2 # dimensionality\n","    X = np.zeros((m,D)) # data matrix where each row is a single example\n","    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n","    a = 2 # maximum ray of the flower\n","\n","    for j in range(2):\n","        \n","        ix = range(N*j,N*(j+1))\n","        if j == 0:\n","            t = np.linspace(j, 4*3.1415*(j+1),N) #+ np.random.randn(N)*randomness # theta\n","            r = 0.3*np.square(t) + np.random.randn(N)*randomness # radius\n","        if j == 1:\n","            t = np.linspace(j, 2*3.1415*(j+1),N) #+ np.random.randn(N)*randomness # theta\n","            r = 0.2*np.square(t) + np.random.randn(N)*randomness # radius\n","            \n","        X[ix] = np.c_[r*np.cos(t), r*np.sin(t)]\n","        Y[ix] = j\n","        \n","    X = X.T\n","    Y = Y.T\n","\n","    return X, Y\n","\n","def plot_decision_boundary(model, X, y):\n","    # Set min and max values and give it some padding\n","    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n","    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n","    h = 0.01\n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    # Predict the function value for the whole grid\n","    Z = model(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","    plt.ylabel('x2')\n","    plt.xlabel('x1')\n","    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n","    plt.show()\n","    \n","def load_2D_dataset():\n","    data = scipy.io.loadmat('data.mat')\n","    train_X = data['X'].T\n","    train_Y = data['y'].T\n","    test_X = data['Xval'].T\n","    test_Y = data['yval'].T\n","\n","    plt.scatter(train_X[0, :], train_X[1, :], c=train_Y, s=40, cmap=plt.cm.Spectral);\n","    \n","    return train_X, train_Y, test_X, test_Y"],"metadata":{"cellView":"form","id":"xk04Mj_AY5Ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Yardımcı Test Fonksiyonları\n","import numpy as np\n","\n","def compute_cost_with_regularization_test_case():\n","    np.random.seed(1)\n","    Y_assess = np.array([[1, 1, 0, 1, 0]])\n","    W1 = np.random.randn(2, 3)\n","    b1 = np.random.randn(2, 1)\n","    W2 = np.random.randn(3, 2)\n","    b2 = np.random.randn(3, 1)\n","    W3 = np.random.randn(1, 3)\n","    b3 = np.random.randn(1, 1)\n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n","    a3 = np.array([[ 0.40682402,  0.01629284,  0.16722898,  0.10118111,  0.40682402]])\n","    return a3, Y_assess, parameters\n","\n","def backward_propagation_with_regularization_test_case():\n","    np.random.seed(1)\n","    X_assess = np.random.randn(3, 5)\n","    Y_assess = np.array([[1, 1, 0, 1, 0]])\n","    cache = (np.array([[-1.52855314,  3.32524635,  2.13994541,  2.60700654, -0.75942115],\n","         [-1.98043538,  4.1600994 ,  0.79051021,  1.46493512, -0.45506242]]),\n","  np.array([[ 0.        ,  3.32524635,  2.13994541,  2.60700654,  0.        ],\n","         [ 0.        ,  4.1600994 ,  0.79051021,  1.46493512,  0.        ]]),\n","  np.array([[-1.09989127, -0.17242821, -0.87785842],\n","         [ 0.04221375,  0.58281521, -1.10061918]]),\n","  np.array([[ 1.14472371],\n","         [ 0.90159072]]),\n","  np.array([[ 0.53035547,  5.94892323,  2.31780174,  3.16005701,  0.53035547],\n","         [-0.69166075, -3.47645987, -2.25194702, -2.65416996, -0.69166075],\n","         [-0.39675353, -4.62285846, -2.61101729, -3.22874921, -0.39675353]]),\n","  np.array([[ 0.53035547,  5.94892323,  2.31780174,  3.16005701,  0.53035547],\n","         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n","         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]),\n","  np.array([[ 0.50249434,  0.90085595],\n","         [-0.68372786, -0.12289023],\n","         [-0.93576943, -0.26788808]]),\n","  np.array([[ 0.53035547],\n","         [-0.69166075],\n","         [-0.39675353]]),\n","  np.array([[-0.3771104 , -4.10060224, -1.60539468, -2.18416951, -0.3771104 ]]),\n","  np.array([[ 0.40682402,  0.01629284,  0.16722898,  0.10118111,  0.40682402]]),\n","  np.array([[-0.6871727 , -0.84520564, -0.67124613]]),\n","  np.array([[-0.0126646]]))\n","    return X_assess, Y_assess, cache\n","\n","def forward_propagation_with_dropout_test_case():\n","    np.random.seed(1)\n","    X_assess = np.random.randn(3, 5)\n","    W1 = np.random.randn(2, 3)\n","    b1 = np.random.randn(2, 1)\n","    W2 = np.random.randn(3, 2)\n","    b2 = np.random.randn(3, 1)\n","    W3 = np.random.randn(1, 3)\n","    b3 = np.random.randn(1, 1)\n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n","    \n","    return X_assess, parameters\n","\n","def backward_propagation_with_dropout_test_case():\n","    np.random.seed(1)\n","    X_assess = np.random.randn(3, 5)\n","    Y_assess = np.array([[1, 1, 0, 1, 0]])\n","    cache = (np.array([[-1.52855314,  3.32524635,  2.13994541,  2.60700654, -0.75942115],\n","           [-1.98043538,  4.1600994 ,  0.79051021,  1.46493512, -0.45506242]]), np.array([[ True, False,  True,  True,  True],\n","           [ True,  True,  True,  True, False]], dtype=bool), np.array([[ 0.        ,  0.        ,  4.27989081,  5.21401307,  0.        ],\n","           [ 0.        ,  8.32019881,  1.58102041,  2.92987024,  0.        ]]), np.array([[-1.09989127, -0.17242821, -0.87785842],\n","           [ 0.04221375,  0.58281521, -1.10061918]]), np.array([[ 1.14472371],\n","           [ 0.90159072]]), np.array([[ 0.53035547,  8.02565606,  4.10524802,  5.78975856,  0.53035547],\n","           [-0.69166075, -1.71413186, -3.81223329, -4.61667916, -0.69166075],\n","           [-0.39675353, -2.62563561, -4.82528105, -6.0607449 , -0.39675353]]), np.array([[ True, False,  True, False,  True],\n","           [False,  True, False,  True,  True],\n","           [False, False,  True, False, False]], dtype=bool), np.array([[ 1.06071093,  0.        ,  8.21049603,  0.        ,  1.06071093],\n","           [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n","           [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]), np.array([[ 0.50249434,  0.90085595],\n","           [-0.68372786, -0.12289023],\n","           [-0.93576943, -0.26788808]]), np.array([[ 0.53035547],\n","           [-0.69166075],\n","           [-0.39675353]]), np.array([[-0.7415562 , -0.0126646 , -5.65469333, -0.0126646 , -0.7415562 ]]), np.array([[ 0.32266394,  0.49683389,  0.00348883,  0.49683389,  0.32266394]]), np.array([[-0.6871727 , -0.84520564, -0.67124613]]), np.array([[-0.0126646]]))\n","\n","\n","    return X_assess, Y_assess, cache\n","    "],"metadata":{"cellView":"form","id":"TrV8nfHzc3ZA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"cIebY7oMW4wH"},"source":["**Problem Açıklaması**: Fransız Futbol Şirketi tarafından yapay zeka uzmanı olarak işe alındınız. Fransız takımının oyuncularının daha sonra kafalarıyla vurabilmeleri için Fransa'nın kalecisinin topa vurması gereken pozisyonları önermenizi istiyorlar.\n","<img src=\"https://drive.google.com/uc?id=1x_C6I9yi91gJUrUxX_-VQ-9P-ipK4S7U\" style=\"width:600px;height:350px;\" alt=\"field_kiank\" title=\"field_kiank\">\n","\n","<caption><center> <u> **Şekil 1** </u>: **Futbol sahası**<br> \n","Kaleci topu havaya atar, her takımın oyuncuları kafalarıyla topa vurmak için savaşırlar. </center></caption>\n","\n","\n","Size Fransa'nın son 10 maçından aşağıdaki 2B veri kümesini veriyorlar."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"VB4NySc6W4wI"},"outputs":[],"source":["train_X, train_Y, test_X, test_Y = load_2D_dataset()"]},{"cell_type":"markdown","metadata":{"id":"daxM5ex3W4wI"},"source":["Her nokta, Fransız kalecinin futbol sahasının sol tarafından şutunu çekmesinden sonra bir futbolcunun kafasıyla topa vurduğu futbol sahasındaki bir konuma karşılık gelir.\n","- Nokta mavi ise Fransız oyuncu kafasıyla topa vurmayı başarmış demektir.\n","- Nokta kırmızı ise diğer takımın oyuncusu topa kafasıyla vurmuş demektir.\n","\n","**Hedefiniz**: Sahada kalecinin topa vurması gereken pozisyonları bulmak için bir derin öğrenme modeli kullanın."]},{"cell_type":"markdown","metadata":{"id":"kmzx3Hp9W4wJ"},"source":["**Veri kümesinin analizi**: Bu veri kümesi biraz gürültülü, ancak sol üst yarıyı (mavi) sağ alt yarıdan (kırmızı) ayıran çapraz bir çizgi iyi işleyecek gibi görünüyor.\n","\n","İlk önce düzenli olmayan bir modeli deneyeceksiniz. Ardından, bunu nasıl düzenli hale getireceğinizi öğrenecek ve French Football Corporation'ın sorununu çözmek için hangi modeli seçeceğinize karar vereceksiniz."]},{"cell_type":"markdown","metadata":{"id":"J62VZtCJW4wJ"},"source":["## 1 - Düzenlenmemiş (Non-regularized) model\n","\n","Aşağıdaki sinir ağını kullanacaksınız (aşağıda sizin için zaten uygulanmış). Bu model kullanılabilir:\n","\n","- *düzenleme modunda (regularization mode)* -- `lambd` girişini sıfır olmayan bir değere ayarlayarak. Python'da \"`lambda`\" tanımlı bir anahtar kelime olduğundan, \"`lambda`\" yerine \"`lambd`\" kullanıyoruz.\n","\n","- *seyreltme modunda (dropout mode)* -- `keep_prob`u birden küçük bir değere ayarlayarak\n","\n","Modeli ilk önce herhangi bir düzenleme yapmadan deneyeceksiniz. Ardından, uygulayacaksınız:\n","- *L2 düzenlileştirme (regularization)* -- fonksiyonlar: \"``compute_cost_with_regularization()`\" ve \"`backward_propagation_with_regularization()`\"\n","\n","- *Seyreltme (Dropout)* -- fonksiyonlar: \"`forward_propagation_with_dropout()`\" ve \"`backward_propagation_with_dropout()`\"\n","\n","Her bölümde, uyguladığınız fonksiyonları çağırması için bu modeli doğru girdilerle çalıştıracaksınız. Modele aşina olmak için aşağıdaki koda bir göz atın."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0HMSQs_TW4wK"},"outputs":[],"source":["def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n","    \"\"\"\n","    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n","    \n","    Arguments:\n","    X -- input data, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n","    learning_rate -- learning rate of the optimization\n","    num_iterations -- number of iterations of the optimization loop\n","    print_cost -- If True, print the cost every 10000 iterations\n","    lambd -- regularization hyperparameter, scalar\n","    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n","    \n","    Returns:\n","    parameters -- parameters learned by the model. They can then be used to predict.\n","    \"\"\"\n","        \n","    grads = {}\n","    costs = []                            # to keep track of the cost\n","    m = X.shape[1]                        # number of examples\n","    layers_dims = [X.shape[0], 20, 3, 1]\n","    \n","    # Initialize parameters dictionary.\n","    parameters = initialize_parameters(layers_dims)\n","\n","    # Loop (gradient descent)\n","\n","    for i in range(0, num_iterations):\n","\n","        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n","        if keep_prob == 1:\n","            a3, cache = forward_propagation(X, parameters)\n","        elif keep_prob < 1:\n","            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n","        \n","        # Cost function\n","        if lambd == 0:\n","            cost = compute_cost(a3, Y)\n","        else:\n","            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n","            \n","        # Backward propagation.\n","        assert(lambd == 0 or keep_prob == 1)    # it is possible to use both L2 regularization and dropout, \n","                                            # but this assignment will only explore one at a time\n","        if lambd == 0 and keep_prob == 1:\n","            grads = backward_propagation(X, Y, cache)\n","        elif lambd != 0:\n","            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n","        elif keep_prob < 1:\n","            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n","        \n","        # Update parameters.\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","        \n","        # Print the loss every 10000 iterations\n","        if print_cost and i % 10000 == 0:\n","            print(\"Cost after iteration {}: {}\".format(i, cost))\n","        if print_cost and i % 1000 == 0:\n","            costs.append(cost)\n","    \n","    # plot the cost\n","    plt.plot(costs)\n","    plt.ylabel('cost')\n","    plt.xlabel('iterations (x1,000)')\n","    plt.title(\"Learning rate =\" + str(learning_rate))\n","    plt.show()\n","    \n","    return parameters"]},{"cell_type":"markdown","metadata":{"id":"B6HCHMTNW4wM"},"source":["Modeli herhangi bir düzenleme yapmadan eğitelim ve eğitim/test kümelerinde doğruluğunu gözlemleyelim."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"wbtBnk3WW4wN"},"outputs":[],"source":["parameters = model(train_X, train_Y)\n","print(\"On the training set:\")\n","predictions_train = predict(train_X, train_Y, parameters)\n","print(\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"]},{"cell_type":"markdown","metadata":{"id":"bn5cuiO3W4wO"},"source":["Eğitim doğruluğu %94,8, test doğruluğu ise %91.5'tir. \n","Bu, **temel modeldir** (düzenlemenin bu model üzerindeki etkisini gözlemleyeceksiniz). \n","Modelinizin karar sınırını çizmek için aşağıdaki kodu çalıştırın."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"Ugm312a5W4wO"},"outputs":[],"source":["plt.title(\"Model without regularization\")\n","axes = plt.gca()\n","axes.set_xlim([-0.75, 0.40])\n","axes.set_ylim([-0.75, 0.65])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"]},{"cell_type":"markdown","metadata":{"id":"X5lCd5JxW4wP"},"source":["Düzenlenmemiş model açıkça eğitim kümesine ağırı uyuyor. \n","Gürültülü noktalara uyuyor! \n","Şimdi aşırı uymayı azaltmak için iki tekniğe bakalım."]},{"cell_type":"markdown","metadata":{"id":"7fLp93enW4wP"},"source":["## 2 - L2 Düzenlemesi (L2 Regularization)\n","\n","Aşırı uyumu önlemenin standart yolu **L2 düzenleme** olarak adlandırılır. Maliyet fonksiyonunuzu aşağıdakilerden uygun şekilde değiştirmekten oluşur:\n","\n","$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n","'dan\n","\n","$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n","'na\n","\n","Maliyetinizi değiştirelim ve sonuçlarını gözlemleyelim.\n","\n","**Alıştırma**: Formül (2) ile verilen maliyeti hesaplayan `compute_cost_with_regularization()` öğesini uygulayın. $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$ hesaplamak için şunu kullanın:\n","```python\n","np.sum(np.square(Wl))\n","```\n","Bunu $W^{[1]}$, $W^{[2]}$ ve $W^{[3]}$ için yapmanız, ardından üç terimi toplamanız ve $ \\frac{1}{m} \\frac{\\lambda}{2} $."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"zAF8pUotW4wQ"},"outputs":[],"source":["# GRADED FUNCTION: compute_cost_with_regularization\n","\n","def compute_cost_with_regularization(A3, Y, parameters, lambd):\n","    \"\"\"\n","    Implement the cost function with L2 regularization. See formula (2) above.\n","    \n","    Arguments:\n","    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n","    Y -- \"true\" labels vector, of shape (output size, number of examples)\n","    parameters -- python dictionary containing parameters of the model\n","    \n","    Returns:\n","    cost - value of the regularized loss function (formula (2))\n","    \"\"\"\n","    m = Y.shape[1]\n","    W1 = parameters[\"W1\"]\n","    W2 = parameters[\"W2\"]\n","    W3 = parameters[\"W3\"]\n","    \n","    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n","    \n","    ### START CODE HERE ### (approx. 1 line)\n","    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2 * m)\n","    ### END CODER HERE ###\n","    \n","    cost = cross_entropy_cost + L2_regularization_cost\n","    \n","    return cost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgfG_2_oW4wQ"},"outputs":[],"source":["A3, Y_assess, parameters = compute_cost_with_regularization_test_case()\n","\n","print(\"cost = \" + str(compute_cost_with_regularization(A3, Y_assess, parameters, lambd = 0.1)))"]},{"cell_type":"markdown","metadata":{"id":"a2VoZ0QWW4wS"},"source":["Elbette maliyeti değiştirdiğiniz için geriye doğru yayılmayı da değiştirmelisiniz! Tüm gradyanlar bu yeni maliyete göre hesaplanmalıdır.\n","\n","**Alıştırma**: Düzenlemeyi hesaba katmak için geriye doğru yayılmada gereken değişiklikleri uygulayın. Değişiklikler yalnızca dW1, dW2 ve dW3 ile ilgilidir. Her biri için, düzenlileştirme teriminin gradyanını ($\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$) eklemelisiniz."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"rTcu_N1lW4wS"},"outputs":[],"source":["# GRADED FUNCTION: backward_propagation_with_regularization\n","\n","def backward_propagation_with_regularization(X, Y, cache, lambd):\n","    \"\"\"\n","    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    Y -- \"true\" labels vector, of shape (output size, number of examples)\n","    cache -- cache output from forward_propagation()\n","    lambd -- regularization hyperparameter, scalar\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","    \n","    dZ3 = A3 - Y\n","    \n","    ### START CODE HERE ### (approx. 1 line)\n","    dW3 = 1. / m * np.dot(dZ3, A2.T) + (lambd * W3) / m\n","    ### END CODE HERE ###\n","    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n","    \n","    dA2 = np.dot(W3.T, dZ3)\n","    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n","    ### START CODE HERE ### (approx. 1 line)\n","    dW2 = 1. / m * np.dot(dZ2, A1.T) + (lambd * W2) / m\n","    ### END CODE HERE ###\n","    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n","    \n","    dA1 = np.dot(W2.T, dZ2)\n","    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","    ### START CODE HERE ### (approx. 1 line)\n","    dW1 = 1. / m * np.dot(dZ1, X.T) + (lambd * W1) / m\n","    ### END CODE HERE ###\n","    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n","    \n","    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2,\n","                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n","                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WeNt-S0UW4wT"},"outputs":[],"source":["X_assess, Y_assess, cache = backward_propagation_with_regularization_test_case()\n","\n","grads = backward_propagation_with_regularization(X_assess, Y_assess, cache, lambd=0.7)\n","print (\"dW1 = \" + str(grads[\"dW1\"]))\n","print (\"dW2 = \" + str(grads[\"dW2\"]))\n","print (\"dW3 = \" + str(grads[\"dW3\"]))"]},{"cell_type":"markdown","metadata":{"id":"2MH_2SnMW4wT"},"source":["Şimdi modeli L2 düzenlileştirme $(\\lambda = 0.7)$ ile çalıştıralım. `model()` fonksiyonu şunu çağırır:\n","- `compute_cost` yerine `compute_cost_with_regularization`\n","- `backward_propagation` yerine `backward_propagation_with_regularization`"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"Mn4n7fMPW4wT"},"outputs":[],"source":["parameters = model(train_X, train_Y, lambd=0.7)\n","print(\"On the train set:\")\n","predictions_train = predict(train_X, train_Y, parameters)\n","print(\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"]},{"cell_type":"markdown","metadata":{"id":"bq5o5T6aW4wU"},"source":["Tebrikler, test kümesindeki doğruluğu %93'e yükseldi. Fransız futbol takımını kurtardın!\n","\n","Artık eğitim verilerine gereğinden aşırı uyum göstermiyorsunuz. \n","\n","Karar sınırını çizelim."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jklUtRiyW4wU"},"outputs":[],"source":["plt.title(\"Model with L2-regularization\")\n","axes = plt.gca()\n","axes.set_xlim([-0.75,0.40])\n","axes.set_ylim([-0.75,0.65])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"inMF5SNEW4wV"},"source":["**Gözlemler**:\n","- $\\lambda$ değeri, bir geliştirme kümesi kullanarak ayarlayabileceğiniz bir hiperparametredir.\n","- L2 düzenlileştirmesi, karar sınırınızı daha yumuşak hale getirir. $\\lambda$ çok büyükse, yüksek sapmalı bir modelle sonuçlanan \"aşırı düzleştirme\" de mümkündür.\n","\n","**L2-düzenlileştirme aslında ne yapıyor?**:\n","\n","L2-düzenlileştirme, küçük ağırlıklı bir modelin büyük ağırlıklı bir modelden daha basit olduğu varsayımına dayanır. Böylece maliyet fonksiyonunda ağırlıkların kare değerlerini cezalandırarak tüm ağırlıkları daha küçük değerlere yönlendirirsiniz. Büyük ağırlıklara sahip olmak maliyet için çok maliyetli hale geliyor! Bu, girdi değiştikçe çıktının daha yavaş değiştiği daha düzgün bir modele yol açar.\n","\n","<font color='blue'>\n","**Hatırlamanız gerekenler** -- L2 düzenlileştirmesinin aşağıdakiler üzerindeki etkileri:\n","\n","- Maliyet hesabı:\n","  - Maliyete bir düzenleme süresi eklendi\n","\n","- Geri yayılım işlevi:\n","  - Ağırlık matrislerine göre gradyanlarda ekstra terimler vardır.\n","\n","- Ağırlıklar küçülür (\"ağırlık azalması\"):\n","  - Ağırlıklar daha küçük değerlere güncellenir."]},{"cell_type":"markdown","source":["## 3 - Seyreltme (Dropout)\n","\n","Son olarak, **seyreltme (dropout)**, derin öğrenmeye özgü, yaygın olarak kullanılan bir düzenlileştirme tekniğidir.\n","**Her iterasyonda bazı nöronları rastgele kapatır.** Bunun ne anlama geldiğini görmek için bu iki videoyu izleyin!"],"metadata":{"id":"RBPA2xtShRI1"}},{"cell_type":"code","source":["%%html\n","<video height=\"720\" src=\"https://drive.google.com/uc?id=1iUUt-0i7UfGW4gmZTVkq0VIA73uXrDn_\" type=\"video/mp4\" controls>\n","</video>\n","\n","<h3>Şekil2: İkinci gizli katmanda seyreltme</h3>\n","<p>\n","  At each iteration, you shut down (= set to zero) each neuron of a layer with probability 1−keep_prob or keep it with probability keep_prob (50% here). The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration.\n","</p>"],"metadata":{"id":"j6_Hdmdngxkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%html\n","<video height=\"720\" src=\"https://drive.google.com/uc?id=1AnssB6AZ8iasDDrfNzLp7_PjT67eAjYN\" type=\"video/mp4\" controls>\n","</video>\n","<h3>Şekil3: Birinci ve üçüncü gizli katmanlarda seyreltme</h3>\n","<p>\n","1st layer: we shut down on average 40% of the neurons. 3rd layer: we shut down on average 20% of the neurons. \n","</p>"],"metadata":{"id":"kEN6h7dpiB3z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bazı nöronları kapattığınızda, aslında modelinizi değiştirirsiniz. Seyreltmenin arkasındaki fikir, her yinelemede, nöronlarınızın yalnızca bir alt kümesini kullanan farklı bir model eğitmenizdir. Seyreltme ile, nöronlarınız diğer bir spesifik nöronun aktivasyonuna karşı daha az duyarlı hale gelir, çünkü bu diğer nöron herhangi bir zamanda kapatılabilir.\n","\n","### 3.1 - Seyreltme ile ileriye yayılma\n","\n","**Alıştırma**: Seyreltme ile ileriye yayılımı uygulayın. 3 katmanlı bir sinir ağı kullanıyorsunuz ve birinci ve ikinci gizli katmanlara dropout ekleyeceksiniz. Giriş katmanına veya çıkış katmanına dropout uygulamayacağız.\n","\n","**Talimatlar**:\n","Birinci ve ikinci katmanlardaki bazı nöronları kapatmak istiyorsunuz. Bunu yapmak için 4 Adımı gerçekleştireceksiniz:\n","\n","1. Derste, `np.random.rand()` kullanarak $a^{[1]}$ ile aynı şekle sahip bir $d^{[1]}$ değişkeni yaratmayı tartışmıştık. 1. Burada, vektörleştirilmiş bir uygulama kullanacaksınız, bu nedenle rastgele bir matris oluşturun $D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] $ ile aynı boyutta $A^{[1]}$.\n","\n","2. $D^{[1]}$ içindeki değerleri eşikleyerek, $D^{[1]}$ öğesinin her girişini 0 olasılıkla (`1-keep_prob`) veya 1 olasılıkla (`keep_prob`) olacak şekilde ayarlayın. uygun şekilde. İpucu: X matrisinin tüm girişlerini 0'a (giriş 0,5'ten küçükse) veya 1'e (giriş 0,5'ten büyükse) ayarlamak için şunları yaparsınız: 'X = (X < 0,5)'. 0 ve 1'in sırasıyla Yanlış ve Doğru'ya eşdeğer olduğuna dikkat edin.\n","\n","3. $A^{[1]}$ öğesini $A^{[1]} * D^{[1]}$ olarak ayarlayın. (Bazı nöronları kapatıyorsunuz). $D^{[1]}$'ı bir maske olarak düşünebilirsiniz, böylece başka bir matrisle çarpıldığında bazı değerleri kapatır.\n","\n","4. $A^{[1]}$'ı `keep_prob` ile bölün. Bunu yaparak, maliyetin sonucunun, seyreltme olmadan aynı beklenen değere sahip olacağını garanti ediyorsunuz. (Bu tekniğe seyreltmenin tersi (inverted dropout) de denir.)"],"metadata":{"id":"RCDKZ5H1if5p"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"txOWRssgW4wW"},"outputs":[],"source":["# GRADED FUNCTION: forward_propagation_with_dropout\n","\n","def forward_propagation_with_dropout(X, parameters, keep_prob=0.5):\n","    \"\"\"\n","    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (2, number of examples)\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","                    W1 -- weight matrix of shape (20, 2)\n","                    b1 -- bias vector of shape (20, 1)\n","                    W2 -- weight matrix of shape (3, 20)\n","                    b2 -- bias vector of shape (3, 1)\n","                    W3 -- weight matrix of shape (1, 3)\n","                    b3 -- bias vector of shape (1, 1)\n","    keep_prob - probability of keeping a neuron active during drop-out, scalar\n","    \n","    Returns:\n","    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n","    cache -- tuple, information stored for computing the backward propagation\n","    \"\"\"\n","    \n","    np.random.seed(1)\n","    \n","    # retrieve parameters\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","    \n","    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = relu(Z1)\n","    ### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. \n","    D1 = np.random.rand(A1.shape[0], A1.shape[1])     # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n","    D1 = D1 < keep_prob                            # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n","    A1 = A1 * D1                                      # Step 3: shut down some neurons of A1\n","    A1 = A1 / keep_prob                               # Step 4: scale the value of neurons that haven't been shut down\n","    ### END CODE HERE ###\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = relu(Z2)\n","    ### START CODE HERE ### (approx. 4 lines)\n","    D2 = np.random.rand(A2.shape[0], A2.shape[1])     # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n","    D2 = D2 < keep_prob                           # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)                           \n","    A2 = A2 * D2                                      # Step 3: shut down some neurons of A2\n","    A2 = A2 / keep_prob                               # Step 4: scale the value of neurons that haven't been shut down\n","    ### END CODE HERE ###\n","    Z3 = np.dot(W3, A2) + b3\n","    A3 = sigmoid(Z3)\n","    \n","    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n","    \n","    return A3, cache"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzLAUb0SW4wX"},"outputs":[],"source":["X_assess, parameters = forward_propagation_with_dropout_test_case()\n","\n","A3, cache = forward_propagation_with_dropout(X_assess, parameters, keep_prob=0.7)\n","print (\"A3 = \" + str(A3))"]},{"cell_type":"markdown","metadata":{"id":"6awSRg07W4wX"},"source":["### 3.2 - Seyreltme ile geriye doğru yayılma\n","\n","**Alıştırma**: Seyreltme ile geriye doğru yayılımı uygulayın. Daha önce olduğu gibi, 3 katmanlı bir ağ eğitimi alıyorsunuz. Önbellekte depolanan $D^{[1]}$ ve $D^{[2]}$ maskelerini kullanarak birinci ve ikinci gizli katmanlara dropout ekleyin.\n","\n","**Talimat**:\n","Seyreltme ile geri yayılım aslında oldukça kolaydır. 2 Adımı gerçekleştirmeniz gerekecek:\n","\n","1. Daha önce `A1`e $D^{[1]}$ maskesi uygulayarak ileri yayılma sırasında bazı nöronları kapatmıştınız. Geri yayılımda, aynı $D^{[1]}$ maskesini `dA1`\"e yeniden uygulayarak aynı nöronları kapatmanız gerekecektir.\n","\n","2. İleri yayılım sırasında `A1`i `keep_prob` ile böldünüz. Geri yayılımda, bu nedenle, `dA1`i tekrar `keep_prob` ile bölmeniz gerekir (hesap yorumu şu şekildedir: $A^{[1]}$, `keep_prob` ile ölçeklenirse, türevi $dA^{[1 ]}$ aynı `keep_prob` tarafından da ölçeklenir)."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ggdL2-koW4wY"},"outputs":[],"source":["# GRADED FUNCTION: backward_propagation_with_dropout\n","\n","def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n","    \"\"\"\n","    Implements the backward propagation of our baseline model to which we added dropout.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (2, number of examples)\n","    Y -- \"true\" labels vector, of shape (output size, number of examples)\n","    cache -- cache output from forward_propagation_with_dropout()\n","    keep_prob - probability of keeping a neuron active during drop-out, scalar\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","    \n","    dZ3 = A3 - Y\n","    dW3 = 1. / m * np.dot(dZ3, A2.T)\n","    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n","    dA2 = np.dot(W3.T, dZ3)\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n","    dA2 = dA2 / keep_prob              # Step 2: Scale the value of neurons that haven't been shut down\n","    ### END CODE HERE ###\n","    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n","    dW2 = 1. / m * np.dot(dZ2, A1.T)\n","    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n","    \n","    dA1 = np.dot(W2.T, dZ2)\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n","    dA1 = dA1 / keep_prob              # Step 2: Scale the value of neurons that haven't been shut down\n","    ### END CODE HERE ###\n","    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","    dW1 = 1. / m * np.dot(dZ1, X.T)\n","    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n","    \n","    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n","                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n","                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75ruq6-sW4wY"},"outputs":[],"source":["X_assess, Y_assess, cache = backward_propagation_with_dropout_test_case()\n","\n","gradients = backward_propagation_with_dropout(X_assess, Y_assess, cache, keep_prob=0.8)\n","\n","print (\"dA1 = \" + str(gradients[\"dA1\"]))\n","print (\"dA2 = \" + str(gradients[\"dA2\"]))"]},{"cell_type":"markdown","metadata":{"id":"-b8X6i-nW4wZ"},"source":["Şimdi modeli bırakma ile çalıştıralım (`keep_prob = 0.86`). Bu, her iterasyonda katman 1 ve 2'nin her nöronunu %24 olasılıkla kapattığınız anlamına gelir. 'model()' işlevi şimdi şunu çağıracak:\n","- `forward_propagation` yerine `forward_propagation_with_dropout`.\n","- `backward_propagation`yerine `backward_propagation_with_dropout`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qDOYgkiW4wZ"},"outputs":[],"source":["parameters = model(train_X, train_Y, keep_prob=0.86, learning_rate=0.3)\n","\n","print(\"On the train set:\")\n","predictions_train = predict(train_X, train_Y, parameters)\n","print(\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"]},{"cell_type":"markdown","metadata":{"id":"HNOZUOmTW4wZ"},"source":["Seyreltme(dropout) harika çalışıyor! Test doğruluğu tekrar arttı (%95'e kadar)! Modeliniz eğitim kümesine aşırı uyum göstermiyor ve test kümesinde harika bir iş çıkarıyor. Fransız futbol takımı sana sonsuza kadar minnettar kalacak!\n","\n","Karar sınırını çizmek için aşağıdaki kodu çalıştırın."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2NM-dN-W4wa"},"outputs":[],"source":["plt.title(\"Model with dropout\")\n","axes = plt.gca()\n","axes.set_xlim([-0.75, 0.40])\n","axes.set_ylim([-0.75, 0.65])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"M6mzL4EOW4wa"},"source":["**Not**:\n","- Seyreltmeyi kullanırken yapılan **yaygın bir hata** onu hem eğitimde hem de testte kullanmaktır. Seyreltmeyi (düğümleri rastgele ortadan kaldırın) yalnızca eğitimde kullanmalısınız.\n","- [tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [PaddlePaddle](http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html), [keras](https://keras.io/layers/core/#dropout) veya [caffe](http://caffe.berkeleyvision.org/tutorial/layers/dropout.html) derin öğrenme kütüphanelerinde dropout birlikte gelir. Stres yapmayın - yakında bu çerçevelerden bazılarını öğreneceksiniz.\n","\n","<font color='blue'>\n","**Seyreltme (dropout) hakkında hatırlamanız gerekenler:**\n","\n","- Seyreltme bir düzenlileştirme tekniğidir.\n","\n","- Seyreltmeyi yalnızca eğitim sırasında kullanırsınız. Test süresi boyunca seyreltme kullanmayın (düğümleri rastgele ortadan kaldırın).\n","\n","- Hem ileri hem de geri yayılım sırasında seyreltme uygulayın.\n","\n","- Eğitim süresi boyunca, aktivasyonlar için aynı beklenen değeri korumak için her seyreltme katmanını keep_prob'a bölün. Örneğin, keep_prob 0,5 ise, ortalama olarak düğümlerin yarısını kapatacağız, bu nedenle çıktı, yalnızca kalan yarısı çözüme katkıda bulunduğundan 0,5 ile ölçeklenecektir. 0,5'e bölmek, 2 ile çarpmaya eşdeğerdir. Bu nedenle, çıktı artık aynı beklenen değere sahiptir. Keep_prob 0,5'ten farklı değerler olduğunda bile bunun çalışıp çalışmadığını kontrol edebilirsiniz."]},{"cell_type":"markdown","metadata":{"id":"pFwIkBHYW4wa"},"source":["## 4. Sonuçlar"]},{"cell_type":"markdown","metadata":{"id":"OcJkwt8sW4wb"},"source":["**İşte üç modelimizin sonuçları**:\n","\n","<table> \n","    <tr>\n","        <td>\n","        model\n","        </td>\n","        <td>\n","        train accuracy\n","        </td>\n","        <td>\n","        test accuracy\n","        </td>\n","    </tr>\n","        <td>\n","        3-layer NN without regularization\n","        </td>\n","        <td>\n","        95%\n","        </td>\n","        <td>\n","        91.5%\n","        </td>\n","    <tr>\n","        <td>\n","        3-layer NN with L2-regularization\n","        </td>\n","        <td>\n","        94%\n","        </td>\n","        <td>\n","        93%\n","        </td>\n","    </tr>\n","    <tr>\n","        <td>\n","        3-layer NN with dropout\n","        </td>\n","        <td>\n","        93%\n","        </td>\n","        <td>\n","        95%\n","        </td>\n","    </tr>\n","</table> "]},{"cell_type":"markdown","metadata":{"id":"LwxhLdGeW4wb"},"source":["Düzenlileştirmenin eğitim kümesi performansına zarar verdiğini unutmayın! Bunun nedeni, ağın eğitim kümesine aşırı uyum yeteneğini sınırlamasıdır. Ancak sonuçta daha iyi test doğruluğu sağladığı için sisteminize yardımcı oluyor."]},{"cell_type":"markdown","metadata":{"id":"amTLCuInW4wc"},"source":["Bu görevi tamamladığınız için tebrikler! Ve ayrıca Fransız futbolunda devrim yarattığınız için. :-)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"PfYNREb6W4wc"},"source":["<font color='blue'>\n","**Bu çalışma dosyasında hatırlamanızı istediğimiz şey**:\n","\n","- Düzenlileştirme, aşırı uyumu azaltmanıza yardımcı olacaktır.\n","- Düzenlileştirme, ağırlıklarınızı daha düşük değerlere çekecektir.\n","- L2 düzenlileştirme (Regularizatio) ve Seyreltme (Dropout), çok etkili iki düzenlileştirme tekniğidir."]},{"cell_type":"code","source":[""],"metadata":{"id":"NNDgojs9llsm"},"execution_count":null,"outputs":[]}],"metadata":{"coursera":{"course_slug":"deep-neural-network","graded_item_id":"SXQaI","launcher_item_id":"UAwhh"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"name":"Day10_ANN_2_Regularization.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}