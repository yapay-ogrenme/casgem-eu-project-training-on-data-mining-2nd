{"cells":[{"cell_type":"markdown","metadata":{"id":"z0-uCG_lLp8M"},"source":["# Başlatma (Initialization)\n","\n","\"Derin Sinir Ağlarını İyileştirme\"nin ilk görevine hoş geldiniz.\n","\n","Sinir ağınızı eğitmek, ağırlıkların bir başlangıç değerini belirtmeyi gerektirir. İyi seçilmiş bir başlatma yöntemi öğrenmeye yardımcı olacaktır.\n","\n","Bu çalışma dosyasında, farklı başlatmaların nasıl farklı sonuçlara yol açtığını göreceksiniz.\n","\n","İyi seçilmiş bir başlatma şunları yapabilir:\n","- Gradyan inişinin yakınsamasını hızlandırın\n","- Daha düşük bir eğitim (ve genelleme) hatasına yakınsayan gradyan iniş olasılığını artırın\n","\n","Başlamak için, paketleri ve sınıflandırmaya çalışacağınız düzlemsel veri kümesini yüklemek için aşağıdaki hücreyi çalıştırın."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRIpFyCvLp8Q"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import sklearn\n","import sklearn.datasets\n","\n","#from init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\n","#from init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["#@title Yardımcı Fonksiyonlar\n","def sigmoid(x):\n","    \"\"\"\n","    Compute the sigmoid of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(x)\n","    \"\"\"\n","    s = 1/(1+np.exp(-x))\n","    return s\n","\n","def relu(x):\n","    \"\"\"\n","    Compute the relu of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- relu(x)\n","    \"\"\"\n","    s = np.maximum(0,x)\n","    \n","    return s\n","\n","def forward_propagation(X, parameters):\n","    \"\"\"\n","    Implements the forward propagation (and computes the loss) presented in Figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","                    W1 -- weight matrix of shape ()\n","                    b1 -- bias vector of shape ()\n","                    W2 -- weight matrix of shape ()\n","                    b2 -- bias vector of shape ()\n","                    W3 -- weight matrix of shape ()\n","                    b3 -- bias vector of shape ()\n","    \n","    Returns:\n","    loss -- the loss function (vanilla logistic loss)\n","    \"\"\"\n","        \n","    # retrieve parameters\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","    \n","    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","    z1 = np.dot(W1, X) + b1\n","    a1 = relu(z1)\n","    z2 = np.dot(W2, a1) + b2\n","    a2 = relu(z2)\n","    z3 = np.dot(W3, a2) + b3\n","    a3 = sigmoid(z3)\n","    \n","    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n","    \n","    return a3, cache\n","\n","def backward_propagation(X, Y, cache):\n","    \"\"\"\n","    Implement the backward propagation presented in figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n","    cache -- cache output from forward_propagation()\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n","    \"\"\"\n","    m = X.shape[1]\n","    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n","    \n","    dz3 = 1./m * (a3 - Y)\n","    dW3 = np.dot(dz3, a2.T)\n","    db3 = np.sum(dz3, axis=1, keepdims = True)\n","    \n","    da2 = np.dot(W3.T, dz3)\n","    dz2 = np.multiply(da2, np.int64(a2 > 0))\n","    dW2 = np.dot(dz2, a1.T)\n","    db2 = np.sum(dz2, axis=1, keepdims = True)\n","    \n","    da1 = np.dot(W2.T, dz2)\n","    dz1 = np.multiply(da1, np.int64(a1 > 0))\n","    dW1 = np.dot(dz1, X.T)\n","    db1 = np.sum(dz1, axis=1, keepdims = True)\n","    \n","    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n","                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n","                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients\n","\n","def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Update parameters using gradient descent\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    grads -- python dictionary containing your gradients, output of n_model_backward\n","    \n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","                  parameters['W' + str(i)] = ... \n","                  parameters['b' + str(i)] = ...\n","    \"\"\"\n","    \n","    L = len(parameters) // 2 # number of layers in the neural networks\n","\n","    # Update rule for each parameter\n","    for k in range(L):\n","        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n","        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n","        \n","    return parameters\n","\n","def compute_loss(a3, Y):\n","    \n","    \"\"\"\n","    Implement the loss function\n","    \n","    Arguments:\n","    a3 -- post-activation, output of forward propagation\n","    Y -- \"true\" labels vector, same shape as a3\n","    \n","    Returns:\n","    loss - value of the loss function\n","    \"\"\"\n","    \n","    m = Y.shape[1]\n","    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n","    loss = 1./m * np.nansum(logprobs)\n","    \n","    return loss\n","\n","def load_cat_dataset():\n","    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    \n","    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    \n","    train_set_x_orig = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n","    test_set_x_orig = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n","    \n","    train_set_x = train_set_x_orig/255\n","    test_set_x = test_set_x_orig/255\n","\n","    return train_set_x, train_set_y, test_set_x, test_set_y, classes\n","\n","\n","def predict(X, y, parameters):\n","    \"\"\"\n","    This function is used to predict the results of a  n-layer neural network.\n","    \n","    Arguments:\n","    X -- data set of examples you would like to label\n","    parameters -- parameters of the trained model\n","    \n","    Returns:\n","    p -- predictions for the given dataset X\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    p = np.zeros((1,m), dtype = np.int)\n","    \n","    # Forward propagation\n","    a3, caches = forward_propagation(X, parameters)\n","    \n","    # convert probas to 0/1 predictions\n","    for i in range(0, a3.shape[1]):\n","        if a3[0,i] > 0.5:\n","            p[0,i] = 1\n","        else:\n","            p[0,i] = 0\n","\n","    # print results\n","    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n","    \n","    return p\n","\n","def plot_decision_boundary(model, X, y):\n","    # Set min and max values and give it some padding\n","    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n","    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n","    h = 0.01\n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    # Predict the function value for the whole grid\n","    Z = model(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","    plt.ylabel('x2')\n","    plt.xlabel('x1')\n","    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n","    plt.show()\n","    \n","def predict_dec(parameters, X):\n","    \"\"\"\n","    Used for plotting decision boundary.\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    X -- input data of size (m, K)\n","    \n","    Returns\n","    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n","    \"\"\"\n","    \n","    # Predict using forward propagation and a classification threshold of 0.5\n","    a3, cache = forward_propagation(X, parameters)\n","    predictions = (a3>0.5)\n","    return predictions\n","\n","def load_dataset():\n","    np.random.seed(1)\n","    train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)\n","    np.random.seed(2)\n","    test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)\n","    # Visualize the data\n","    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n","    train_X = train_X.T\n","    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n","    test_X = test_X.T\n","    test_Y = test_Y.reshape((1, test_Y.shape[0]))\n","    return train_X, train_Y, test_X, test_Y"],"metadata":{"cellView":"form","id":"eUzfRKTLNZxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load image dataset: blue/red dots in circles\n","train_X, train_Y, test_X, test_Y = load_dataset()"],"metadata":{"id":"TB8_qhEONYyp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8ou2iN2Lp8R"},"source":["Mavi noktaları kırmızı noktalardan ayırmak için bir sınıflandırıcı eğitmek istiyoruz"]},{"cell_type":"markdown","metadata":{"id":"wkQq3m_pLp8S"},"source":["## 1 - Sinir Ağı modeli"]},{"cell_type":"markdown","metadata":{"id":"XD_NJ5fkLp8T"},"source":["3 katmanlı bir sinir ağı kullanacaksınız (zaten sizin için uygulanmış). Deneyeceğiniz başlatma yöntemleri şunlardır:\n","- *Sıfır başlatma* -- giriş bağımsız değişkeninde `initialization = \"zeros\"` ayarı.\n","- *Rastgele başlatma* -- giriş bağımsız değişkeninde `initialization = \"random\"` ayarı. Bu, ağırlıkları büyük rastgele değerlere başlatır.\n","- *He başlatma* -- giriş bağımsız değişkeninde `initialization = \"he\"` ayarı. Bu, ağırlıkları He ve diğerleri, 2015 tarafından hazırlanan bir makaleye göre ölçeklenen rastgele değerlere başlatır.\n","\n","**Talimatlar**: Lütfen aşağıdaki kodu hızlıca okuyun ve çalıştırın. Sonraki bölümde, bu `model()`in çağırdığı üç başlatma yöntemini uygulayacaksınız."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"vCIAOd5eLp8U"},"outputs":[],"source":["def model(X, Y, learning_rate=0.01, num_iterations=15000, print_cost=True, initialization=\"he\"):\n","    \"\"\"\n","    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n","    \n","    Arguments:\n","    X -- input data, of shape (2, number of examples)\n","    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n","    learning_rate -- learning rate for gradient descent \n","    num_iterations -- number of iterations to run gradient descent\n","    print_cost -- if True, print the cost every 1000 iterations\n","    initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n","    \n","    Returns:\n","    parameters -- parameters learnt by the model\n","    \"\"\"\n","        \n","    grads = {}\n","    costs = [] # to keep track of the loss\n","    m = X.shape[1] # number of examples\n","    layers_dims = [X.shape[0], 10, 5, 1]\n","    \n","    # Initialize parameters dictionary.\n","    if initialization == \"zeros\":\n","        parameters = initialize_parameters_zeros(layers_dims)\n","    elif initialization == \"random\":\n","        parameters = initialize_parameters_random(layers_dims)\n","    elif initialization == \"he\":\n","        parameters = initialize_parameters_he(layers_dims)\n","\n","    # Loop (gradient descent)\n","\n","    for i in range(0, num_iterations):\n","\n","        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n","        a3, cache = forward_propagation(X, parameters)\n","        \n","        # Loss\n","        cost = compute_loss(a3, Y)\n","\n","        # Backward propagation.\n","        grads = backward_propagation(X, Y, cache)\n","        \n","        # Update parameters.\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","        \n","        # Print the loss every 1000 iterations\n","        if print_cost and i % 1000 == 0:\n","            print(\"Cost after iteration {}: {}\".format(i, cost))\n","            costs.append(cost)\n","            \n","    # plot the loss\n","    plt.plot(costs)\n","    plt.ylabel('cost')\n","    plt.xlabel('iterations (per hundreds)')\n","    plt.title(\"Learning rate =\" + str(learning_rate))\n","    plt.show()\n","    \n","    return parameters"]},{"cell_type":"markdown","metadata":{"id":"_txAE95OLp8V"},"source":["## 2 - Sıfır başlatma (Zero initialization)\n","\n","Bir sinir ağında başlatılacak iki tür parametre vardır:\n","- ağırlık (weight) matrisleri $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n","- önyargı (bias) vektörleri $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$\n","\n","**Alıştırma**: Tüm parametreleri sıfır olarak başlatmak için aşağıdaki fonksiyonu uygulayın. Daha sonra bunun \"simetriyi kırmayı\" başaramadığı için iyi çalışmadığını göreceksiniz, ama yine de deneyelim ve ne olacağını görelim. np.zeros((..,..)) doğru şekillerle kullanın."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"66hzjY2zLp8X"},"outputs":[],"source":["# GRADED FUNCTION: initialize_parameters_zeros \n","\n","def initialize_parameters_zeros(layers_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the size of each layer.\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n","                    b1 -- bias vector of shape (layers_dims[1], 1)\n","                    ...\n","                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n","                    bL -- bias vector of shape (layers_dims[L], 1)\n","    \"\"\"\n","    \n","    parameters = {}\n","    L = len(layers_dims)            # number of layers in the network\n","    \n","    for l in range(1, L):\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l - 1]))\n","        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n","        ### END CODE HERE ###\n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWxG-zfPLp8X"},"outputs":[],"source":["parameters = initialize_parameters_zeros([3,2,1])\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"TE0OvHXELp8Z"},"source":["Sıfır başlatmayı kullanarak modelinizi 15.000 iterasyonda eğitmek için aşağıdaki kodu çalıştırın."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"1UKvqqISLp8a"},"outputs":[],"source":["parameters = model(train_X, train_Y, initialization = \"zeros\")\n","print (\"On the train set:\")\n","predictions_train = predict(train_X, train_Y, parameters)\n"]},{"cell_type":"code","source":["print (\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"],"metadata":{"id":"oH1UUU-nUNRd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecRMb-sYLp8b"},"source":["Performans gerçekten kötü ve maliyet gerçekten düşmüyor ve algoritma rastgele tahminden daha iyi performans göstermiyor. Peki neden? Tahminlerin detaylarına ve karar sınırına bakalım:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFfhrsnYLp8c"},"outputs":[],"source":["print(\"predictions_train = \" + str(predictions_train))\n","print(\"predictions_test = \" + str(predictions_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSm5eaO-Lp8c"},"outputs":[],"source":["plt.title(\"Model with Zeros initialization\")\n","axes = plt.gca()\n","axes.set_xlim([-1.5, 1.5])\n","axes.set_ylim([-1.5, 1.5])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"]},{"cell_type":"markdown","metadata":{"id":"tXrfP0GYLp8d"},"source":["Model her örnek için 0 öngörüyor.\n","\n","Genel olarak, tüm ağırlıkların sıfır olması, ağın simetriyi kıramamasına neden olur. Bu, her katmandaki her nöronun aynı şeyi öğreneceği anlamına gelir ve her katman için $n^{[l]}=1$ ile bir sinir ağını eğitiyor olabilirsiniz ve ağ, lojistik regresyon gibi doğrusal bir sınıflandırıcıdan daha güçlü değildir."]},{"cell_type":"markdown","metadata":{"id":"c56H57GILp8d"},"source":["<font color='blue'>\n","**Hatırlamanız gerekenler**:\n","\n","- Simetriyi kırmak için $W^{[l]}$ ağırlıkları rastgele başlatılmalıdır.\n","- Bununla birlikte, $b^{[l]}$ önyargılarını sıfır olarak başlatmakta bir sakınca yoktur. $W^{[l]}$ rastgele başlatıldığı sürece simetri hala bozuktur.\n"]},{"cell_type":"markdown","metadata":{"id":"J9WCaoC9Lp8e"},"source":["## 3 - Rastgele başlatma\n","\n","Simetriyi kırmak için ağırlıkları rastgele başlatalım. Rastgele başlatmanın ardından, her nöron girdilerinin farklı bir işlevini öğrenmeye devam edebilir. Bu çalışma dosyasında, ağırlıklar rastgele fakat çok büyük değerlere başlatılırsa ne olacağını göreceksiniz.\n","\n","**Alıştırma**: Ağırlıklarınızı büyük rastgele değerlere (\\*10 ile ölçeklendirilmiş) ve önyargılarınızı sıfıra başlatmak için aşağıdaki fonksiyonları uygulayın. Ağırlıklar için `np.random.randn(..,..) * 10` ve sapmalar için `np.zeros((.., ..))` kullanın. \"Rastgele\" ağırlıklarınızın bizimkilerle eşleştiğinden emin olmak için sabit bir `np.random.seed(..)` kullanıyoruz, bu nedenle, kodunuzu birkaç kez çalıştırmanız size parametreler için her zaman aynı başlangıç değerlerini veriyorsa endişelenmeyin."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"DGgS9kSiLp8e"},"outputs":[],"source":["# GRADED FUNCTION: initialize_parameters_random\n","\n","def initialize_parameters_random(layers_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the size of each layer.\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n","                    b1 -- bias vector of shape (layers_dims[1], 1)\n","                    ...\n","                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n","                    bL -- bias vector of shape (layers_dims[L], 1)\n","    \"\"\"\n","    \n","    np.random.seed(3)               # This seed makes sure your \"random\" numbers will be the as ours\n","    parameters = {}\n","    L = len(layers_dims)            # integer representing the number of layers\n","    \n","    for l in range(1, L):\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10\n","        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n","        ### END CODE HERE ###\n","\n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6qnZKhZLp8f"},"outputs":[],"source":["parameters = initialize_parameters_random([3, 2, 1])\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"ZLII8adHLp8g"},"source":["Modelinizi rastgele başlatma kullanarak 15.000 iterastonda eğitmek için aşağıdaki kodu çalıştırın."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"Q9QGqdr5Lp8g"},"outputs":[],"source":["parameters = model(train_X, train_Y, initialization = \"random\")\n","print(\"On the train set:\")\n","predictions_train = predict(train_X, train_Y, parameters)"]},{"cell_type":"code","source":["print(\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"],"metadata":{"id":"f2qbjTkBVYfj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aNpJdQGkLp8h"},"source":["İlk iterasyondan sonra maliyet olarak \"inf\" görüyorsanız, bunun nedeni sayısal yuvarlamadır; sayısal olarak daha karmaşık bir uygulama bunu düzeltebilir. Ama bu bizim amaçlarımız için endişelenmeye değmez.\n","\n","Her neyse, simetriyi bozmuşsunuz gibi görünüyor ve bu daha eskisinden daha iyi sonuçlar veriyor. Model artık tüm 0'ları vermiyor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3VkVRC9Lp8h"},"outputs":[],"source":["print(predictions_train)\n","print(predictions_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ClDl6sBLp8h"},"outputs":[],"source":["plt.title(\"Model with large random initialization\")\n","axes = plt.gca()\n","axes.set_xlim([-1.5, 1.5])\n","axes.set_ylim([-1.5, 1.5])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"]},{"cell_type":"markdown","metadata":{"id":"g0u_i7QLLp8i"},"source":["**Gözlemler**:\n","- Maliyet çok yüksek başlar. Bunun nedeni, büyük rastgele değerli ağırlıklarla, son aktivasyonun (sigmoid) bazı örnekler için 0 veya 1'e çok yakın sonuçlar vermesi ve bu örneği yanlış yaptığında o örnek için çok yüksek bir kayba maruz kalmasıdır. Gerçekten de, $\\log(a^{[3]}) = \\log(0)$ olduğunda, kayıp sonsuza gider.\n","- Zayıf başlatma, aynı zamanda optimizasyon algoritmasını yavaşlatan kaybolan/patlayan gradyanlara yol açabilir.\n","- Bu ağı daha uzun süre eğitirseniz daha iyi sonuçlar görürsünüz, ancak aşırı büyük rastgele sayılarla başlatma optimizasyonu yavaşlatır.\n","\n","<font color='blue'>\n","**Özetle**:\n","\n","- Ağırlıkları çok büyük rastgele değerlere başlatmak iyi çalışmıyor.\n","- Umarım küçük rastgele değerlerle başlatma daha iyi sonuç verir. Önemli soru şudur: Bu rastgele değerler ne kadar küçük olmalıdır? Bir sonraki bölümde öğrenelim!"]},{"cell_type":"markdown","metadata":{"id":"vh2c7I13Lp8i"},"source":["## 4 - He Başlatma (He Initialization)\n","\n","Son olarak, \"He Initialization\" deneyin; bu, He et al., 2015'in ilk yazarı için adlandırılmıştır. (\"Xavier başlatma\" diye bir şey duyduysanız, Xavier başlatmanın $W^{[l]}$ ağırlıkları için bir ölçekleme faktörü kullanması dışında bu benzerdir. `sqrt(1./layers_dims[l-1])` burada başlatma işlemi `sqrt(2./layers_dims[l-1])` kullanır.)\n","\n","**Alıştırma**: Parametrelerinizi He Initialization ile başlatmak için aşağıdaki işlevi uygulayın.\n","\n","**İpucu**: Bu işlev, önceki `initialize_parameters_random(...)` işlevine benzer. Tek fark, `np.random.randn(..,..)` öğesini 10 ile çarpmak yerine, onu $\\sqrt{\\frac{2}{\\text{önceki katmanın boyutu}} ile çarpmanızdır. }$, ReLU aktivasyonu olan katmanlar için başlatmanın önerdiği şeydir."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"kt_SWyOsLp8i"},"outputs":[],"source":["# GRADED FUNCTION: initialize_parameters_he\n","\n","def initialize_parameters_he(layers_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the size of each layer.\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n","                    b1 -- bias vector of shape (layers_dims[1], 1)\n","                    ...\n","                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n","                    bL -- bias vector of shape (layers_dims[L], 1)\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layers_dims) - 1 # integer representing the number of layers\n","     \n","    for l in range(1, L + 1):\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(2 / layers_dims[l - 1])\n","        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n","        ### END CODE HERE ###\n","        \n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zw8jN8GTLp8j"},"outputs":[],"source":["parameters = initialize_parameters_he([2, 4, 1])\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"zRt9ndI7Lp8k"},"source":["Modelinizi He Initialization kullanarak 15.000 iterasyon eğitmek için aşağıdaki kodu çalıştırın."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fh0wUH5NLp8k"},"outputs":[],"source":["parameters = model(train_X, train_Y, initialization = \"he\")\n","print(\"On the train set:\")\n","predictions_train = predict(train_X, train_Y, parameters)"]},{"cell_type":"code","source":["print(\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"],"metadata":{"id":"VxeBkhblWdPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"ylltPIUxLp8l"},"outputs":[],"source":["plt.title(\"Model with He initialization\")\n","axes = plt.gca()\n","axes.set_xlim([-1.5, 1.5])\n","axes.set_ylim([-1.5, 1.5])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"]},{"cell_type":"markdown","metadata":{"id":"HEgJYoAhLp8l"},"source":["**Gözlemler**:\n","- He initialization'lı model, az sayıda iterasyonda mavi ve kırmızı noktaları çok iyi ayırır."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Gj966ox3Lp8n"},"source":["## 5. Sonuçlar"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Yv3R_PyILp8n"},"source":["Üç farklı başlatma türü gördünüz. Aynı sayıda yineleme ve aynı hiperparametreler için karşılaştırma şöyledir:\n","\n","<table> \n","    <tr>\n","        <td>\n","        Model\n","        </td>\n","        <td>\n","        Train accuracy\n","        </td>\n","        <td>\n","        Problem/Comment\n","        </td>\n","    </tr>\n","        <td>\n","        3-layer NN with zeros initialization\n","        </td>\n","        <td>\n","        50%\n","        </td>\n","        <td>\n","        fails to break symmetry\n","        </td>\n","    <tr>\n","        <td>\n","        3-layer NN with large random initialization\n","        </td>\n","        <td>\n","        83%\n","        </td>\n","        <td>\n","        too large weights \n","        </td>\n","    </tr>\n","    <tr>\n","        <td>\n","        3-layer NN with He initialization\n","        </td>\n","        <td>\n","        99%\n","        </td>\n","        <td>\n","        recommended method\n","        </td>\n","    </tr>\n","</table> "]},{"cell_type":"markdown","metadata":{"id":"HCzVRMLiLp8n"},"source":["<font color='blue'>\n","**Bu çalışma dosyasında hatırlamanız gerekenler**:\n","\n","- Farklı başlatmalar farklı sonuçlara yol açar\n","- Simetriyi kırmak ve farklı gizli birimlerin farklı şeyler öğrenebilmesini sağlamak için rastgele başlatma kullanılır\n","- Çok büyük değerlere başlatmayın\n","- Başlatma, ReLU aktivasyonlarına sahip ağlar için iyi çalışıyor."]},{"cell_type":"code","source":[""],"metadata":{"id":"leqChPvmW1PC"},"execution_count":null,"outputs":[]}],"metadata":{"coursera":{"course_slug":"deep-neural-network","graded_item_id":"XOESP","launcher_item_id":"8IhFN"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"name":"Day10_ANN_1_Initialization.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}