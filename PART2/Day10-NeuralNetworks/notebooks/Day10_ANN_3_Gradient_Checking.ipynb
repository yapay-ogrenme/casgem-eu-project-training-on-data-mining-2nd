{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCXOUpPFlstK"
      },
      "source": [
        "# Gradyan Kontrolü\n",
        "\n",
        "Bu çalışma dosyasında gradyan kontrolünü uygulamayı ve kullanmayı öğreneceksiniz.\n",
        "\n",
        "Mobil ödemeleri küresel olarak kullanılabilir hale getirmek için çalışan bir ekibin parçasısınız ve sahtekarlığı (fraud) tespit etmek için bir derin öğrenme modeli oluşturmanız isteniyor - biri ödeme yaptığında, ödemenin sahte olup olmadığını, örneğin kullanıcının bir bilgisayar korsanı tarafından ele geçirildi.\n",
        "\n",
        "Ancak geri yayılımın uygulanması oldukça zordur ve bazen hatalar olabilir. Bu kritik bir uygulama olduğundan, şirketinizin CEO'su geri yayılım uygulamanızın doğru olduğundan gerçekten emin olmak ister. CEO'nuz, \"Bana geri yayılımınızın gerçekten işe yaradığına dair bir kanıt verin!\" diyor. Bu güvenceyi vermek için \"gradyan kontrolü\" kullanacaksınız.\n",
        "\n",
        "Haydi Yapalım şunu!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nsmJAheblstN"
      },
      "outputs": [],
      "source": [
        "# Packages\n",
        "import numpy as np\n",
        "#from testCases import *\n",
        "#from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Yardımcı Fonksiyonlar\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of x\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size.\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "    s = 1/(1+np.exp(-x))\n",
        "    return s\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Compute the relu of x\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size.\n",
        "    Return:\n",
        "    s -- relu(x)\n",
        "    \"\"\"\n",
        "    s = np.maximum(0,x)\n",
        "    \n",
        "    return s\n",
        "\n",
        "def dictionary_to_vector(parameters):\n",
        "    \"\"\"\n",
        "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
        "    \"\"\"\n",
        "    keys = []\n",
        "    count = 0\n",
        "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n",
        "        \n",
        "        # flatten parameter\n",
        "        new_vector = np.reshape(parameters[key], (-1,1))\n",
        "        keys = keys + [key]*new_vector.shape[0]\n",
        "        \n",
        "        if count == 0:\n",
        "            theta = new_vector\n",
        "        else:\n",
        "            theta = np.concatenate((theta, new_vector), axis=0)\n",
        "        count = count + 1\n",
        "\n",
        "    return theta, keys\n",
        "\n",
        "def vector_to_dictionary(theta):\n",
        "    \"\"\"\n",
        "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
        "    \"\"\"\n",
        "    parameters = {}\n",
        "    parameters[\"W1\"] = theta[:20].reshape((5,4))\n",
        "    parameters[\"b1\"] = theta[20:25].reshape((5,1))\n",
        "    parameters[\"W2\"] = theta[25:40].reshape((3,5))\n",
        "    parameters[\"b2\"] = theta[40:43].reshape((3,1))\n",
        "    parameters[\"W3\"] = theta[43:46].reshape((1,3))\n",
        "    parameters[\"b3\"] = theta[46:47].reshape((1,1))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def gradients_to_vector(gradients):\n",
        "    \"\"\"\n",
        "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
        "    \"\"\"\n",
        "    \n",
        "    count = 0\n",
        "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\"]:\n",
        "        # flatten parameter\n",
        "        new_vector = np.reshape(gradients[key], (-1,1))\n",
        "        \n",
        "        if count == 0:\n",
        "            theta = new_vector\n",
        "        else:\n",
        "            theta = np.concatenate((theta, new_vector), axis=0)\n",
        "        count = count + 1\n",
        "\n",
        "    return theta"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KFFHw1vJmQIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Yardımcı Test Fonksiyonları\n",
        "import numpy as np\n",
        "\n",
        "def gradient_check_n_test_case(): \n",
        "    np.random.seed(1)\n",
        "    x = np.random.randn(4,3)\n",
        "    y = np.array([1, 1, 0])\n",
        "    W1 = np.random.randn(5,4) \n",
        "    b1 = np.random.randn(5,1) \n",
        "    W2 = np.random.randn(3,5) \n",
        "    b2 = np.random.randn(3,1) \n",
        "    W3 = np.random.randn(1,3) \n",
        "    b3 = np.random.randn(1,1) \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2,\n",
        "                  \"W3\": W3,\n",
        "                  \"b3\": b3}\n",
        "\n",
        "    \n",
        "    return x, y, parameters"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7erlpB21mK2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nmtxc2IlstP"
      },
      "source": [
        "## 1) Gradyan kontrolü nasıl çalışır?\n",
        "\n",
        "Geri yayılım $\\frac{\\partial J}{\\partial \\theta}$ gradyanlarını hesaplar, burada $\\theta$ modelin parametrelerini gösterir. $J$, ileriye doğru yayılma ve sizin kayıp fonksiyonunuz kullanılarak hesaplanır.\n",
        "\n",
        "İleri yayılımın uygulanması nispeten kolay olduğu için, bunu doğru yaptığınızdan eminsiniz ve dolayısıyla $J$ maliyetini doğru hesapladığınızdan neredeyse %100 eminsiniz. Böylece, $\\frac{\\partial J}{\\partial \\theta}$ hesaplama kodunu doğrulamak için kodunuzu $J$ hesaplamak için kullanabilirsiniz.\n",
        "\n",
        "Bir türevin (veya gradyanın) tanımına tekrar bakalım:\n",
        "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
        "\n",
        "\"$\\displaystyle \\lim_{\\varepsilon \\to 0}$\" gösterimine aşina değilseniz, bu sadece \"$\\varepsilon$ gerçekten çok küçük olduğunda\" demenin bir yoludur.\n",
        "\n",
        "Aşağıdakileri biliyoruz:\n",
        "\n",
        "- $\\frac{\\partial J}{\\partial \\theta}$, doğru hesaplama yaptığınızdan emin olmak istediğiniz şeydir.\n",
        "- $J(\\theta + \\varepsilon)$ ve $J(\\theta - \\varepsilon)$'ı ($\\theta$'ın gerçek bir sayı olması durumunda) hesaplayabilirsiniz, çünkü $J$ için uygulama doğrudur.\n",
        "\n",
        "\n",
        "CEO'nuzu $\\frac{\\partial J}{\\partial \\theta}$ hesaplama kodunuzun doğru olduğuna ikna etmek için denklem (1) ve $\\varepsilon$ için küçük bir değer kullanalım!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InzGHaCzlstQ"
      },
      "source": [
        "## 2) 1 boyutlu gradyan kontrolü\n",
        "\n",
        "Bir 1B doğrusal fonksiyon $J(\\theta) = \\theta x$ düşünün. Model yalnızca tek bir gerçek değerli parametre $\\theta$ içerir ve girdi olarak $x$ alır.\n",
        "\n",
        "$J(.)$ ve onun türevini $\\frac{\\partial J}{\\partial \\theta}$ hesaplamak için kod uygulayacaksınız. Daha sonra $J$ için türev hesaplamanızın doğru olduğundan emin olmak için gradyan kontrolünü kullanacaksınız.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1NMOvE2Q2vklLB5G4Nsm7BGc3SNf3ApkR\" style=\"width:600px;height:250px;\" alt=\"1Dgrad_kiank\" title=\"1Dgrad_kiank\">\n",
        "<caption><center> <u> **Şekil 1** </u>: **1B doğrusal model**<br> </center></caption>\n",
        "\n",
        "Yukarıdaki şema, temel hesaplama adımlarını göstermektedir: Önce $x$ ile başlayın, ardından $J(x)$ (\"ileri yayılım\") işlevini değerlendirin. Ardından, $\\frac{\\partial J}{\\partial \\theta}$ (\"geriye doğru yayılma\") türevini hesaplayın.\n",
        "\n",
        "**Alıştırma**: Bu basit işlev için \"ileri yayılım\" ve \"geri yayılım\" uygulayın. Yani, hem $J(.)$ (\"ileri yayılma\") hem de onun türevini $\\theta$'a (\"geri yayılma\") göre iki ayrı fonksiyonda hesaplayın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ef9EDa0VlstR"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: forward_propagation\n",
        "\n",
        "def forward_propagation(x, theta):\n",
        "    \"\"\"\n",
        "    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)\n",
        "    \n",
        "    Arguments:\n",
        "    x -- a real-valued input\n",
        "    theta -- our parameter, a real number as well\n",
        "    \n",
        "    Returns:\n",
        "    J -- the value of function J, computed using the formula J(theta) = theta * x\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (approx. 1 line)\n",
        "    J = np.dot(theta, x)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCwKo88ZlstT"
      },
      "outputs": [],
      "source": [
        "x, theta = 2, 4\n",
        "J = forward_propagation(x, theta)\n",
        "print (\"J = \" + str(J))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kc69288lstU"
      },
      "source": [
        "**Alıştırma**: Şimdi, Şekil 1'deki geriye doğru yayılma adımını (türevsel hesaplama) uygulayın. Yani, $J(\\theta) = \\theta x$'ın $\\theta$'a göre türevini hesaplayın. Sizi hesap yapmaktan kurtarmak için $dtheta = \\frac { \\partial J }{ \\partial \\theta} = x$ elde etmelisiniz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "thzkGg8slstV"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: backward_propagation\n",
        "\n",
        "def backward_propagation(x, theta):\n",
        "    \"\"\"\n",
        "    Computes the derivative of J with respect to theta (see Figure 1).\n",
        "    \n",
        "    Arguments:\n",
        "    x -- a real-valued input\n",
        "    theta -- our parameter, a real number as well\n",
        "    \n",
        "    Returns:\n",
        "    dtheta -- the gradient of the cost with respect to theta\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (approx. 1 line)\n",
        "    dtheta = x\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return dtheta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vGsozxyglstX"
      },
      "outputs": [],
      "source": [
        "x, theta = 2, 4\n",
        "dtheta = backward_propagation(x, theta)\n",
        "print (\"dtheta = \" + str(dtheta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNVTHXRClstZ"
      },
      "source": [
        "**Alıştırma**: `backward_propagation()` fonksiyonunun $\\frac{\\partial J}{\\partial \\theta}$ gradyanını doğru bir şekilde hesapladığını göstermek için gradyan kontrolünü uygulayalım.\n",
        "\n",
        "**Talimatlar**:\n",
        "- İlk önce yukarıdaki formülü (1) ve küçük bir $\\varepsilon$ değerini kullanarak \"yaklaşık\" derecesini hesaplayın. İzlenecek Adımlar:\n",
        "  1. $\\theta^{+} = \\theta + \\varepsilon$\n",
        "  2. $\\theta^{-} = \\theta - \\varepsilon$\n",
        "  3. $J^{+} = J(\\theta^{+})$\n",
        "  4. $J^{-} = J(\\theta^{-})$\n",
        "  5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
        "\n",
        "- Ardından geriye doğru yayılmayı kullanarak gradyanı hesaplayın ve sonucu bir \"grad\" değişkeninde saklayın\n",
        "\n",
        "- Son olarak, aşağıdaki formülü kullanarak \"yaklaşık dereceli\" ve \"dereceli\" arasındaki bağıl farkı hesaplayın:\n",
        "\n",
        "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
        "\n",
        "Bu formülü hesaplamak için 3 Adıma ihtiyacınız olacak:\n",
        "  - 1'. np.linalg.norm(...) kullanarak payı hesaplayın\n",
        "  - 2'. paydayı hesaplayın. np.linalg.norm(...)'u iki kez aramanız gerekecek.\n",
        "  - 3'. onları böl.\n",
        "\n",
        "- Bu fark küçükse (10$^{-7}$'dan az diyelim), gradyanınızı doğru hesapladığınızdan oldukça emin olabilirsiniz. Aksi takdirde, gradyan hesaplamasında bir hata olabilir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9K-kzOt_lstZ"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: gradient_check\n",
        "\n",
        "def gradient_check(x, theta, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation presented in Figure 1.\n",
        "    \n",
        "    Arguments:\n",
        "    x -- a real-valued input\n",
        "    theta -- our parameter, a real number as well\n",
        "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
        "    \n",
        "    Returns:\n",
        "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
        "    \"\"\"\n",
        "    \n",
        "    # Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit.\n",
        "    ### START CODE HERE ### (approx. 5 lines)\n",
        "    thetaplus = theta + epsilon                               # Step 1\n",
        "    thetaminus = theta - epsilon                              # Step 2\n",
        "    J_plus = forward_propagation(x, thetaplus)                # Step 3\n",
        "    J_minus = forward_propagation(x, thetaminus)              # Step 4\n",
        "    gradapprox = (J_plus - J_minus) / (2 * epsilon)           # Step 5\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Check if gradapprox is close enough to the output of backward_propagation()\n",
        "    ### START CODE HERE ### (approx. 1 line)\n",
        "    grad = backward_propagation(x, theta)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    ### START CODE HERE ### (approx. 1 line)\n",
        "    numerator = np.linalg.norm(grad - gradapprox)                      # Step 1'\n",
        "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)    # Step 2'\n",
        "    difference = numerator / denominator                               # Step 3'\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    if difference < 1e-7:\n",
        "        print(\"The gradient is correct!\")\n",
        "    else:\n",
        "        print(\"The gradient is wrong!\")\n",
        "    \n",
        "    return difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fuWIGTrMlsta"
      },
      "outputs": [],
      "source": [
        "x, theta = 2, 4\n",
        "difference = gradient_check(x, theta)\n",
        "print(\"difference = \" + str(difference))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0lhCQX6lstb"
      },
      "source": [
        "Tebrikler, fark 10$^{-7}$ eşiğinden daha küçük. Böylece, `backward_propagation()` içindeki gradyanı doğru hesapladığınızdan emin olabilirsiniz.\n",
        "\n",
        "Şimdi, daha genel durumda, maliyet fonksiyonunuz $J$'ın birden fazla 1D girdisi var. Bir sinir ağını eğitirken, $\\theta$ aslında birden çok $W^{[l]}$ matrisinden ve $b^{[l]}$ sapmalarından oluşur! Daha yüksek boyutlu girdilerle gradyan kontrolünün nasıl yapıldığını bilmek önemlidir. Haydi Yapalım şunu!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qk02UMFlstc"
      },
      "source": [
        "## 3) N boyutlu gradyan kontrolü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "F_9SfDWxlstc"
      },
      "source": [
        "Aşağıdaki şekil, dolandırıcılık tespit modelinizin ileriye ve geriye doğru yayılmasını açıklamaktadır.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1uFZTCD9kNXn92G5JNzyZbiJJxxMu9Ggn\" style=\"width:600px;height:400px;\" alt=\"NDgrad_kiank\" title=\"NDgrad_kiank\">\n",
        "\n",
        "<caption><center> <u> **Şekil 2** </u>: **derin sinir ağı**<br>*LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID*</center></caption>\n",
        "\n",
        "İleriye yayılma ve geriye yayılma için uygulamalarınıza bakalım."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dvUWg7aflstc"
      },
      "outputs": [],
      "source": [
        "def forward_propagation_n(X, Y, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation (and computes the cost) presented in Figure 3.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- training set for m examples\n",
        "    Y -- labels for m examples \n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
        "                    W1 -- weight matrix of shape (5, 4)\n",
        "                    b1 -- bias vector of shape (5, 1)\n",
        "                    W2 -- weight matrix of shape (3, 5)\n",
        "                    b2 -- bias vector of shape (3, 1)\n",
        "                    W3 -- weight matrix of shape (1, 3)\n",
        "                    b3 -- bias vector of shape (1, 1)\n",
        "    \n",
        "    Returns:\n",
        "    cost -- the cost function (logistic cost for one example)\n",
        "    \"\"\"\n",
        "    \n",
        "    # retrieve parameters\n",
        "    m = X.shape[1]\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "\n",
        "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    A3 = sigmoid(Z3)\n",
        "\n",
        "    # Cost\n",
        "    logprobs = np.multiply(-np.log(A3), Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
        "    cost = 1. / m * np.sum(logprobs)\n",
        "    \n",
        "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "    \n",
        "    return cost, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QcphItDlstd"
      },
      "source": [
        "Şimdi, geriye doğru yayılmayı çalıştırın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7-teGi6Plstd"
      },
      "outputs": [],
      "source": [
        "def backward_propagation_n(X, Y, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation presented in figure 2.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input datapoint, of shape (input size, 1)\n",
        "    Y -- true \"label\"\n",
        "    cache -- cache output from forward_propagation_n()\n",
        "    \n",
        "    Returns:\n",
        "    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "    \n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
        "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
        "    \n",
        "    dA2 = np.dot(W3.T, dZ3)\n",
        "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "    dW2 = 1. / m * np.dot(dZ2, A1.T) * 2  # Should not multiply by 2\n",
        "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    \n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
        "    db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True) # Should not multiply by 4\n",
        "    \n",
        "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
        "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
        "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "    \n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "AsLGzXxElste"
      },
      "source": [
        "Sahtekarlık tespit test setinde bazı sonuçlar elde ettiniz ancak modelinizden %100 emin değilsiniz. Kimse mükemmel değildir! Degradelerinizin doğru olup olmadığını doğrulamak için degrade denetimi uygulayalım."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbSC7qbKlste"
      },
      "source": [
        "**Grandyan denetimi nasıl çalışır?**.\n",
        "\n",
        "1) ve 2)'de olduğu gibi, \"yaklaşık gradyan\"ı geri yayılım tarafından hesaplanan gradyanla karşılaştırmak istiyorsunuz. Formül hala:\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
        "\n",
        "Ancak, $\\theta$ artık bir skaler değildir.  \"parameters\" adlı bir sözlüktür. Sizin için \"`dictionary_to_vector()`\" fonksiyonunu uyguladık.  \"parameters\" sözlüğünü, tüm parametreleri (W1, b1, W2, b2, W3, b3) vektörlere dönüştürerek ve birleştirerek elde edilen \"değerler\" adlı bir vektöre dönüştürür.\n",
        "\n",
        "Ters işlev, \"parameters\" sözlüğünü geri veren \"`vector_to_dictionary`\" işlevidir.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1wndk8bJjkKW2Bl9EmeOWgXoP6WQGwT6f\" style=\"width:600px;height:400px;\" alt=\"dictionary_to_vector\" title=\"dictionary_to_vector\">\n",
        "\n",
        "<caption><center> <u> **Şekil 3** </u>: **dictionary_to_vector() ve vector_to_dictionary()**<br> Bu fonksiyonlara gradyan_check_n() içinde ihtiyacınız olacak</center></caption>\n",
        "\n",
        "Ayrıca \"gradients\" sözlüğünü degrades_to_vector() kullanarak bir \"grad\" vektörüne dönüştürdük. Bunun için endişelenmene gerek yok.\n",
        "\n",
        "**Alıştırma**: gradient_check_n() uygulayın.\n",
        "\n",
        "**Talimatlar**: Degrade kontrolünü uygulamanıza yardımcı olacak sözde kod burada.\n",
        "\n",
        "num_parameters içindeki her i için:\n",
        "- \"J_plus[i]\" hesaplamak için:\n",
        "     1. $\\theta^{+}$ öğesini `np.copy(parameters_values)` olarak ayarlayın\n",
        "     2. $\\theta^{+}_i$'ı $\\theta^{+}_i + \\varepsilon$ olarak ayarlayın\n",
        "     3. `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))` kullanarak $J^{+}_i$ değerini hesaplayın.\n",
        "- `J_minus[i]` hesaplamak için: $\\theta^{-}$ ile aynı şeyi yapın\n",
        "- $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$ hesaplayın\n",
        "\n",
        "Böylece, gradapprox vektör elde edersiniz; burada gradapprox[i], `parameter_values[i]`ne göre gradyanın bir tahminidir. Şimdi bu gradyan vektörünü geri yayılımdan gradyan vektörüyle karşılaştırabilirsiniz. 1B durum için olduğu gibi (Adım 1', 2', 3'), hesaplayın:\n",
        "\n",
        "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WaFvX-Uglstf"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: gradient_check_n\n",
        "\n",
        "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
        "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
        "    x -- input datapoint, of shape (input size, 1)\n",
        "    y -- true \"label\"\n",
        "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
        "    \n",
        "    Returns:\n",
        "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set-up variables\n",
        "    parameters_values, _ = dictionary_to_vector(parameters)\n",
        "    grad = gradients_to_vector(gradients)\n",
        "    num_parameters = parameters_values.shape[0]\n",
        "    J_plus = np.zeros((num_parameters, 1))\n",
        "    J_minus = np.zeros((num_parameters, 1))\n",
        "    gradapprox = np.zeros((num_parameters, 1))\n",
        "    \n",
        "    # Compute gradapprox\n",
        "    for i in range(num_parameters):\n",
        "        \n",
        "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
        "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
        "        ### START CODE HERE ### (approx. 3 lines)\n",
        "        thetaplus =  np.copy(parameters_values)                                       # Step 1\n",
        "        thetaplus[i][0] = thetaplus[i][0] + epsilon                                   # Step 2\n",
        "        J_plus[i], _ =  forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))  # Step 3\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
        "        ### START CODE HERE ### (approx. 3 lines)\n",
        "        thetaminus = np.copy(parameters_values)                                       # Step 1\n",
        "        thetaminus[i][0] = thetaminus[i][0] - epsilon                                 # Step 2        \n",
        "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus)) # Step 3\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute gradapprox[i]\n",
        "        ### START CODE HERE ### (approx. 1 line)\n",
        "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
        "    ### START CODE HERE ### (approx. 1 line)\n",
        "    numerator = np.linalg.norm(grad - gradapprox)                                     # Step 1'\n",
        "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                   # Step 2'\n",
        "    difference = numerator / denominator                                              # Step 3'\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    if difference > 1e-7:\n",
        "        print(\"There is a mistake in the backward propagation! difference = \" + str(difference))\n",
        "    else:\n",
        "        print(\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
        "    \n",
        "    return difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "NvKVAZLLlstg"
      },
      "outputs": [],
      "source": [
        "X, Y, parameters = gradient_check_n_test_case()\n",
        "\n",
        "cost, cache = forward_propagation_n(X, Y, parameters)\n",
        "gradients = backward_propagation_n(X, Y, cache)\n",
        "difference = gradient_check_n(parameters, gradients, X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrj16Oeulsth"
      },
      "source": [
        "Size verdiğimiz `backward_propagation_n` kodunda hatalar var gibi görünüyor! Degrade kontrolünü uyguladığınız iyi oldu. `backward_propagation`a geri dönün ve hataları bulmaya/düzeltmeye çalışın *(İpucu: dW2 ve db1'i kontrol edin*). Düzelttiğinizi düşündüğünüzde degrade kontrolünü yeniden çalıştırın. Kodu değiştirirseniz, `backward_propagation_n()` tanımlayan hücreyi yeniden çalıştırmanız gerekeceğini unutmayın.\n",
        "\n",
        "**Not**\n",
        "- Gradyan Kontrolü yavaş! Gradyanı $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ ile yaklaşık olarak hesaplamak hesaplama açısından maliyetlidir . Bu nedenle, eğitim sırasında her yinelemede gradyan kontrolü yapmıyoruz. Gradyanın doğru olup olmadığını kontrol etmek için sadece birkaç kez.\n",
        "\n",
        "- Gradyan Kontrolü, en azından sunduğumuz gibi, bırakma ile çalışmaz. Backprop'unuzun doğru olduğundan emin olmak için genellikle gradyan kontrol algoritmasını bırakma olmadan çalıştırırsınız, ardından bırakma eklersiniz.\n",
        "\n",
        "Tebrikler, dolandırıcılık tespiti için derin öğrenme modelinizin doğru çalıştığından emin olabilirsiniz! Bunu CEO'nuzu ikna etmek için bile kullanabilirsiniz. :)\n",
        "\n",
        "\n",
        "<font color='blue'>\n",
        "**Bu çalışma dosyasında hatırlamanız gerekenler**:\n",
        "\n",
        "- Gradyan kontrolü, geri yayılımdan gelen gradyanlar ile gradyanın sayısal yaklaşımı (ileri yayılım kullanılarak hesaplanır) arasındaki yakınlığı doğrular.\n",
        "\n",
        "- Gradyan kontrolü yavaştır, bu yüzden onu her eğitim yinelemesinde çalıştırmıyoruz. Genellikle yalnızca kodunuzun doğru olduğundan emin olmak için çalıştırır, ardından kapatır ve gerçek öğrenme süreci için backprop kullanırsınız."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4p4C8h0e1RWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "deep-neural-network",
      "graded_item_id": "n6NBD",
      "launcher_item_id": "yfOsE"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Day10_ANN_3_Gradient_Checking.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}