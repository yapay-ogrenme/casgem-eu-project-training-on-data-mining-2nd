{"cells":[{"cell_type":"markdown","metadata":{"id":"jYysdyb-CaWM"},"source":["# Simple audio recognition: Recognizing keywords"]},{"cell_type":"markdown","metadata":{"id":"SPfDNFlb66XF"},"source":["Bu çalışma dosyasında, WAV biçimindeki ses dosyalarına nasıl ön işlem uygulanacağını ve temel bir <a href=\"https://en.wikipedia.org/wiki/Speech_recognition\" class=\"external\">otomatik konuşma tanıma (automatic speech recognition)</a> (ASR) modeli oluşturulup eğitileceğini gösterir. \n","\n","On farklı kelimeyi tanımak için **\"aşağı (down)\", \"git (go)\", \"sol (left)\", \"hayır (no)\", \"sağ (right)\", \"dur (stop)\", \"yukarı (up)\" ve \"evet (yes)\"** gibi komutların kısa (bir saniye veya daha az) ses kliplerini içeren [Speech Commands dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) bir bölümünü kullanacaksınız (<a href=\"https://arxiv.org/abs/1804.03209\" class=\"external\">Warden, 2018</a>)\n","\n","Gerçek dünyadaki konuşma ve ses tanıma <a href=\"https://ai.googleblog.com/search/label/Speech%20Recognition\" class=\"external\">sistemleri</a> karmaşıktır. Ancak, MNIST veri kümesiyle görüntü sınıflandırması gibi, bu eğitim size ilgili teknikler hakkında temel bir anlayış vermelidir."]},{"cell_type":"markdown","metadata":{"id":"Go9C3uLL8Izc"},"source":["## Kurulum\n","\n","Gerekli modülleri ve bağımlılıkları import edin. Bu çalışma dosyasında görselleştirme için <a href=\"https://seaborn.pydata.org/\" class=\"external\">seaborn</a> kullanacağınızı unutmayın."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzLKpmZICaWN"},"outputs":[],"source":["import os\n","import pathlib\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from IPython import display\n","\n","# Set the seed value for experiment reproducibility.\n","seed = 42\n","tf.random.set_seed(seed)\n","np.random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"yR0EdgrLCaWR"},"source":["## Mini Konuşma Komutları Veri Kümesini yükleyin\n","\n","Veri yüklemeyle zaman kazanmak için Konuşma Komutları veri kümesinin daha küçük bir sürümüyle çalışacaksınız. [Orijinal veri kümesi](https://www.tensorflow.org/datasets/catalog/speech_commands), adresindeki 105.000'den fazla ses dosyasından oluşur. <a href=\"https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf\" class=\"external\">WAV (Dalga formu) ses dosyası formatı</a>, 35 farklı kelime söyleyen kişiler.\n","\n","Daha küçük Konuşma Komutları veri kümelerini içeren `mini_speech_commands.zip dosyasını `tf.keras.utils.get_file` ile indirin ve çıkarın:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-rayb7-3Y0I"},"outputs":[],"source":["DATASET_PATH = 'data/mini_speech_commands'\n","\n","data_dir = pathlib.Path(DATASET_PATH)\n","if not data_dir.exists():\n","  tf.keras.utils.get_file(\n","      'mini_speech_commands.zip',\n","      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n","      extract=True,\n","      cache_dir='.', cache_subdir='data')"]},{"cell_type":"markdown","metadata":{"id":"BgvFq3uYiS5G"},"source":["Veri kümesinin ses klipleri, her konuşma komutuna karşılık gelen sekiz klasörde saklanır: \"hayır\", \"evet\", \"aşağı\", \"git\", \"sol\", \"yukarı\", \"sağ\" ve \"dur\":"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70IBxSKxA1N9"},"outputs":[],"source":["commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n","commands = commands[commands != 'README.md']\n","print('Commands:', commands)"]},{"cell_type":"markdown","metadata":{"id":"aMvdU9SY8WXN"},"source":["Ses kliplerini `filenames` adlı bir listeye çıkarın ve karıştırın:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlX685l1wD9k"},"outputs":[],"source":["filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n","filenames = tf.random.shuffle(filenames)\n","num_samples = len(filenames)\n","print('Number of total examples:', num_samples)\n","print('Number of examples per label:',\n","      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))\n","print('Example file tensor:', filenames[0])"]},{"cell_type":"markdown","metadata":{"id":"9vK3ymy23MCP"},"source":["`filenames` sırasıyla 80:10:10 oranını kullanarak eğitim, doğrulama ve test kümelerine ayırın:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cv_wts-l3KgD"},"outputs":[],"source":["train_files = filenames[:6400]\n","val_files = filenames[6400: 6400 + 800]\n","test_files = filenames[-800:]\n","\n","print('Training set size', len(train_files))\n","print('Validation set size', len(val_files))\n","print('Test set size', len(test_files))"]},{"cell_type":"markdown","metadata":{"id":"g2Cj9FyvfweD"},"source":["## Ses dosyalarını ve etiketlerini okuyun"]},{"cell_type":"markdown","metadata":{"id":"j1zjcWteOcBy"},"source":["Bu bölümde, dalga formları ve ilgili etiketler için kodu çözülmüş tensörler oluşturarak veri setini önceden işleyeceksiniz.\n","\n","- Her WAV dosyası, saniyede belirli sayıda örnek içeren zaman serisi verileri içerir.\n","\n","- Her örnek, belirli bir zamanda ses sinyalinin <a href=\"https://en.wikipedia.org/wiki/Amplitude\" class=\"external\">genliğini (amplitude)</a> temsil eder.\n","\n","- Bir <a href=\"https://en.wikipedia.org/wiki/Audio_bit_depth\" class=\"external\">16-bit</a> sistemde, mini Konuşma Komutları veri kümesindeki WAV dosyaları gibi, genlik değerler -32.768 ile 32.767 arasındadır.\n","\n","- Bu veri kümesi için <a href=\"https://en.wikipedia.org/wiki/Sampling_(signal_processing)#Audio_sampling\" class=\"external\">örnek hızı (sample rate)</a> 16kHz'dir.\n","\n","`tf.audio.decode_wav`tarafından döndürülen tensörün boyutu `[samples, channels]` şeklindedir; burada `channels` mono için `1` veya stereo için `2`dir. \n","Mini Konuşma Komutları veri kümesi yalnızca mono kayıtları içerir."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d16bb8416f90"},"outputs":[],"source":["test_file = tf.io.read_file(DATASET_PATH+'/down/0a9f9af7_nohash_0.wav')\n","test_audio, _ = tf.audio.decode_wav(contents=test_file)\n","test_audio.shape"]},{"cell_type":"code","source":["display.display(display.Audio(DATASET_PATH+'/down/0a9f9af7_nohash_0.wav', rate=16000))"],"metadata":{"id":"A6s4ksXqsyUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6bb8defd2ef"},"source":["Şimdi, veri kümesinin ham WAV ses dosyalarını ses tensörlerine önişleyen bir işlev tanımlayalım:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PjJ2iXYwftD"},"outputs":[],"source":["def decode_audio(audio_binary):\n","  # Decode WAV-encoded audio files to `float32` tensors, normalized\n","  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n","  audio, _ = tf.audio.decode_wav(contents=audio_binary)\n","  # Since all the data is single channel (mono), drop the `channels`\n","  # axis from the array.\n","  return tf.squeeze(audio, axis=-1)"]},{"cell_type":"markdown","metadata":{"id":"GPQseZElOjVN"},"source":["Her dosya için üst dizinleri kullanarak etiketler oluşturan bir fonksiyon tanımlayın:\n","\n","- Dosya yollarını `tf.RaggedTensor`lara ayırın (düzensiz boyutlara sahip tensörler - farklı uzunluklara sahip olabilen dilimler)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8VTtX1nr3YT-"},"outputs":[],"source":["def get_label(file_path):\n","  parts = tf.strings.split(\n","      input=file_path,\n","      sep=os.path.sep)\n","  # Note: You'll use indexing here instead of tuple unpacking to enable this\n","  # to work in a TensorFlow graph.\n","  return parts[-2]"]},{"cell_type":"markdown","metadata":{"id":"E8Y9w_5MOsr-"},"source":["Hepsini bir araya getiren başka bir yardımcı fonksiyon (`get_waveform_and_label`) tanımlayın:\n","\n","- Giriş, WAV ses dosyası adıdır.\n","- Çıktı, denetimli öğrenmeye hazır ses ve etiket tensörlerini içeren bir demettir."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdgUD5T93NyT"},"outputs":[],"source":["def get_waveform_and_label(file_path):\n","  label = get_label(file_path)\n","  audio_binary = tf.io.read_file(file_path)\n","  waveform = decode_audio(audio_binary)\n","  return waveform, label"]},{"cell_type":"markdown","metadata":{"id":"nvN8W_dDjYjc"},"source":["Ses etiketi çiftlerini çıkarmak için eğitim setini oluşturun:\n","\n","- Daha önce tanımlanan `get_waveform_and_label` kullanarak `Dataset.from_tensor_slices` ve `Dataset.map` ile bir `tf.data.Dataset` oluşturun.\n","\n","Daha sonra benzer bir prosedür kullanarak doğrulama ve test setlerini oluşturacaksınız."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SQl8yXl3kNP"},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n","\n","waveform_ds = files_ds.map(\n","    map_func=get_waveform_and_label,\n","    num_parallel_calls=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"voxGEwvuh2L7"},"source":["Birkaç ses dalga biçimi çizelim:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yuX6Nqzf6wT"},"outputs":[],"source":["rows = 3\n","cols = 3\n","n = rows * cols\n","fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n","\n","for i, (audio, label) in enumerate(waveform_ds.take(n)):\n","  r = i // cols\n","  c = i % cols\n","  ax = axes[r][c]\n","  ax.plot(audio.numpy())\n","  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n","  label = label.numpy().decode('utf-8')\n","  ax.set_title(label)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"EWXPphxm0B4m"},"source":["## Dalga formlarını spektrogramlara dönüştürün\n","\n","Veri kümesindeki dalga biçimleri zaman alanında temsil edilir. Ardından, dalga biçimlerini zaman içinde frekans değişikliklerini gösteren ve dalga biçimlerini spektrogramlara dönüştürmek için <a href=\"https://en.wikipedia.org/wiki/Short-time_Fourier_transform\" class=\"external\">short-time Fourier transform (STFT)</a> hesaplayarak dalga biçimlerini zaman etki alanı sinyallerinden zaman-frekans etki alanı sinyallerine dönüştüreceksiniz. 2D görüntüler olarak temsil edilir. Modeli eğitmek için <a href=\"https://en.wikipedia.org/wiki/Spectrogram\" clas=\"external\">spektrogram</a> görüntülerini sinir ağınıza besleyeceksiniz.\n","\n","Fourier dönüşümü (`tf.signal.fft`), bir sinyali bileşen frekanslarına dönüştürür, ancak tüm zaman bilgilerini kaybeder. Buna karşılık, STFT (`tf.signal.stft`) sinyali zaman pencerelerine böler ve her pencerede bir Fourier dönüşümü çalıştırarak zaman bilgisini korur ve üzerinde standart evrişimler çalıştırabileceğiniz bir 2B tensör döndürür.\n","\n","Dalga biçimlerini spektrogramlara dönüştürmek için bir yardımcı program işlevi oluşturun:\n","\n","- Dalga biçimlerinin aynı uzunlukta olması gerekir, böylece onları spektrogramlara dönüştürdüğünüzde sonuçlar benzer boyutlara sahip olur. Bu, bir saniyeden daha kısa olan ses kliplerinin sıfırlanmasıyla (`tf.zeros` kullanılarak) yapılabilir.\n","\n","- `tf.signal.stft`yi çağırırken, oluşturulan spektrogram \"görüntü\" neredeyse kare olacak şekilde `frame_length` ve `frame_step` parametrelerini seçin. STFT parametreleri seçimi hakkında daha fazla bilgi için, ses sinyali işleme ve STFT ile ilgili bu <a href=\"https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe\" class=\"external\">this Coursera video</a> videosuna bakın.\n","\n","- STFT, büyüklük ve fazı temsil eden bir dizi karmaşık sayı üretir. Ancak, bu öğreticide yalnızca `tf.signal.stft` çıktısına `tf.abs` uygulayarak türetebileceğiniz büyüklüğü kullanacaksınız."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4CK75DHz_OR"},"outputs":[],"source":["def get_spectrogram(waveform):\n","  # Zero-padding for an audio waveform with less than 16,000 samples.\n","  input_len = 16000\n","  waveform = waveform[:input_len]\n","  zero_padding = tf.zeros(\n","      [16000] - tf.shape(waveform),\n","      dtype=tf.float32)\n","  # Cast the waveform tensors' dtype to float32.\n","  waveform = tf.cast(waveform, dtype=tf.float32)\n","  # Concatenate the waveform with `zero_padding`, which ensures all audio\n","  # clips are of the same length.\n","  equal_length = tf.concat([waveform, zero_padding], 0)\n","  # Convert the waveform to a spectrogram via a STFT.\n","  spectrogram = tf.signal.stft(\n","      equal_length, frame_length=255, frame_step=128)\n","  # Obtain the magnitude of the STFT.\n","  spectrogram = tf.abs(spectrogram)\n","  # Add a `channels` dimension, so that the spectrogram can be used\n","  # as image-like input data with convolution layers (which expect\n","  # shape (`batch_size`, `height`, `width`, `channels`).\n","  spectrogram = spectrogram[..., tf.newaxis]\n","  return spectrogram"]},{"cell_type":"markdown","metadata":{"id":"5rdPiPYJphs2"},"source":["Ardından, verileri keşfetmeye başlayın. Bir örneğin gerginleştirilmiş dalga formunun ve ilgili spektrogramın şekillerini yazdırın ve orijinal sesi çalın:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Mu6Y7Yz3C-V"},"outputs":[],"source":["for waveform, label in waveform_ds.take(1):\n","  label = label.numpy().decode('utf-8')\n","  spectrogram = get_spectrogram(waveform)\n","\n","print('Label:', label)\n","print('Waveform shape:', waveform.shape)\n","print('Spectrogram shape:', spectrogram.shape)\n","print('Audio playback')\n","display.display(display.Audio(waveform, rate=16000))"]},{"cell_type":"markdown","metadata":{"id":"xnSuqyxJ1isF"},"source":["Şimdi, bir spektrogramı görüntülemek için bir fonksiyon tanımlayın:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e62jzb36-Jog"},"outputs":[],"source":["def plot_spectrogram(spectrogram, ax):\n","  if len(spectrogram.shape) > 2:\n","    assert len(spectrogram.shape) == 3\n","    spectrogram = np.squeeze(spectrogram, axis=-1)\n","  # Convert the frequencies to log scale and transpose, so that the time is\n","  # represented on the x-axis (columns).\n","  # Add an epsilon to avoid taking a log of zero.\n","  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n","  height = log_spec.shape[0]\n","  width = log_spec.shape[1]\n","  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n","  Y = range(height)\n","  ax.pcolormesh(X, Y, log_spec)"]},{"cell_type":"markdown","metadata":{"id":"baa5c91e8603"},"source":["Örneğin zaman içindeki dalga biçimini ve karşılık gelen spektrogramı (zaman içindeki frekanslar) çizin:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2_CikgY1tjv"},"outputs":[],"source":["fig, axes = plt.subplots(2, figsize=(12, 8))\n","timescale = np.arange(waveform.shape[0])\n","axes[0].plot(timescale, waveform.numpy())\n","axes[0].set_title('Waveform')\n","axes[0].set_xlim([0, 16000])\n","\n","plot_spectrogram(spectrogram.numpy(), axes[1])\n","axes[1].set_title('Spectrogram')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GyYXjW07jCHA"},"source":["Şimdi, dalga biçimi veri kümesini spektrogramlara ve bunlara karşılık gelen etiketleri tamsayı kimlikleri olarak dönüştüren bir işlev tanımlayın:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43IS2IouEV40"},"outputs":[],"source":["def get_spectrogram_and_label_id(audio, label):\n","  spectrogram = get_spectrogram(audio)\n","  label_id = tf.argmax(label == commands)\n","  return spectrogram, label_id"]},{"cell_type":"markdown","metadata":{"id":"cf5d5b033a45"},"source":["\n","`get_spectrogram_and_label_id` öğesini `Dataset.map` ile veri kümesinin öğeleri arasında eşleyin:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEVb_oK0oBLQ"},"outputs":[],"source":["spectrogram_ds = waveform_ds.map(\n","  map_func=get_spectrogram_and_label_id,\n","  num_parallel_calls=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"6gQpAAgMnyDi"},"source":["Veri kümesinin farklı örnekleri için spektrogramları inceleyin:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QUbHfTuon4iF"},"outputs":[],"source":["rows = 3\n","cols = 3\n","n = rows*cols\n","fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n","\n","for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n","  r = i // cols\n","  c = i % cols\n","  ax = axes[r][c]\n","  plot_spectrogram(spectrogram.numpy(), ax)\n","  ax.set_title(commands[label_id.numpy()])\n","  ax.axis('off')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"z5KdY8IF8rkt"},"source":["## Modeli oluşturun ve eğitin\n","\n","Doğrulama ve test kümelerinde, eğitim kümesindeki ön işlemesini tekrarlayın:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10UI32QH_45b"},"outputs":[],"source":["def preprocess_dataset(files):\n","  files_ds = tf.data.Dataset.from_tensor_slices(files)\n","\n","  output_ds = files_ds.map(\n","      map_func=get_waveform_and_label,\n","      num_parallel_calls=AUTOTUNE)\n","  \n","  output_ds = output_ds.map(\n","      map_func=get_spectrogram_and_label_id,\n","      num_parallel_calls=AUTOTUNE)\n","  \n","  return output_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNv4xwYkB2P6"},"outputs":[],"source":["train_ds = spectrogram_ds\n","val_ds = preprocess_dataset(val_files)\n","test_ds = preprocess_dataset(test_files)"]},{"cell_type":"markdown","metadata":{"id":"assnWo6SB3lR"},"source":["Model eğitimi için eğitim ve doğrulama kümelerini toplu olarak oluşturun:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UgY9WYzn61EX"},"outputs":[],"source":["batch_size = 64\n","train_ds = train_ds.batch(batch_size)\n","val_ds = val_ds.batch(batch_size)"]},{"cell_type":"markdown","metadata":{"id":"GS1uIh6F_TN9"},"source":["Modeli eğitirken okuma gecikmesini azaltmak için `Dataset.cache` ve `Dataset.prefetch` işlemlerini ekleyin:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdZ6M-F5_QzY"},"outputs":[],"source":["train_ds = train_ds.cache().prefetch(AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"rwHkKCQQb5oW"},"source":["Ses dosyalarını spektrogram görüntülerine dönüştürdüğünüz için model için basit bir evrişimsel sinir ağı (CNN) kullanacaksınız.\n","\n","\"`tf.keras.Sequential` modeliniz aşağıdaki Keras ön işleme katmanlarını kullanacaktır:\n","\n","- `tf.keras.layers.Resizing`: modelin daha hızlı çalışmasını sağlamak için girdiyi aşağı örneklemek (downsample) için.\n","\n","- `tf.keras.layers.Normalization`: görüntüdeki her pikseli ortalama ve standart sapmaya göre normalleştirmek için.\n","\n","`Normalization` katmanı için, toplu istatistikleri (yani, ortalama ve standart sapmayı) hesaplamak için ilk önce eğitim verilerinde `adapt` yönteminin çağrılması gerekir."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALYz7PFCHblP"},"outputs":[],"source":["for spectrogram, _ in spectrogram_ds.take(1):\n","  input_shape = spectrogram.shape\n","print('Input shape:', input_shape)\n","num_labels = len(commands)\n","\n","# Instantiate the `tf.keras.layers.Normalization` layer.\n","norm_layer = layers.Normalization()\n","# Fit the state of the layer to the spectrograms\n","# with `Normalization.adapt`.\n","norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n","\n","model = models.Sequential([\n","    layers.Input(shape=input_shape),\n","    # Downsample the input.\n","    layers.Resizing(32, 32),\n","    # Normalize.\n","    norm_layer,\n","    layers.Conv2D(32, 3, activation='relu'),\n","    layers.Conv2D(64, 3, activation='relu'),\n","    layers.MaxPooling2D(),\n","    layers.Dropout(0.25),\n","    layers.Flatten(),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dropout(0.5),\n","    layers.Dense(num_labels),\n","])\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"de52e5afa2f3"},"source":["Keras modelini Adam optimizer ve çapraz entropi kaybıyla (cross-entropy loss) yapılandırın:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFjj7-EmsTD-"},"outputs":[],"source":["#from_logits \n","#y_pred öğesinin bir logits tensörü olmasının beklenip beklenmediği. \n","#Varsayılan olarak, y_pred'in bir olasılık dağılımını kodladığını varsayıyoruz.\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=['accuracy'],\n",")"]},{"cell_type":"markdown","metadata":{"id":"f42b9e3a4705"},"source":["Modeli 10 tur(epoch) fazla eğitin:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttioPJVMcGtq"},"outputs":[],"source":["EPOCHS = 11\n","history = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=EPOCHS,\n","    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",")"]},{"cell_type":"markdown","metadata":{"id":"gjpCDeQ4mUfS"},"source":["Modelinizin eğitim sırasında nasıl geliştiğini kontrol etmek için eğitim ve doğrulama kaybı eğrilerini çizelim:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzhipg3Gu2AY"},"outputs":[],"source":["metrics = history.history\n","plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n","plt.legend(['loss', 'val_loss'])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5ZTt3kO3mfm4"},"source":["## Model performansını değerlendirin\n","\n","Modeli test kümesinde çalıştırın ve modelin performansını kontrol edin:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"biU2MwzyAo8o"},"outputs":[],"source":["test_audio = []\n","test_labels = []\n","\n","for audio, label in test_ds:\n","  test_audio.append(audio.numpy())\n","  test_labels.append(label.numpy())\n","\n","test_audio = np.array(test_audio)\n","test_labels = np.array(test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktUanr9mRZky"},"outputs":[],"source":["y_pred = np.argmax(model.predict(test_audio), axis=1)\n","y_true = test_labels\n","\n","test_acc = sum(y_pred == y_true) / len(y_true)\n","print(f'Test set accuracy: {test_acc:.0%}')"]},{"cell_type":"markdown","metadata":{"id":"en9Znt1NOabH"},"source":["### Karışıklık matrisi (confusion matrix) \n","\n","Modelin her birini ne kadar iyi sınıflandırdığını kontrol etmek için bir <a href=\"https://developers.google.com/machine-learning/glossary#confusion-matrix\" class=\"external\">karışıklık matrisi (confusion matrix)</a> kullanın. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvoSAOiXU3lL"},"outputs":[],"source":["confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n","\n","plt.figure(figsize=(10, 8))\n","\n","sns.heatmap(confusion_mtx,\n","            xticklabels=commands,\n","            yticklabels=commands,\n","            annot=True, fmt='g')\n","\n","plt.xlabel('Prediction')\n","plt.ylabel('Label')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mQGi_mzPcLvl"},"source":["## Bir ses dosyasında çıkarımı çalıştır\n","\n","Son olarak, **\"hayır\"** diyen birinin giriş ses dosyasını kullanarak modelin tahmin çıktısını doğrulayın. \n","Modeliniz ne kadar iyi performans gösteriyor?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRxauKMdhofU"},"outputs":[],"source":["sample_file = data_dir/'no/01bb6a2a_nohash_0.wav'\n","\n","sample_ds = preprocess_dataset([str(sample_file)])\n","\n","for spectrogram, label in sample_ds.batch(1):\n","  prediction = model(spectrogram)\n","  plt.bar(commands, tf.nn.softmax(prediction[0]))\n","  plt.title(f'Predictions for \"{commands[label[0]]}\"')\n","  plt.show()"]},{"cell_type":"code","source":["prediction"],"metadata":{"id":"OWCZ73vn2B1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.nn.softmax(prediction[0])"],"metadata":{"id":"2eYgqHdd2E5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display.display(display.Audio(str(data_dir/'no/01bb6a2a_nohash_0.wav'), rate=16000))"],"metadata":{"id":"5Hl4BsaUzZsX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VgWICqdqQNaQ"},"source":["Çıktının önerdiği gibi, modelinizin sesli komutu \"hayır\" olarak tanıması gerekir."]},{"cell_type":"markdown","metadata":{"id":"J3jF933m9z1J"},"source":["## Sonraki adımlar\n","\n","Bu öğretici, TensorFlow ve Python ile evrişimli bir sinir ağı kullanarak basit ses sınıflandırmasının/otomatik konuşma tanımanın nasıl gerçekleştirileceğini gösterdi. Daha fazla bilgi edinmek için aşağıdaki kaynakları göz önünde bulundurun:\n","\n","- The [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) tutorial shows how to use transfer learning for audio classification.\n","- The notebooks from <a href=\"https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview\" class=\"external\">Kaggle's TensorFlow speech recognition challenge</a>.\n","- The \n","<a href=\"https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0\" class=\"external\">TensorFlow.js - Audio recognition using transfer learning codelab</a> teaches how to build your own interactive web app for audio classification.\n","- <a href=\"https://arxiv.org/abs/1709.04396\" class=\"external\">A tutorial on deep learning for music information retrieval</a> (Choi et al., 2017) on arXiv.\n","- TensorFlow also has additional support for [audio data preparation and augmentation](https://www.tensorflow.org/io/tutorials/audio) to help with your own audio-based projects.\n","- Consider using the <a href=\"https://librosa.org/\" class=\"external\">librosa</a> library—a Python package for music and audio analysis."]},{"cell_type":"code","source":[""],"metadata":{"id":"WVIrvJYNI34b"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Day11_DeepLearning_6_simple_audio_recognition.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}