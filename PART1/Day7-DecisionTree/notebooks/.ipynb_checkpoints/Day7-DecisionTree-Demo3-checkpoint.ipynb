{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBg2kzsJzM9w"
   },
   "source": [
    "# AdaBoost Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbJYy3ZwzM93"
   },
   "source": [
    "\n",
    "## 1. Topluluk (Ensemble) Makine Öğrenmesin'e Giriş\n",
    "\n",
    "\n",
    "- Bir topluluk modeli, güçlü bir sınıflandırıcı oluşturmak amacıyla bir dizi düşük performanslı veya zayıf sınıflandırıcıyı birleştiren bir bileşik modeldir.\n",
    "\n",
    "- Burada, bireysel sınıflandırıcılar oy verir ve çoğunluk oylaması yapan nihai tahmin etiketi döndürülür.\n",
    "\n",
    "- Şimdi, bu bireysel sınıflandırıcılar, bir topluluk modeli oluşturmak için belirli bir kritere göre birleştirilir.\n",
    "\n",
    "- Bu topluluk modelleri, bireysel veya temel sınıflandırıcılardan daha fazla doğruluk sunar.\n",
    "\n",
    "- Bu modeller, her bir temel öğreniciyi farklı mekanizmalara tahsis ederek paralel hale getirilebilir.\n",
    "\n",
    "- Yani, topluluk öğrenme yöntemlerinin, performansı artırmak için birkaç makine öğrenmesi algoritmasını tek bir tahmin modelinde birleştiren meta-algoritmalar olduğunu söyleyebiliriz.\n",
    "\n",
    "- Topluluk modelleri, aşağıda belirtildiği gibi bazı belirli kriterlere göre oluşturulur:-\n",
    "\n",
    "  - **Bagging** - Bagging yaklaşımı kullanılarak model varyansını azaltmak için oluşturulabilirler.\n",
    "  \n",
    "  - **Boosting** - Boosting yaklaşımı kullanılarak model yanlılığını azaltmak için oluşturulabilirler.\n",
    "  \n",
    "  - **Stacking** - Stacking yaklaşımı kullanılarak model tahminlerini geliştirmek için oluşturulabilirler.\n",
    "  \n",
    "  \n",
    "- Aşağıdaki diyagram yardımı ile gösterilebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1YG88GczM95"
   },
   "source": [
    "![Ensemble Machine Learning](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1542651255/image_1_joyt3x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFyydkl7zM96"
   },
   "source": [
    "### 1.1 Bagging\n",
    "\n",
    "- **Bagging**, tahminlerin varyansını azaltacak şekilde birden fazla öğrenciyi birleştirir.\n",
    "\n",
    "- Örneğin, rastgele orman (random forest), verilerin farklı rastgele alt kümelerinde N farklı ağacı eğiteceğimiz ve nihai tahmin için oylama yapacağımız N Karar Ağacı eğitir.\n",
    "\n",
    "- **Bagging ensembles** yöntemleri **Rastgele Orman (Random Forest)** ve **Ekstra Ağaçlar (Extra Trees)**'dır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7TqHaQizM97"
   },
   "source": [
    "### 1.2 Boosting\n",
    "\n",
    "- **Boosting** algoritmaları, güçlü bir sınıflandırıcı oluşturmak için bir dizi zayıf sınıflandırıcıdır.\n",
    "\n",
    "- Güçlü sınıflandırıcılar 0'a yakın hata oranı sunar.\n",
    "\n",
    "- Boosting algoritması, doğru tahminde başarısız olan modeli izleyebilir.\n",
    "\n",
    "- Boosting algoritmaları, overfitting probleminden daha az etkilenir.\n",
    "\n",
    "- Aşağıdaki üç algoritma, veri bilimi yarışmalarında büyük popülerlik kazanmıştır.\n",
    "\n",
    "   - AdaBoost (Uyarlanabilir Güçlendirme)\n",
    "   - Gradyan Ağacı Artırma (GBM)\n",
    "   - XGBoost\n",
    "  \n",
    "- Bu çekirdekte AdaBoost'u ve gelecekteki çekirdeklerde GBM ve XGBoost'u tartışacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "filGTuQnzM9-"
   },
   "source": [
    "### 1.3 Stacking\n",
    "\n",
    "- **Stacking** (veya yığın genelleştirme), birden çok temel sınıflandırma modeli tahminlerini yeni bir veri kümesinde birleştiren bir topluluk (ensemble) öğrenme tekniğidir.\n",
    "\n",
    "- Bu yeni veriler, başka bir sınıflandırıcı için giriş verileri olarak değerlendirilir.\n",
    "\n",
    "- Bu sınıflandırıcı, bu sorunu çözmek için kullanılmıştır. Stacking genellikle blending (harmanlama) olarak adlandırılır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oaRkoW9zM9_"
   },
   "source": [
    "## 2. Base-learners Sınıflandırıcılar\n",
    "\n",
    "- Base-learners Sınıflandırıcılar iki türe ayrılır.\n",
    "\n",
    "\n",
    "- Base-learners düzenlenmesi temelinde, topluluk (ensemble) yöntemleri iki gruba ayrılabilir.\n",
    "\n",
    "   - Paralel topluluk yöntemlerinde, Base-learners paralel olarak oluşturulur, örneğin - Rastgele Orman (Random Forest).\n",
    "  \n",
    "   - Sıralı topluluk (sequential ensemble) yöntemlerinde, temel öğrenenler örneğin AdaBoost gibi sıralı olarak oluşturulur.\n",
    "  \n",
    "\n",
    "- Base-learners türüne göre, topluluk (ensemble) yöntemleri iki gruba ayrılabilir.\n",
    "\n",
    "   - Homojen topluluk yöntemi, her yinelemede aynı türde base-learner kullanır.\n",
    "  \n",
    "   - Heterojen topluluk yöntemi, her yinelemede farklı türde base-learner kullanır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbn9MyKAzM-B"
   },
   "source": [
    "# 3. AdaBoost Sınıflandırıcı\n",
    "\n",
    "\n",
    "- **AdaBoost veya Adaptive Boosting**, 1996 yılında Yoav Freund ve Robert Schapire tarafından önerilen ensemble boosting sınıflandırıcılarından biridir.\n",
    "\n",
    "- Sınıflandırıcıların doğruluğunu artırmak için birden çok zayıf sınıflandırıcıyı birleştirir.\n",
    "\n",
    "- AdaBoost, yinelemeli bir topluluk yöntemidir. AdaBoost sınıflandırıcı, yüksek doğrulukta güçlü sınıflandırıcı elde etmeniz için düşük performans gösteren birden çok sınıflandırıcıyı birleştirerek güçlü bir sınıflandırıcı oluşturur.\n",
    "\n",
    "- Adaboost'un arkasındaki temel kavram, sınıflandırıcıların ağırlıklarını ayarlamak ve her yinelemede veri örneğini, anormal gözlemlerin doğru tahminlerini sağlayacak şekilde eğitmektir.\n",
    "\n",
    "- Herhangi bir makine öğrenmesi algoritması, eğitim setindeki ağırlıkları kabul ederse temel sınıflandırıcı olarak kullanılabilir.\n",
    "\n",
    "- **AdaBoost** iki koşulu karşılamalıdır:\n",
    "\n",
    "   1. Sınıflandırıcı, çeşitli ağırlıklı eğitim örnekleri üzerinde etkileşimli olarak eğitilmelidir.\n",
    "  \n",
    "   2. Her yinelemede, eğitim hatasını en aza indirerek bu örnekler için mükemmel bir uyum sağlamaya çalışır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHpI0psZzM-C"
   },
   "source": [
    "  - Bir AdaBoost sınıflandırıcısı oluşturmak için, birinci temel sınıflandırıcı olarak, eğitim verilerimiz üzerinde tahminler yapmak için bir Karar Ağacı algoritması eğittiğimizi hayal edin.\n",
    "\n",
    "- Artık AdaBoost metodolojisi izlenerek yanlış sınıflandırılan eğitim örneklerinin ağırlığı artırıldı.\n",
    "\n",
    "- İkinci sınıflandırıcı eğitilir ve güncellenen ağırlıkları kabul eder ve prosedürü defalarca tekrarlar.\n",
    "\n",
    "- Her model tahmininin sonunda, bir sonraki modelin onlar üzerinde daha iyi bir iş çıkarması için yanlış sınıflandırılan örneklerin ağırlıklarını artırıyoruz vb.\n",
    "\n",
    "- AdaBoost, topluluğa kademeli olarak daha iyi hale getiren tahminciler ekler. Bu algoritmanın en büyük dezavantajı, modelin paralelleştirilememesidir, çünkü her bir tahmin edici ancak bir öncekinin eğitilmesi ve değerlendirilmesinden sonra eğitilebilir.\n",
    "\n",
    "- AdaBoost algoritmasını gerçekleştirme adımları aşağıdadır:\n",
    "\n",
    "  1. Başlangıçta, tüm gözlemlere eşit ağırlıklar verilir.\n",
    "  \n",
    "  2. Bir model, bir veri alt kümesi üzerine kuruludur.\n",
    "  \n",
    "  3. Bu model kullanılarak tüm veri seti üzerinde tahminler yapılır.\n",
    "  \n",
    "  4. Tahminler ve gerçek değerler karşılaştırılarak hatalar hesaplanır.\n",
    "  \n",
    "  5. Bir sonraki model oluşturulurken yanlış tahmin edilen veri noktalarına daha yüksek ağırlıklar verilir.\n",
    "  \n",
    "  6. Ağırlıklar hata değeri kullanılarak belirlenebilir. Örneğin, hata ne kadar yüksek olursa, gözleme verilen ağırlık o kadar fazla olur.\n",
    "  \n",
    "  7. Bu işlem, hata fonksiyonu değişmeyinceye veya tahminci sayısının maksimum sınırına ulaşılıncaya kadar tekrarlanır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjtWPcIwzM-C"
   },
   "source": [
    "## 4. AdaBoost algoritması\n",
    "\n",
    "- Aşağıdaki adımlarda çalışır:\n",
    "\n",
    "   1. Başlangıçta, Adaboost rastgele bir eğitim alt kümesi seçer.\n",
    "  \n",
    "   2. Son eğitimin doğru tahminine dayalı eğitim setini seçerek AdaBoost makine öğrenimi modelini yinelemeli olarak eğitir.\n",
    "  \n",
    "   3. Yanlış sınıflandırılmış gözlemlere daha yüksek ağırlık atar, böylece bir sonraki yinelemede bu gözlemler sınıflandırma için yüksek olasılık alır.\n",
    "  \n",
    "   4. Ayrıca, sınıflandırıcının doğruluğuna göre her yinelemede eğitilen sınıflandırıcıya ağırlık atar. Daha doğru sınıflandırıcı yüksek ağırlık alacaktır.\n",
    "  \n",
    "   5. Bu süreç, eğitim verilerinin tamamı hatasız olarak oturana veya belirtilen maksimum tahminci sayısına ulaşılana kadar yinelenir.\n",
    "  \n",
    "   6. Sınıflandırmak için oluşturduğunuz tüm öğrenme algoritmalarında bir \"oylama\" yapar.\n",
    "  \n",
    "  \n",
    "Aşağıdaki şema ile gösterilebilir:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rtlUtwVzM-D"
   },
   "source": [
    "![AdaBoost Classifier](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1542651255/image_3_nwa5zf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS3YkdilzM-D"
   },
   "source": [
    "## **5. AdaBoost ve Gradient Boost** arasındaki farklar\n",
    "\n",
    "- **AdaBoost**, **Adaptive Boosting** anlamına gelir. sequential ensemble machine learning tekniği üzerinde çalışır. Boosting algoritmalarının genel fikri, ardışık her modelin bir önceki modelin hatalarını düzeltmeye çalıştığı yerde, tahmin edicileri sırayla denemektir.\n",
    "\n",
    "\n",
    "- **GBM veya Gradient Boosting**, sequential modelde de çalışır. Gradient boosting, tahmine göre (öznitelikler yerine) Kayıp Fonksiyonunun (Loss Function) gradyanını (türevi) hesaplar. Gradient boosting, Kayıp Fonksiyonunu (gerçek ve tahmin edilen değerin farkı olan hata) en aza indirerek ve bu kaybı bir sonraki yineleme için hedef olarak alarak doğruluğu artırır.\n",
    "\n",
    "\n",
    "- Gradient boosting algoritması ilk zayıf öğreniciyi oluşturur ve KKayıp Fonksiyonunu hesaplar. Daha sonra, ilk adımdan sonraki kaybı tahmin etmek için ikinci bir öğrenci oluşturur. Adım, üçüncü öğrenci için ve daha sonra dördüncü öğrenci için devam eder ve belirli bir eşiğe ulaşılana kadar böyle devam eder.\n",
    "\n",
    "\n",
    "- Bu durumda akıllara AdaBoost'un Gradient Boosting algoritmasından farklı olduğu, çünkü her ikisinin de Boosting tekniği üzerinde çalıştığı sorusu geliyor.\n",
    "\n",
    "\n",
    "- Hem AdaBoost hem de Gradient Boosting, sıralı (sequential) bir şekilde zayıf öğrenenler oluşturur. Başlangıçta AdaBoost, örnek dağılımının her adımda yanlış sınıflandırılmış örneklere daha fazla ağırlık ve doğru sınıflandırılmış örneklere daha az ağırlık verecek şekilde uyarlanacağı şekilde tasarlanmıştır. Nihai tahmin, daha güçlü öğrencilere daha fazla ağırlık verildiği tüm zayıf öğrencilerin ağırlıklı ortalamasıdır.\n",
    "\n",
    "\n",
    "- Daha sonra, AdaBoost'un belirli bir kayıp fonksiyonuna (üssel kayıp) sahip daha genel toplanır model (additive models) çerçevesi cinsinden ifade edilebileceği keşfedildi.\n",
    "\n",
    "\n",
    "- Yani AdaBoost ve GBM arasındaki temel farklar şunlardır:-\n",
    "\n",
    "\n",
    "  1. Gradient Boost'un toplanır modelleme problemine (additive modeling problem) yaklaşık çözümler bulmak için genel bir algoritma olması, AdaBoost'un ise belirli bir kayıp fonksiyonuna (Üssel kayıp fonksiyonu) sahip özel bir durum olarak görülebilmesidir. Bu nedenle, gradyan artırma çok daha esnektir.\n",
    "\n",
    "\n",
    "  2. AdaBoost, çok daha sezgisel bir bakış açısıyla yorumlanabilir ve eğitim örneklerinin önceki öğrenicilerden alınan sınıflandırmalara dayalı olarak yeniden ağırlıklandırılmasıyla gradyanlara atıfta bulunulmadan uygulanabilir.\n",
    "\n",
    "\n",
    "  3. Adaboost'ta eksiklikler yüksek ağırlıklı veri noktalarıyla belirlenirken, Gradient Boost'ta mevcut zayıf öğrencilerin eksiklikleri gradyanlarla belirlenir.\n",
    "\n",
    "\n",
    "  4. Adaboost daha çok 'oylama ağırlıkları' ile ilgilidir ve Gradyan güçlendirme daha çok 'gradyan optimizasyonu ekleme' ile ilgilidir.\n",
    "\n",
    "\n",
    "  5. Adaboost, model tarafından yanlış sınıflandırılan hedefe daha fazla ağırlık vererek doğruluğu artırır. Her yinelemede, Uyarlanabilir artırma algoritması, örneklerin her birine eklenen ağırlıkları değiştirerek örnek dağılımını değiştirir. Yanlış tahmin edilen örneklerin ağırlıklarını arttırır ve doğru tahmin edilen örneklerin ağırlıklarını azaltır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsUQaZOrzM-F"
   },
   "source": [
    "## 6. AdaBoost uygulaması\n",
    "\n",
    "- Şimdi Python'da AdaBoost algoritmasının uygulama kısmına geldik.\n",
    "\n",
    "- İlk adım, gerekli kütüphaneleri yüklemektir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QonZL4XzM-F"
   },
   "source": [
    "### 6.1 Kütüphane Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "xMS3_MVMzM-G"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv7mGVdWzM-H"
   },
   "source": [
    "### 6.2 Veri Kümesi Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GN5amk3Vzmuf"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9DdX-PCznjD"
   },
   "outputs": [],
   "source": [
    "#ROOT_DIR = \"/content/drive/MyDrive/CASGEM-Egitim/Egitim-Part1/Day7-DecisionTree/notebooks\"\n",
    "ROOT_DIR = \"https://media.githubusercontent.com/media/yapay-ogrenme/casgem-eu-project-training-on-data-mining/main/PART1/Day7-DecisionTree/notebooks/\"\n",
    "DATASET_PATH = ROOT_DIR + \"/datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8-EzRDXzM-I"
   },
   "outputs": [],
   "source": [
    "iris = pd.read_csv(DATASET_PATH + 'Iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX7EvY4qzM-I"
   },
   "source": [
    "### 6.3 Keşifsel Veri Analizi (Exploratory Data Analysis) - EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcgzeKXlzM-I"
   },
   "source": [
    "### Veri kümesi ön izleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lcghw774zM-J"
   },
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jixOdsb9zM-J"
   },
   "source": [
    "### Veri kümesi özet bilgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3inU7sTzM-K"
   },
   "outputs": [],
   "source": [
    "iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cP5sK5-zM-L"
   },
   "source": [
    "Veri setinde eksik değer olmadığını görebiliriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZjmihKozM-L"
   },
   "source": [
    "### Öznitelik vektörü ve hedef değişkeni tanımlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJ151niLzM-L"
   },
   "outputs": [],
   "source": [
    "X = iris[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cHX2vhEzM-M"
   },
   "outputs": [],
   "source": [
    "y = iris['Species']\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UQAPog_zM-N"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le=LabelEncoder()\n",
    "\n",
    "y=le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQpVyzFVzM-N"
   },
   "source": [
    "### 6.4 Veri kümesini eğitim ve test kümelerine ayıralım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XhYt_ZYzM-P"
   },
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kaL13CpzM-Q"
   },
   "source": [
    "### 6.5 AdaBoost modeli oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0PZl8sYzM-S"
   },
   "outputs": [],
   "source": [
    "# Import the AdaBoost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "# Create adaboost classifer object\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=0)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "model1 = abc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOWnIcGOzM-T"
   },
   "source": [
    "### Adaboost Sınıflandırıcısı Oluşturma\n",
    "\n",
    "- En önemli parametreler `base_estimator`, `n_estimators` ve `learning_rate`dir.\n",
    "\n",
    "- **base_estimator**, zayıf modelleri eğitmek için kullanılacak öğrenme algoritmasıdır. Bunun hemen hemen her zaman değiştirilmesi gerekmeyecektir çünkü AdaBoost ile kullanılan en yaygın öğrenici bir karar ağacıdır – bu parametrenin varsayılan argümanıdır.\n",
    "\n",
    "- **n_estimators**, yinelemeli olarak eğitilecek model sayısıdır.\n",
    "\n",
    "- **learning_rate**, her modelin ağırlıklara katkısıdır ve varsayılan olarak 1'dir. Öğrenme oranının düşürülmesi, ağırlıkların küçük bir dereceye kadar artırılacağı veya azaltılacağı anlamına gelir, bu da model eğitimi daha yavaş yapmaya zorlar (ancak bazen daha iyi performans ile sonuçlanır).\n",
    "\n",
    "- **loss**, AdaBoostRegressor'a özeldir ve ağırlıkları güncellerken kullanılacak kayıp işlevini ayarlar. Bu, varsayılan olarak doğrusal bir kayıp işlevidir, ancak kare veya üstel olarak değiştirilebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6by655VnzM-U"
   },
   "source": [
    "### 6.6 Model Değerlendirmesi \n",
    "\n",
    "Sınıflandırıcının veya modelin çeşitlerin türünü ne kadar doğru tahmin edebileceğini tahmin edelim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7tLwkvjzM-U"
   },
   "outputs": [],
   "source": [
    "#import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# calculate and print model accuracy\n",
    "print(\"AdaBoost Classifier Model Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugj1T0sszM-W"
   },
   "source": [
    "### 6.7 SVC temel tahmincisi ile daha fazla değerlendirme\n",
    "\n",
    "\n",
    "- Daha fazla değerlendirme için, SVC'yi temel tahmin edici olarak aşağıdaki gibi kullanacağız: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ua9GcMMJzM-W"
   },
   "outputs": [],
   "source": [
    "# load required classifer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "# import Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.metrics import accuracy_score\n",
    "svc=SVC(probability=True, kernel='linear')\n",
    "\n",
    "\n",
    "# create adaboost classifer object\n",
    "abc =AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1, random_state=0)\n",
    "\n",
    "\n",
    "# train adaboost classifer\n",
    "model2 = abc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# predict the response for test dataset\n",
    "y_pred = model2.predict(X_test)\n",
    "\n",
    "\n",
    "# calculate and print model accuracy\n",
    "print(\"Model Accuracy with SVC Base Estimator:\",accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXhomfqNzM-X"
   },
   "source": [
    "- Bu durumda, SVC Base Estimator, Decision Tree Base Estimator'dan daha iyi doğruluk elde ediliyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoLa3L0CzM-Y"
   },
   "source": [
    "# **7. AdaBoost**'un avantajları ve dezavantajları \n",
    "\n",
    "- Avantajları aşağıdaki gibidir:\n",
    "\n",
    "    1. AdaBoost'u uygulamak kolaydır.\n",
    "  \n",
    "    2. Zayıf sınıflandırıcının hatalarını yinelemeli olarak düzeltir ve zayıf öğrenicileri birleştirerek doğruluğu artırır.\n",
    "  \n",
    "    3. AdaBoost ile birçok temel sınıflandırıcı kullanabiliriz.\n",
    "  \n",
    "    4. AdaBoost overfitting'e meyilli değildir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV0DWhvfzM-Y"
   },
   "source": [
    "- Dezavantajları aşağıdaki gibidir:\n",
    "\n",
    "    1. AdaBoost, gürültülü verilerine duyarlıdır.\n",
    "  \n",
    "    2. Her noktayı tam olarak uydurmaya çalıştığı için aykırı değerlerden oldukça etkilenir.\n",
    "  \n",
    "    3. AdaBoost, XGBoost'a kıyasla daha yavaştır."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WbJYy3ZwzM93"
   ],
   "name": "Day7-DecisionTree-Demo3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
