{"cells":[{"metadata":{"id":"NBg2kzsJzM9w"},"cell_type":"markdown","source":["# AdaBoost Classifier\n"]},{"metadata":{"id":"WbJYy3ZwzM93"},"cell_type":"markdown","source":["\n","## 1. Topluluk (Ensemble) Modellerine Giriş\n","\n","\n","**- Bir topluluk modeli, güçlü bir sınıflandırıcı oluşturmak amacıyla bir dizi düşük performanslı veya zayıf sınıflandırıcıyı birleştiren bir bileşik modeldir.**\n","\n","- Burada, bireysel sınıflandırıcılar oy verir ve çoğunluk oylaması yapan nihai tahmin etiketi döndürülür.\n","\n","- Bu bireysel sınıflandırıcılar, bir topluluk modeli oluşturmak için belirli bir kritere göre birleştirilir.\n","\n","- Topluluk modelleri, bireysel veya temel sınıflandırıcılardan daha fazla doğruluk sunar.\n","\n","- Topluluk öğrenme yöntemlerinin, performansı artırmak için birkaç makine öğrenmesi algoritmasını tek bir tahmin modelinde birleştiren meta-algoritmalar olduğunu söyleyebiliriz.\n","\n","- Topluluk modelleri, aşağıda belirtildiği gibi bazı belirli kriterlere göre oluşturulur:-\n","\n","  - **Bagging** - Bagging yaklaşımı kullanılarak model varyansını azaltmak için oluşturulabilirler.\n","  \n","  - **Boosting** - Boosting yaklaşımı kullanılarak model yanlılığını azaltmak için oluşturulabilirler.\n","  \n","  - **Stacking** - Stacking yaklaşımı kullanılarak model tahminlerini geliştirmek için oluşturulabilirler.\n","  \n","  \n","- Aşağıdaki diyagram yardımı ile gösterilebilir."]},{"metadata":{"trusted":true,"id":"X1YG88GczM95"},"cell_type":"markdown","source":["![Ensemble Machine Learning](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1542651255/image_1_joyt3x.png)"]},{"metadata":{"id":"gFyydkl7zM96"},"cell_type":"markdown","source":["### 1.1 Bagging\n","\n","- **Bagging**, tahminlerin varyansını azaltacak şekilde birden fazla öğrenciyi birleştirir.\n","\n","- Örneğin, rastgele orman (random forest), verilerin farklı rastgele alt kümelerinde N farklı ağacı eğiteceğimiz ve nihai tahmin için oylama yapacağımız N Karar Ağacı eğitir.\n","\n","- **Bagging ensembles** yöntemleri **Rastgele Orman (Random Forest)** ve **Ekstra Ağaçlar (Extra Trees)**'dır."]},{"metadata":{"id":"Z7TqHaQizM97"},"cell_type":"markdown","source":["### 1.2 Boosting\n","\n","- **Boosting** algoritmaları, güçlü bir sınıflandırıcı oluşturmak için bir dizi zayıf sınıflandırıcıyı kullanır.\n","\n","- Boosting algoritmaları, overfitting probleminden daha az etkilenir.\n","\n","- Aşağıdaki üç algoritma, veri bilimi yarışmalarında büyük popülerlik kazanmıştır.\n","\n","   - AdaBoost (Uyarlanabilir Güçlendirme)\n","   - Gradyan Ağacı Artırma (GBM)\n","   - XGBoost\n","  "]},{"metadata":{"id":"filGTuQnzM9-"},"cell_type":"markdown","source":["### 1.3 Stacking\n","\n","- **Stacking** (veya yığın genelleştirme), birden çok temel sınıflandırma modeli tahminlerini yeni bir veri kümesinde birleştiren bir topluluk (ensemble) öğrenme tekniğidir.\n","\n","- Bu yeni veriler, başka bir sınıflandırıcı için giriş verileri olarak değerlendirilir.\n"]},{"metadata":{"id":"sbn9MyKAzM-B"},"cell_type":"markdown","source":["# 2. AdaBoost Sınıflandırıcı\n","\n","\n","- **AdaBoost veya Adaptive Boosting**, 1996 yılında Yoav Freund ve Robert Schapire tarafından önerilen ensemble boosting sınıflandırıcılarından biridir.\n","\n","**- Sınıflandırıcıların doğruluğunu artırmak için birden çok zayıf sınıflandırıcıyı birleştirir.**\n","\n","- AdaBoost, yinelemeli bir topluluk yöntemidir. AdaBoost sınıflandırıcı, yüksek doğrulukta güçlü sınıflandırıcı elde etmeniz için düşük performans gösteren birden çok sınıflandırıcıyı birleştirerek güçlü bir sınıflandırıcı oluşturur.\n","\n","- Adaboost'un arkasındaki temel kavram, sınıflandırıcıların ağırlıklarını ayarlamak ve her yinelemede veri örneğini, anormal gözlemlerin doğru tahminlerini sağlayacak şekilde eğitmektir.\n"]},{"metadata":{"id":"QHpI0psZzM-C"},"cell_type":"markdown","source":["**- AdaBoost algoritmasını gerçekleştirme adımları aşağıdadır:**\n","\n","  1. Başlangıçta, tüm gözlemlere eşit ağırlıklar verilir.\n","  \n","  2. Bir model, bir veri alt kümesi üzerinde eğitilir.\n","  \n","  3. Bu model kullanılarak tüm veri seti üzerinde tahminler yapılır.\n","  \n","  4. Tahminler ve gerçek değerler karşılaştırılarak hatalar hesaplanır.\n","  \n","  5. Bir sonraki model oluşturulurken yanlış tahmin edilen veri noktalarına daha yüksek ağırlıklar verilir.\n","  \n","  6. Ağırlıklar hata değeri kullanılarak belirlenebilir. Örneğin, hata ne kadar yüksek olursa, gözleme verilen ağırlık o kadar fazla olur.\n","  \n","  7. Bu işlem, hata fonksiyonu değişmeyinceye veya tahminci sayısının maksimum sınırına ulaşılıncaya kadar tekrarlanır."]},{"metadata":{"id":"5rtlUtwVzM-D"},"cell_type":"markdown","source":["![AdaBoost Classifier](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1542651255/image_3_nwa5zf.png)"]},{"metadata":{"id":"MsUQaZOrzM-F"},"cell_type":"markdown","source":["## 3. AdaBoost uygulaması\n"]},{"metadata":{"id":"4QonZL4XzM-F"},"cell_type":"markdown","source":["### 3.1 Kütüphane Yükleme"]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"xMS3_MVMzM-G"},"cell_type":"code","source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Fv7mGVdWzM-H"},"cell_type":"markdown","source":["### 3.2 Veri Kümesi Yükleme"]},{"cell_type":"code","source":["#from google.colab import drive\n","#drive.mount('/content/drive')"],"metadata":{"id":"GN5amk3Vzmuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ROOT_DIR = \"/content/drive/MyDrive/CASGEM-Egitim/Egitim-Part1/Day7-DecisionTree/notebooks\"\n","ROOT_DIR = \"https://media.githubusercontent.com/media/yapay-ogrenme/casgem-eu-project-training-on-data-mining/main/PART1/Day7-DecisionTree/notebooks\"\n","\n","DATASET_PATH = ROOT_DIR + \"/datasets/\""],"metadata":{"id":"i9DdX-PCznjD"},"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"m8-EzRDXzM-I"},"cell_type":"code","source":["iris = pd.read_csv(DATASET_PATH + 'Iris.csv')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"YX7EvY4qzM-I"},"cell_type":"markdown","source":["### 3.3 Keşifsel Veri Analizi (Exploratory Data Analysis) - EDA "]},{"metadata":{"id":"RcgzeKXlzM-I"},"cell_type":"markdown","source":["### Veri kümesi ön izleme"]},{"metadata":{"trusted":true,"id":"Lcghw774zM-J"},"cell_type":"code","source":["iris.head()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"jixOdsb9zM-J"},"cell_type":"markdown","source":["### Veri kümesi özet bilgi"]},{"metadata":{"trusted":true,"id":"o3inU7sTzM-K"},"cell_type":"code","source":["iris.info()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"9cP5sK5-zM-L"},"cell_type":"markdown","source":["Veri setinde eksik değer olmadığını görebiliriz."]},{"metadata":{"id":"QZjmihKozM-L"},"cell_type":"markdown","source":["### Öznitelik vektörü ve hedef değişkeni tanımlama"]},{"metadata":{"trusted":true,"id":"CJ151niLzM-L"},"cell_type":"code","source":["X = iris[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n","\n","X.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"9cHX2vhEzM-M"},"cell_type":"code","source":["y = iris['Species']\n","\n","y.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"-UQAPog_zM-N"},"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","le=LabelEncoder()\n","\n","y=le.fit_transform(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"id":"W2iugRbqf8Ii"},"execution_count":null,"outputs":[]},{"metadata":{"id":"uQpVyzFVzM-N"},"cell_type":"markdown","source":["### 3.4 Veri kümesini eğitim ve test kümelerine ayıralım"]},{"metadata":{"trusted":true,"id":"2XhYt_ZYzM-P"},"cell_type":"code","source":["# Import train_test_split function\n","from sklearn.model_selection import train_test_split\n","\n","# Split dataset into training set and test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"0kaL13CpzM-Q"},"cell_type":"markdown","source":["### 3.5 AdaBoost modeli oluşturma"]},{"metadata":{"trusted":true,"id":"T0PZl8sYzM-S"},"cell_type":"code","source":["# Import the AdaBoost classifier\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","\n","# Create adaboost classifer object\n","abc = AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=0)\n","\n","# Train Adaboost Classifer\n","model1 = abc.fit(X_train, y_train)\n","\n","\n","#Predict the response for test dataset\n","y_pred = model1.predict(X_test)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"OOWnIcGOzM-T"},"cell_type":"markdown","source":["### Adaboost Sınıflandırıcısı Oluşturma\n","\n","- **En önemli parametreler `base_estimator`, `n_estimators` ve `learning_rate`dir.**\n","\n","- **base_estimator**, zayıf modelleri eğitmek için kullanılacak öğrenme algoritmasıdır. Bunun hemen hemen her zaman değiştirilmesi gerekmeyecektir çünkü AdaBoost ile kullanılan en yaygın öğrenici bir karar ağacıdır – bu parametrenin varsayılan argümanıdır.\n","\n","- **n_estimators**, yinelemeli olarak eğitilecek model sayısıdır.\n","\n","- **learning_rate**, her modelin ağırlıklara katkısıdır ve varsayılan olarak 1'dir. Öğrenme oranının düşürülmesi, ağırlıkların küçük bir dereceye kadar artırılacağı veya azaltılacağı anlamına gelir, bu da model eğitimi daha yavaş yapmaya zorlar (ancak bazen daha iyi performans ile sonuçlanır).\n","\n","- **loss**, AdaBoostRegressor'a özeldir ve ağırlıkları güncellerken kullanılacak kayıp işlevini ayarlar. Bu, varsayılan olarak doğrusal bir kayıp işlevidir, ancak kare veya üstel olarak değiştirilebilir."]},{"metadata":{"id":"6by655VnzM-U"},"cell_type":"markdown","source":["### 3.6 Model Değerlendirmesi \n","\n","Sınıflandırıcının veya modelin çeşitlerin türünü ne kadar doğru tahmin edebileceğini tahmin edelim. "]},{"metadata":{"trusted":true,"id":"b7tLwkvjzM-U"},"cell_type":"code","source":["#import scikit-learn metrics module for accuracy calculation\n","from sklearn.metrics import accuracy_score\n","\n","\n","# calculate and print model accuracy\n","print(\"AdaBoost Classifier Model Accuracy:\", accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Ugj1T0sszM-W"},"cell_type":"markdown","source":["### 3.7 SVC temel tahmincisi ile daha fazla değerlendirme\n","\n","\n","- Daha fazla değerlendirme için, SVC'yi temel tahmin edici olarak aşağıdaki gibi kullanacağız: "]},{"metadata":{"trusted":true,"id":"ua9GcMMJzM-W"},"cell_type":"code","source":["# load required classifer\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","\n","# import Support Vector Classifier\n","from sklearn.svm import SVC\n","\n","\n","# import scikit-learn metrics module for accuracy calculation\n","from sklearn.metrics import accuracy_score\n","svc=SVC(probability=True, kernel='linear')\n","\n","\n","# create adaboost classifer object\n","abc = AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1, random_state=0)\n","\n","\n","# train adaboost classifer\n","model2 = abc.fit(X_train, y_train)\n","\n","\n","# predict the response for test dataset\n","y_pred = model2.predict(X_test)\n","\n","\n","# calculate and print model accuracy\n","print(\"Model Accuracy with SVC Base Estimator:\",accuracy_score(y_test, y_pred))\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"mXhomfqNzM-X"},"cell_type":"markdown","source":["- Bu durumda, SVC Base Estimator, Decision Tree Base Estimator'dan daha iyi doğruluk elde ediliyor."]},{"metadata":{"id":"CoLa3L0CzM-Y"},"cell_type":"markdown","source":["# **4. AdaBoost'un avantajları ve dezavantajları** \n","\n","- Avantajları aşağıdaki gibidir:\n","\n","    1. AdaBoost'u uygulamak kolaydır.\n","  \n","    2. Zayıf sınıflandırıcının hatalarını yinelemeli olarak düzeltir ve zayıf öğrenicileri birleştirerek doğruluğu artırır.\n","  \n","    3. AdaBoost ile birçok temel sınıflandırıcı kullanabiliriz.\n","  \n","    4. AdaBoost overfitting'e meyilli değildir."]},{"metadata":{"id":"BV0DWhvfzM-Y"},"cell_type":"markdown","source":["- Dezavantajları aşağıdaki gibidir:\n","\n","    1. AdaBoost, gürültülü verilerine duyarlıdır.\n","  \n","    2. Her noktayı tam olarak uydurmaya çalıştığı için aykırı değerlerden oldukça etkilenir.\n","  \n","    3. AdaBoost, XGBoost'a kıyasla daha yavaştır."]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Day7-DecisionTree-Demo3.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}